{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00a6fec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(CVXPY) Nov 15 10:43:15 AM: Encountered unexpected exception importing solver OSQP:\n",
      "ImportError('DLL load failed while importing qdldl: The specified module could not be found.')\n",
      "10.0\n"
     ]
    }
   ],
   "source": [
    "import cvxpy as cp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import cvxpy_dcopf as cd\n",
    "import cvxpy_scopf as cs\n",
    "#import cvxpy_imb as ci\n",
    "from cvxpylayers.torch import CvxpyLayer\n",
    "import torch\n",
    "import torchvision\n",
    "import timeit\n",
    "import precontingency as pc\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data as Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import psutil\n",
    "import pickle\n",
    "\n",
    "import loadsampling as ls\n",
    "\n",
    "#SOLVER SETTINGS FOR SCS\n",
    "# used as solver for cvxpylayer in NN\n",
    "# SCS is standard solver for CVXPYlayer, you can specify other solvers\n",
    "\n",
    "SCS_solver_args={'use_indirect': False,\n",
    "         'gpu': False,\n",
    "         'verbose': False, # False\n",
    "         'normalize': True, #True heuristic data rescaling\n",
    "         'max_iters': 10000, #2500 giving the maximum number of iterations\n",
    "         'scale': 100, #1 if normalized, rescales by this factor\n",
    "         'eps':1e-3, #1e-3 convergence tolerance\n",
    "         'cg_rate': 2, #2 for indirect, tolerance goes down like 1/iter^cg_rate\n",
    "         'alpha': 1.5, #1.5 relaxation parameter\n",
    "         'rho_x':1e-3, #1e-3 x equality constraint scaling\n",
    "         'acceleration_lookback': 10, #10\n",
    "         'write_data_filename':None}\n",
    "\n",
    "\n",
    "SCS_solver_args2={'use_indirect': False,\n",
    "         'gpu': False,\n",
    "         'verbose': False, # False\n",
    "         'normalize': True, #True heuristic data rescaling\n",
    "         'max_iters': 20000, #2500 giving the maximum number of iterations\n",
    "         'scale': 1, #1 if normalized, rescales by this factor\n",
    "         'eps':1e-3, #1e-3 convergence tolerance\n",
    "         'cg_rate': 2, #2 for indirect, tolerance goes down like 1/iter^cg_rate\n",
    "         'alpha': 1.5, #1.5 relaxation parameter\n",
    "         'rho_x':1e-3, #1e-3 x equality constraint scaling\n",
    "         'acceleration_lookback': 10, #10\n",
    "         'write_data_filename':None}\n",
    "\n",
    "\n",
    "SCS_solver_args3={#'use_indirect': False,\n",
    "         #'gpu': False,\n",
    "         'verbose': False, # False\n",
    "         'normalize': False, #True heuristic data rescaling\n",
    "         'max_iters': 10000, #2500 giving the maximum number of iterations\n",
    "         'scale': 1, #1 if normalized, rescales by this factor\n",
    "         'eps':1e-3, #1e-3 convergence tolerance ????????\n",
    "         #'cg_rate': 2, #2 for indirect, tolerance goes down like 1/iter^cg_rate ????????\n",
    "         'alpha': 1.5, #1.5 relaxation parameter\n",
    "         'rho_x':1e-3, #1e-3 x equality constraint scaling\n",
    "         'acceleration_lookback': 10, #10\n",
    "         'write_data_filename':None}\n",
    "\n",
    "ECOS_solver_args = {\"solve_method\":\"ECOS\",\n",
    "    'verbose': False, # False\n",
    "    'max_iters': 10000, # Maximum number of iterations\n",
    "    'reltol': 1e-3, # Convergence tolerance\n",
    "    # Add other ECOS solver arguments as needed\n",
    "}\n",
    "\n",
    "\n",
    "data = pd.read_excel('IEEE27OG.xlsx',sheet_name=None)\n",
    "Sbase = data['par']['base'][0]\n",
    "Nloads = len(data['load'])\n",
    "load_loc = torch.from_numpy(np.array(data['load']['bus']))\n",
    "Ngens = len(data['gen'])\n",
    "gen_loc = torch.from_numpy(np.array(data['gen']['bus']))\n",
    "Nlines = len(data['line'])\n",
    "Nbus = len(data['bus'])\n",
    "print(data[\"line\"]['max_f'][0]/Sbase)\n",
    "\n",
    "max_p = torch.from_numpy(np.array(-data[\"gen\"]['min_p']/Sbase))\n",
    "min_p = torch.from_numpy(np.array(data[\"gen\"]['max_p']/Sbase))\n",
    "\n",
    "Nsamples = 1000\n",
    "Niterations = 100\n",
    "batch_size = 100\n",
    "train = 800/1000\n",
    "test = 200/1000\n",
    "tollerance_crit = 0.1\n",
    "learning_rate = 0.01\n",
    "wd = 0.1 \n",
    "case = 'N-3'\n",
    "probabilistic = 'False'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42304390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.parallel import DataParallel\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # GPU is available\n",
    "    device = torch.device('cuda')\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    # GPU is not available\n",
    "    device = torch.device('cpu')\n",
    "    print(\"GPU is not available\")\n",
    "\n",
    "\n",
    "if torch.cuda.device_count() >= 1:\n",
    "    device_ids = list(range(torch.cuda.device_count()))\n",
    "else:\n",
    "    device_ids = None\n",
    "    \n",
    "num_workers = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72387614",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "#X = np.array(data['load']['p'])/Sbase*np.random.uniform(low=0.75, high=1.25, size = (Nsamples,Nloads))\n",
    "\n",
    "#LB = 0.75*np.ones(Nloads)\n",
    "#UB = 1.25*np.ones(Nloads)\n",
    "#X = np.array(data['load']['p'])/Sbase*ls.kumaraswamymontecarlo(1.6, 2.8, 0.75, LB, UB, Nsamples).T\n",
    "\n",
    "file_path = 'trained models/training data/Xtrain.pkl'  \n",
    "with open(file_path, 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "\n",
    "Xtrain = torch.tensor(X[0:int(Nsamples),:],dtype = torch.float32)        \n",
    "Xtrain_transpose = Xtrain.transpose(0,1)\n",
    "#Xtest = torch.tensor(X[int(train*Nsamples):,:],dtype = torch.float64)\n",
    "#standardise\n",
    "Xmin, Xmax,Xmean,Xstd = np.min(X, axis = 0),np.max(X, axis = 0),np.mean(X, axis = 0),np.std(X, axis = 0)\n",
    "Xscal = torch.tensor(( X - Xmean ) / Xstd, dtype=torch.float32)\n",
    "#Xscaltrain = Xscal[0:int(train*Nsamples),:]\n",
    "#Xscaltest = Xscal[int(train*Nsamples):,:]\n",
    "\n",
    "\n",
    "c_max = np.max(np.array(data['gen']['cost']))\n",
    "c_min = np.min(np.array(data['gen']['cost']))\n",
    "\n",
    "gencost = torch.tensor(np.array(data['gen']['cost']),dtype = torch.float32)  \n",
    "c_delta = c_max - c_min\n",
    "cgi0 = torch.tensor((np.array(data['gen']['cost'])-c_min)/c_delta,dtype = torch.float32)\n",
    "alpha = 1\n",
    "\n",
    "PTDF = torch.from_numpy(pc.compptdfs(data)).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af6e0289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to create model:  0.14757009999999937\n",
      "RAM Used (GB): 6.12376576\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "#problem0, Pd0, cg0, cll0, Pgi, Fl0, Th0, lam10, lam20 = cd.create_dcopf_correction(data)\n",
    "problem0, Pd0, cll0, Pgi, Fl0, Th0, lam10, lam20 = cd.create_dcopf_correction(data)\n",
    "#problem0, Pd0, Pgi, Fl0, Th0, lam10, lam20 = cd.create_dcopf_correction0(data)\n",
    "assert problem0.is_dpp()\n",
    "\n",
    "#cvxpylayer0 = CvxpyLayer(problem0, parameters=[Pd0, cg0, cll0], variables=[Pgi, Fl0, Th0, lam10, lam20])\n",
    "cvxpylayer0 = CvxpyLayer(problem0, parameters=[Pd0, cll0], variables=[ Pgi, Fl0, Th0, lam10, lam20])\n",
    "#cvxpylayer0 = CvxpyLayer(problem0, parameters=[Pd0,  Pgi], variables=[Fl0, Th0, lam10, lam20])\n",
    "time1 = timeit.default_timer() - start\n",
    "print(\"Time to create model: \", time1)\n",
    "\n",
    "max_f = data[\"line\"]['max_f'][0]/Sbase\n",
    "l = len(data['line'])\n",
    "print('RAM Used (GB):', psutil.virtual_memory()[3]/1000000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62978df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load LODF time: 0.004001140594482422\n",
      "RAM Used (GB): 6.134112256\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "dict_method = 'pickle'\n",
    "\n",
    "start_load = time.time()\n",
    "if case == 'N-1':\n",
    "    if dict_method == 'pickle': \n",
    "        file_path_lodf = 'LODF_list/IEEE27/LODF_listN1.pkl'  \n",
    "        with open(file_path_lodf, 'rb') as f:\n",
    "            LODF_dict = pickle.load(f)\n",
    "            \n",
    "        file_path_SL = 'LODF_list/IEEE27/SL_N1.pkl'\n",
    "        with open(file_path_SL, 'rb') as f:\n",
    "            singular_list = pickle.load(f)\n",
    "    \n",
    "    Ncontingencies = int(l) \n",
    "    tot_lodf_batches = len(LODF_dict)\n",
    "    lodf_batch_size = Ncontingencies\n",
    "    last_batch_size = Ncontingencies - (lodf_batch_size*(tot_lodf_batches-1))\n",
    "\n",
    "if case == 'N-2':\n",
    "    if dict_method == 'pickle': \n",
    "        file_path = 'LODF_list/IEEE27/LODF_listN2.pkl'  \n",
    "        with open(file_path, 'rb') as f:\n",
    "            LODF_dict = pickle.load(f)\n",
    "        \n",
    "        file_path_SL = 'LODF_list/IEEE27/SL_N2.pkl'\n",
    "        with open(file_path_SL, 'rb') as f:\n",
    "            singular_list = pickle.load(f)\n",
    "        \n",
    "        print(singular_list)\n",
    "\n",
    "    Ncontingencies = int(l*(l-1)/2)\n",
    "    tot_lodf_batches = len(LODF_dict)\n",
    "    lodf_batch_size = Ncontingencies\n",
    "    last_batch_size = Ncontingencies - (lodf_batch_size*(tot_lodf_batches-1))\n",
    "            \n",
    "\n",
    "if case == 'N-3':        \n",
    "    if dict_method == 'pickle': \n",
    "        file_path = 'LODF_list/IEEE27/LODF_listN3.pkl'  \n",
    "        with open(file_path, 'rb') as f:\n",
    "            LODF_dict = pickle.load(f)\n",
    "        \n",
    "        file_path_SL = 'LODF_list/IEEE27/SL_N3.pkl'\n",
    "        with open(file_path_SL, 'rb') as f:\n",
    "            singular_list = pickle.load(f)\n",
    "    \n",
    "    Ncontingencies = int(l*(l-1)/2*(l-2)/3)\n",
    "    tot_lodf_batches = len(LODF_dict)\n",
    "    lodf_batch_size = Ncontingencies #20000\n",
    "    last_batch_size = Ncontingencies - (lodf_batch_size*(tot_lodf_batches-1))\n",
    "    \n",
    "if case == 'N-4':\n",
    "        \n",
    "    if dict_method == 'pickle': \n",
    "        file_path = 'LODF_list/IEEE27/LODF_listN4.pkl'  \n",
    "        with open(file_path, 'rb') as f:\n",
    "            LODF_dict = pickle.load(f)\n",
    "        \n",
    "        file_path_SL = 'LODF_list/IEEE27/SL_N4.pkl'\n",
    "        with open(file_path_SL, 'rb') as f:\n",
    "            singular_list = pickle.load(f)\n",
    "    \n",
    "    Ncontingencies = int(l*(l-1)/2*(l-2)/3*(l-3)/4)\n",
    "    tot_lodf_batches = len(LODF_dict)\n",
    "    lodf_batch_size = 20000\n",
    "    last_batch_size = Ncontingencies - (lodf_batch_size*(tot_lodf_batches-1))\n",
    "    \n",
    "if case == 'N-5':\n",
    "        \n",
    "    if dict_method == 'pickle': \n",
    "        file_path = 'LODF_list/IEEE27/LODF_listN5.pkl'  \n",
    "        with open(file_path, 'rb') as f:\n",
    "            LODF_dict = pickle.load(f)\n",
    "        \n",
    "        file_path_SL = 'LODF_list/IEEE27/SL_N5.pkl'\n",
    "        with open(file_path_SL, 'rb') as f:\n",
    "            singular_list = pickle.load(f)\n",
    "    \n",
    "    Ncontingencies = int(l*(l-1)/2*(l-2)/3*(l-3)/4*(l-4)/5)\n",
    "    tot_lodf_batches = len(LODF_dict)\n",
    "    lodf_batch_size = 20000\n",
    "    last_batch_size = Ncontingencies - (lodf_batch_size*(tot_lodf_batches-1))\n",
    "    \n",
    "if case == 'N-6':\n",
    "        \n",
    "    if dict_method == 'pickle': \n",
    "        file_path = 'LODF_list/IEEE27/LODF_listN6.pkl'  \n",
    "        with open(file_path, 'rb') as f:\n",
    "            LODF_dict = pickle.load(f)\n",
    "        \n",
    "        file_path_SL = 'LODF_list/IEEE27/SL_N6.pkl'\n",
    "        with open(file_path_SL, 'rb') as f:\n",
    "            singular_list = pickle.load(f)\n",
    "    \n",
    "    Ncontingencies = int(l*(l-1)/2*(l-2)/3*(l-3)/4*(l-4)/5*(l-5)/6)\n",
    "    tot_lodf_batches = len(LODF_dict)\n",
    "    lodf_batch_size = 20000\n",
    "    last_batch_size = Ncontingencies - (lodf_batch_size*(tot_lodf_batches-1))\n",
    "        \n",
    "    \n",
    "end_load = time.time()\n",
    "\n",
    "print('Load LODF time:', end_load - start_load)\n",
    "print('RAM Used (GB):', psutil.virtual_memory()[3]/1000000000)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f463485",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = len(data['line'])\n",
    "\n",
    "COO_elements = 0\n",
    "values_tot = 0\n",
    "lodf_elements = l*l*Ncontingencies\n",
    "for b in range(len(LODF_dict)):\n",
    "    lodf_batch = lodf_batch_size\n",
    "    if b == (tot_lodf_batches - 1): \n",
    "        lodf_batch = last_batch_size\n",
    "                \n",
    "    index1 = LODF_dict[b][0].detach()\n",
    "    index2 = LODF_dict[b][1].detach()\n",
    "    indices = torch.stack((index1,index2)).detach()\n",
    "    values = LODF_dict[b][2].detach()\n",
    "    shape = (l,int(l*lodf_batch))\n",
    "    lodf = torch.sparse_coo_tensor(indices,values,size = shape).coalesce()\n",
    "\n",
    "    values_coo = lodf.values()\n",
    "    #print('size value coo tensor:', values_coo.size())\n",
    "    indices_coo = lodf.indices()\n",
    "    #print('size index tensor:', indices_coo.size())\n",
    "    values_tot += torch.numel(values_coo) \n",
    "    \n",
    "    COO_elements += torch.numel(values_coo) \n",
    "    \n",
    "    COO_elements += torch.numel(indices_coo)\n",
    "\n",
    "\n",
    "print('total elements lodf:', lodf_elements)\n",
    "element_increase = (COO_elements/lodf_elements)*100 - 100\n",
    "print('element increase coo:', element_increase, '%')\n",
    "sparsity = (values_tot/lodf_elements)*100 - 100\n",
    "print('sparsity:', sparsity, '%')\n",
    "\n",
    "print('compression:', lodf_elements/COO_elements)\n",
    "\n",
    "#l = 187\n",
    "k = 6\n",
    "\n",
    "sparsity_guarantee = (1-((k*(l-k))/(l**2)))*100\n",
    "print(sparsity_guarantee)\n",
    "\n",
    "print(len(singular_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "690b68e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:03<00:00, 328.31it/s]\n"
     ]
    }
   ],
   "source": [
    "#Benchmark either DCOPF or SCOPF\n",
    "benchmark = 'DCOPF' #'SCOPF'\n",
    "\n",
    "Pgisscopf = torch.zeros(Ngens,int(Nsamples),dtype = torch.float32)\n",
    "Fl0sscopf = torch.zeros(Nlines,int(Nsamples),dtype = torch.float32)\n",
    "Flcs = torch.zeros(Nlines,l,int(Nsamples),dtype = torch.float32)\n",
    "lam1s = torch.zeros(Nbus,int(Nsamples),dtype = torch.float32)\n",
    "lam2s = torch.zeros(Nbus,int(Nsamples),dtype = torch.float32)\n",
    "lam1sc = torch.zeros(Nbus,l,int(Nsamples),dtype = torch.float32)\n",
    "lam2sc = torch.zeros(Nbus,l,int(Nsamples),dtype = torch.float32)\n",
    "Th0 = torch.zeros(Nbus,int(Nsamples),dtype = torch.float32)\n",
    "\n",
    "#======== remove islanding cases from contingency list\n",
    "lcontingencies = range(l)\n",
    "islanding_cases = [15,20]\n",
    "lcontingencies_filtered = [l for l in lcontingencies if l not in islanding_cases]\n",
    "lcontingencies = lcontingencies_filtered\n",
    "\n",
    "time_scopfs = time.time()\n",
    "\n",
    "#======== determine infeasible cases\n",
    "# add loads\n",
    "load_profile = torch.zeros((Nsamples,Nbus)).to(torch.float32)\n",
    "load_profile[:, load_loc] = torch.from_numpy(X).to(torch.float32)\n",
    "\n",
    "if benchmark == 'SCOPF':\n",
    "    problem0scopf, Pd0scopf, cg0scopf, Pgiscopf, Fl0scopf, Th0scopf, Flc, lam1, lam2, lam1c, lam2c = cs.create_scopf(data, lcontingencies)\n",
    "\n",
    "elif benchmark == 'SCOPF2':\n",
    "    problem0scopf, Pd0scopf, cg0scopf, Pgiscopf, Fl0scopf, Th0scopf, Flc = cs.create_scopf2(data, lcontingencies)\n",
    "\n",
    "elif benchmark == 'DCOPF': \n",
    "    #problem0scopf, Pd0scopf, cg0scopf, cl0scopf, Pgiscopf, Fl0scopf, Th0scopf, lam1dc, lam2dc = cd.create_dcopf_upper(data)\n",
    "    problem0scopf, Pd0scopf, cg0scopf, cll0scopf, Pgiscopf, Fl0scopf, Th0scopf, lam1, lam2 = cd.create_dcopf_lower(data)\n",
    "    cll0scopf.value = np.zeros(Ngens)\n",
    "\n",
    "cg0scopf.value = np.array(data['gen']['cost'])\n",
    "for entry in tqdm(range(int(Nsamples)),position=0, leave=True):   \n",
    "    Pd0scopf.value = X[entry,:] #np.array(data['load']['p'])/Sbase    \n",
    "    solution  = problem0scopf.solve(solver=cp.ECOS)    # cp.MOSEK\n",
    "    if problem0scopf.status in [\"infeasible\", \"unbounded\"]:\n",
    "         # Otherwise, problem.value is inf or -inf, respectively.\n",
    "         print(\"Problem infeasible or unbounded\")\n",
    "         #print(\"Optimal value: %s\" % problem0scopf.value)\n",
    "    Pgisscopf[:,entry] = torch.tensor(Pgiscopf.value)\n",
    "    Fl0sscopf[:,entry] = torch.tensor(Fl0scopf.value)\n",
    "    #Flcs[:,:,entry] = insert_zero_columns(torch.tensor(Flc.value),islanding_cases)\n",
    "    Th0[:,entry] = torch.tensor(Th0scopf.value)\n",
    "    \n",
    "    lam1s[:,entry] = torch.tensor(lam1.value)\n",
    "    lam2s[:,entry] = torch.tensor(lam2.value)\n",
    "    #lam1sc[:,:,entry] = insert_zero_columns(torch.tensor(lam1c.value),islanding_cases)\n",
    "    #lam2sc[:,:,entry] = insert_zero_columns(torch.tensor(lam2c.value),islanding_cases)\n",
    "    \n",
    "time_scopfs = time.time() - time_scopfs\n",
    "cost_scopf = torch.matmul(gencost,Pgisscopf)\n",
    "\n",
    "# add gens\n",
    "load_profile[:, gen_loc] += Pgisscopf.T\n",
    "\n",
    "# add smallest values to zero elements\n",
    "nonzero_values, _ = torch.min(load_profile.masked_fill(load_profile == 0, float('inf')), dim=1, keepdim=True)\n",
    "mask = load_profile == 0\n",
    "load_profile = torch.where(mask, nonzero_values, load_profile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01f5ccfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=21, out_features=8, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.2, inplace=False)\n",
      "  (3): Linear(in_features=8, out_features=8, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Dropout(p=0.2, inplace=False)\n",
      "  (6): Linear(in_features=8, out_features=8, bias=True)\n",
      "  (7): ReLU()\n",
      "  (8): Dropout(p=0.2, inplace=False)\n",
      "  (9): Linear(in_features=8, out_features=10, bias=True)\n",
      "  (10): Tanh()\n",
      ")\n",
      "Load LODF time: 0.004001140594482422\n",
      "RAM Used (GB): 6.137348096\n",
      "\n",
      "FC neural network: 410\n",
      "input loads: 21 output gens: 10\n"
     ]
    }
   ],
   "source": [
    "with torch.set_grad_enabled(True):\n",
    "    N, D_in, D_out = Nsamples, Nloads, Ngens\n",
    "    H1, H2, H3 = 8,8,8\n",
    "   \n",
    "    #very large network\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(D_in, H1),\n",
    "        torch.nn.ReLU(),\n",
    "        nn.Dropout(p=0.2),\n",
    "        torch.nn.Linear(H1, H2),\n",
    "        torch.nn.ReLU(),\n",
    "        nn.Dropout(p=0.2),\n",
    "        torch.nn.Linear(H2, H3),\n",
    "        torch.nn.ReLU(),\n",
    "        nn.Dropout(p=0.2),\n",
    "        torch.nn.Linear(H3, D_out), #before 23/12 when everything was working!\n",
    "        #torch.nn.Sigmoid(),\n",
    "        torch.nn.Tanh(),\n",
    "        #torch.nn.Softmax(dim=0),\n",
    "    )\n",
    "    \n",
    "    #model = torch.nn.RNN(input_size = D_in, hidden_size = D_out, num_layers=3, nonlinearity = 'tanh', batch_first = True)\n",
    "    model.double()\n",
    "    \n",
    "    def init_weights(u):\n",
    "        if type(u) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(u.weight)\n",
    "            #u.reset_parameters() # default gaussion initializaiton\n",
    "            torch.nn.init.xavier_uniform_(u.weight, gain=nn.init.calculate_gain('relu'))\n",
    "            u.bias.data.fill_(0)    \n",
    "    model.apply(init_weights)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    # adaptive momentum with weight decay\n",
    "    \n",
    "    model.to(torch.float32)\n",
    "    print(model)\n",
    "\n",
    "print('Load LODF time:', end_load - start_load)\n",
    "print('RAM Used (GB):', psutil.virtual_memory()[3]/1000000000)\n",
    "    \n",
    "num_params_full = sum(p.numel() for p in model.parameters())\n",
    "#num_params_TT = sum(p.numel() for p in modelTT.parameters())\n",
    "print('')\n",
    "print('FC neural network:', num_params_full)\n",
    "#print('TT neural network:', num_params_TT)\n",
    "print('input loads:', D_in, 'output gens:', D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29b8ccce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "if device_ids:\n",
    "    model = DataParallel(model, device_ids=device_ids)\n",
    "    model = model.to(device)\n",
    "    print(device_ids)\n",
    "\n",
    "model.to(torch.float32)\n",
    "print(next(model.parameters()).device)\n",
    "print(next(model.parameters()).dtype)\n",
    "\n",
    "# prompt to empty GPU cache: torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce8b1ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 5, 9, 16, 18, 19, 21, 25, 26]\n",
      "0     1\n",
      "1     9\n",
      "2    16\n",
      "3    16\n",
      "4    18\n",
      "5    19\n",
      "6    21\n",
      "7    25\n",
      "8    26\n",
      "9     5\n",
      "Name: bus, dtype: int64\n",
      "[0, 2, 3, 5, 6, 7, 8, 10, 12, 13, 15, 16, 17, 19, 20, 21, 22, 23, 24, 25, 26]\n"
     ]
    }
   ],
   "source": [
    "gen_indices = data['gen']['bus'] \n",
    "generator_nodes = np.zeros(Nbus)\n",
    "for index in gen_indices:\n",
    "    generator_nodes[index] = 1\n",
    "gen_nodes = np.nonzero(generator_nodes)\n",
    "gen_nodes = gen_nodes[0].tolist()\n",
    "print(gen_nodes)\n",
    "print(gen_indices)\n",
    "\n",
    "load_indices = data['load']['bus'] \n",
    "load_nodes = np.zeros(Nbus)\n",
    "for index in load_indices:\n",
    "    load_nodes[index] = 1\n",
    "load_nodes = np.nonzero(load_nodes)\n",
    "load_nodes = load_nodes[0].tolist()\n",
    "print(load_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de083d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "max_p = torch.from_numpy(np.array(-data[\"gen\"]['min_p']/Sbase))\n",
    "min_p = torch.from_numpy(np.array(data[\"gen\"]['max_p']/Sbase))\n",
    "\n",
    "#============== Initialize training testing data\n",
    "Xscal_train, Xscal_test, cost_scopf_train, cost_scopf_test = train_test_split(Xscal, cost_scopf, test_size=1-train, random_state=42)\n",
    "\n",
    "training_dataset = Data.TensorDataset(Xscal_train, cost_scopf_train)\n",
    "testing_dataset = Data.TensorDataset(Xscal_test, cost_scopf_test)\n",
    "\n",
    "train_loader = Data.DataLoader(dataset=training_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = Data.DataLoader(dataset=testing_dataset, batch_size=batch_size, shuffle=False)  # Note: shuffle=False for testing\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "flow_penalty = 0.1\n",
    "crit_penalty = 0\n",
    "infeasibility_penalty = 0\n",
    "cost_penalty = 5\n",
    "soft_ineq_penalty = 100\n",
    "soft_eq_penalty = 2000\n",
    "\n",
    "#================ Initialize tensors\n",
    "error = np.zeros(shape=(Niterations,2))\n",
    "avg_infeasibility = np.zeros(shape=(Niterations,2))\n",
    "relcost = np.zeros(shape=(Niterations,2))\n",
    "\n",
    "time_CF = np.zeros(shape=(Niterations,1))\n",
    "memory_CF = np.zeros(shape=(Niterations,1))\n",
    "percentage_CF = np.zeros(shape=(Niterations,1))\n",
    "backward_CF = np.zeros(shape=(Niterations,1))\n",
    "\n",
    "Pgis = torch.zeros(size = (Ngens,int(batch_size)),dtype = torch.float32)\n",
    "Flis = torch.zeros(size = (Nlines,int(batch_size)),dtype = torch.float32)\n",
    "clil = torch.zeros(size = (Ngens,int(batch_size)),dtype = torch.float32)\n",
    "infeasibility = torch.zeros(int(batch_size), requires_grad = False)\n",
    "\n",
    "relcotrain_epoch = torch.zeros(size = (int(Nsamples*(train)/batch_size),int(batch_size)),dtype = torch.float32, requires_grad = False)\n",
    "relcotest_epoch = torch.zeros(size = (int(Nsamples*(test)/batch_size),int(batch_size)),dtype = torch.float32, requires_grad = False)\n",
    "\n",
    "imbalancetrain_epoch = torch.zeros(size = (int(Nsamples*(train)/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "imbalancetest_epoch = torch.zeros(size = (int(Nsamples*(test)/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "\n",
    "infeasibilitytrain_epoch = torch.zeros(size = (int(Nsamples*(train)/batch_size),Nbus),dtype = torch.float32, requires_grad = False)\n",
    "infeasibilitytest_epoch = torch.zeros(size = (int(Nsamples*(test)/batch_size),Nbus),dtype = torch.float32, requires_grad = False)\n",
    "infeasibility_count = torch.zeros(size = (int(Nsamples/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "\n",
    "line_violation_count = torch.zeros(size = (int(Nsamples/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "crit_violation_count = torch.zeros(size = (int(Nsamples/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "avg_violation_list = torch.zeros(size = (int(Nsamples/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "count_violation_list = torch.zeros(size = (int(Nsamples/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "\n",
    "violation_percentage = torch.zeros(size = (int(Nsamples*train/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "time_contflow = torch.zeros(size = (int(Nsamples*train/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "memory_contflow = torch.zeros(size = (int(Nsamples*train/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "backward_contflow = torch.zeros(size = (int(Nsamples*train/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "\n",
    "print(\"It |   Time   |    Imbalance    | Mod out  |   Infeasibility   |  Rel cost\")\n",
    "for epoch in range(Niterations):\n",
    "    \n",
    "    time_nnSCS = time.time()\n",
    "    for i, (Xbatch, cost_scopf_batch) in enumerate(tqdm(train_loader)):\n",
    "            start_forward = time.time()\n",
    "            start_forward_memory = psutil.virtual_memory()[3] / 1000000000            \n",
    "            \n",
    "            #============= initialize\n",
    "            model.train()\n",
    "            optimizer.zero_grad() \n",
    "            model.zero_grad()\n",
    "            Xbatch = Xbatch.clone().detach().to(device)\n",
    "            cost_scopf_batch = cost_scopf_batch.clone().detach().to(device)\n",
    "\n",
    "            Xtrain_batch = ((Xbatch.cpu() * Xstd) + Xmean).to(torch.float32).to(device)\n",
    "            load_profile = torch.zeros((batch_size, Nbus)).to(device)\n",
    "            #load_profile[:, load_loc] = Xtrain_batch\n",
    "\n",
    "            #================== Perform forward pass\n",
    "            start_FP = time.time()\n",
    "            clil = (model(Xbatch) + 1) / 2\n",
    "            end_FP = time.time()\n",
    "            \n",
    "            #============= soft violation base case\n",
    "            #===================== line flow limits\n",
    "            Pg_guess = torch.mul(max_p,clil) \n",
    "            columns_to_sum = [3,4]\n",
    "            sum_result = torch.sum(Pg_guess[:, columns_to_sum], dim=1).unsqueeze(1)\n",
    "            modified_tensor = torch.cat((Pg_guess[:, :3], sum_result, Pg_guess[:, 5:]), dim=1)\n",
    "            Pg_mod = modified_tensor\n",
    "            \n",
    "            Pg_train = torch.zeros((batch_size,Nbus)).to(torch.float32)\n",
    "            Pd_train = torch.zeros((batch_size,Nbus)).to(torch.float32)\n",
    "\n",
    "            Pg_train[:,gen_nodes] = Pg_mod.to(torch.float32)\n",
    "            Pd_train[:,load_nodes] = Xtrain_batch.to(torch.float32)\n",
    "                \n",
    "            Fl_guess = PTDF@(Pg_train-Pd_train).T\n",
    "            Fl_base = F.relu(torch.abs(Fl_guess)-max_f).sum()/int(batch_size)\n",
    "            \n",
    "            #================= power balance\n",
    "            Pg_tot = Pg_guess.sum(dim=1)\n",
    "            Pd_tot = Xtrain_batch.sum(dim=1)\n",
    "            eq_imbalance = torch.abs(Pg_tot - Pd_tot).sum()/int(batch_size)\n",
    "            eq_infeasible = torch.abs(Pg_tot - Pd_tot)\n",
    "            eq_infeasible[torch.abs(eq_infeasible) < 0.1*Pg_tot] = 0\n",
    "            \n",
    "            #============= cvxpylayer\n",
    "            start_cvxpy = time.time()\n",
    "            Pgi, Fli, Thi, lam1i, lam2i = cvxpylayer0(Xtrain_batch, clil, solver_args=ECOS_solver_args)\n",
    "            Pgis = Pgi.T # 9x100\n",
    "            Flis = Fli.T.to(torch.float32) #33x100\n",
    "            end_cvxpy = time.time()\n",
    "            \n",
    "            #=============== Without correction layer\n",
    "            Pgis = Pg_guess.T.to(torch.float32)\n",
    "            Flis = Fl_guess.to(torch.float32)\n",
    "                       \n",
    "            #================ infeasibility\n",
    "            infeasibility_tensor = (lam1i + lam2i)\n",
    "            infeasibility_loss = infeasibility_tensor.sum() / int(batch_size)\n",
    "            infeasibility_batch = infeasibility_tensor.sum(dim=0).detach()\n",
    "\n",
    "            infeasibility_tensor[abs(infeasibility_tensor) < 1e-5] = 0\n",
    "            infeasible_buses = (abs(infeasibility_tensor) > 1e-5)\n",
    "\n",
    "            infeasible_cases = torch.zeros((batch_size, 1)).to(device)\n",
    "            row_has_true = torch.any(infeasible_buses, axis=1)\n",
    "            infeasible_cases[row_has_true] = True\n",
    "            infeasible_cases = infeasible_cases.sum()\n",
    "            \n",
    "            #================== without correction layer\n",
    "            infeasible_cases = torch.zeros((batch_size, 1)).to(device)\n",
    "            row_has_true_ineq = torch.any(F.relu(torch.abs(Fl_guess)-max_f), axis=0)\n",
    "            row_has_true_eq = torch.any(eq_infeasible.unsqueeze(1), axis = 1)\n",
    "            infeasible_cases[row_has_true_ineq] = True\n",
    "            infeasible_cases[row_has_true_eq] = True\n",
    "            infeasible_cases = infeasible_cases.sum()\n",
    "\n",
    "            #==================== cost\n",
    "            gencost = gencost.requires_grad_()\n",
    "            \n",
    "            batch_cost = torch.matmul(gencost, Pgis)\n",
    "            batch_relco = ((batch_cost - cost_scopf_batch) / cost_scopf_batch) * 100\n",
    "            relco_avg = torch.mean(batch_relco)\n",
    "\n",
    "            cost_loss = torch.mean(batch_cost) #/ int(batch_size)\n",
    "\n",
    "            #===================== compute contingency flows for each sample  \n",
    "            imbalancetrain_batch = 0\n",
    "            critical_loss = 0\n",
    "            count_crit_violation = 0\n",
    "            count_line_violation = 0\n",
    "            count_violation = 0\n",
    "\n",
    "            for b in range(tot_lodf_batches):\n",
    "                lodf_batch = lodf_batch_size\n",
    "                if b == (tot_lodf_batches - 1): \n",
    "                    lodf_batch = last_batch_size\n",
    "                left_ones = torch.ones((lodf_batch, 1)).to(device)\n",
    "                right_ones = torch.ones((int(l * batch_size), 1)).to(device)\n",
    "                Fli_0 = Flis.expand(lodf_batch, -1, -1)\n",
    "\n",
    "                index1 = LODF_dict[b][0].detach()\n",
    "                index2 = LODF_dict[b][1].detach()\n",
    "                indices = torch.stack((index1, index2)).detach()\n",
    "                values = LODF_dict[b][2].detach()\n",
    "                shape = (l, int(l * lodf_batch))\n",
    "                lodf = torch.sparse_coo_tensor(indices, values, size=shape, requires_grad=False).T.to(device).detach()\n",
    "\n",
    "                imbalancetrain_batch_loop = (F.relu(torch.abs(Fli_0 + torch.sparse.mm(lodf, Flis).reshape((lodf_batch, l, batch_size))) - max_f)).detach().to(device)\n",
    "                count_crit_violation_loop = torch.sum(imbalancetrain_batch_loop > tollerance_crit * max_f, dim=1)\n",
    "                count_line_violation_loop = torch.sum(imbalancetrain_batch_loop > 0, dim=1)\n",
    "                imbalancetrain_batch_loop = imbalancetrain_batch_loop.reshape(int(l * lodf_batch), -1)\n",
    "\n",
    "                mask = torch.all(imbalancetrain_batch_loop == 0, dim=1) \n",
    "                indices_remove = mask.nonzero().squeeze(1)\n",
    "                num_zero_rows = torch.sum(mask)\n",
    "                batch_percentage_violation = (num_zero_rows.item() / len(imbalancetrain_batch_loop[:, 0])) * 100\n",
    "\n",
    "                mask_flow = torch.ones(imbalancetrain_batch_loop.shape[0], dtype=torch.bool).to(device)\n",
    "                mask_flow[indices_remove] = False\n",
    "                lodf_test = lodf.to_dense()[mask_flow]\n",
    "                lodf_test = lodf_test.clone().detach()\n",
    "                lodf = lodf_test.to_sparse_coo().requires_grad_().to(device)\n",
    "\n",
    "\n",
    "                Fli_0 = Flis.repeat(lodf_batch, 1).to(torch.float32).to(device)[mask_flow].to(device)\n",
    "                imbalancetrain_batch_loop = (F.relu(torch.abs(Fli_0 + torch.sparse.mm(lodf, Flis)) - max_f)).to(device)\n",
    "                critical_violation_penalty = imbalancetrain_batch_loop[imbalancetrain_batch_loop > tollerance_crit * max_f]\n",
    "                count_violation_loop = torch.count_nonzero(imbalancetrain_batch_loop)  \n",
    "\n",
    "                imbalancetrain_batch += torch.sum(imbalancetrain_batch_loop) / int(batch_size)\n",
    "                critical_loss += torch.sum(critical_violation_penalty)/int(batch_size)\n",
    "                \n",
    "                count_violation += count_violation_loop\n",
    "                count_crit_violation += torch.count_nonzero(count_crit_violation_loop.detach())\n",
    "                count_line_violation += torch.count_nonzero(count_line_violation_loop.detach())\n",
    "\n",
    "            end_forward = time.time()\n",
    "            end_forward_memory = psutil.virtual_memory()[3] / 1000000000\n",
    "            \n",
    "            memory_contflow_batch = end_forward_memory - start_forward_memory\n",
    "            time_contflow_batch = end_forward - start_forward\n",
    "            \n",
    "            #======================== Perform back pass\n",
    "            loss = flow_penalty * imbalancetrain_batch + infeasibility_penalty * infeasibility_loss \\\n",
    "            + cost_penalty * cost_loss + Fl_base * soft_ineq_penalty + eq_imbalance * soft_eq_penalty\n",
    "\n",
    "            start_BP = time.time()\n",
    "            loss.backward()\n",
    "            end_BP = time.time()\n",
    "            backward_batch = end_BP - start_BP\n",
    "            #print('Backward pass:', end_BP - start_BP)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            relcotrain_epoch[i,:] = batch_relco.detach()\n",
    "            imbalancetrain_epoch[i,:] = imbalancetrain_batch.detach()\n",
    "            infeasibilitytrain_epoch[i,:] = infeasibility_batch.detach()\n",
    "            infeasibility_count[i,:] = infeasible_cases.detach()\n",
    "            line_violation_count[i,:] = count_line_violation.detach()\n",
    "            crit_violation_count[i,:] = count_crit_violation.detach()\n",
    "            #avg_violation_list[i,:] = avg_violation.detach()\n",
    "            count_violation_list[i,:] = count_violation\n",
    "            \n",
    "            violation_percentage[i,:] = batch_percentage_violation\n",
    "            time_contflow[i,:] = time_contflow_batch\n",
    "            memory_contflow[i,:] = memory_contflow_batch\n",
    "            backward_contflow[i,:] = backward_batch\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (Xbatch, cost_scopf_batch) in enumerate(tqdm(test_loader)):\n",
    "                \n",
    "                model.eval() # deactivate dropout\n",
    "\n",
    "                Xbatch = Xbatch.clone().detach().to(device)\n",
    "                cost_scopf_batch = cost_scopf_batch.clone().detach().to(device)\n",
    "\n",
    "                Xtest_batch = ((Xbatch.cpu() * Xstd) + Xmean).to(torch.float32).to(device)\n",
    "\n",
    "                #================== Perform forward pass\n",
    "                clil = (model(Xbatch) + 1) / 2\n",
    "                \n",
    "                #============= soft violation base case\n",
    "                Pg_guess = torch.mul(max_p,clil) \n",
    "                columns_to_sum = [3,4]\n",
    "                sum_result = torch.sum(Pg_guess[:, columns_to_sum], dim=1).unsqueeze(1)\n",
    "                modified_tensor = torch.cat((Pg_guess[:, :3], sum_result, Pg_guess[:, 5:]), dim=1)\n",
    "                Pg_mod = modified_tensor\n",
    "\n",
    "                Pg_train = torch.zeros((batch_size,Nbus)).to(torch.float32)\n",
    "                Pd_train = torch.zeros((batch_size,Nbus)).to(torch.float32)\n",
    "\n",
    "                Pg_train[:,gen_nodes] = Pg_mod.to(torch.float32)\n",
    "                Pd_train[:,load_nodes] = Xtrain_batch.to(torch.float32)\n",
    "\n",
    "                Fl_guess = PTDF@(Pg_train-Pd_train).T\n",
    "                #Fl_base = F.relu(torch.abs(Fl_guess)-max_f).sum()/int(batch_size)\n",
    "\n",
    "                #============= cvxpylayer\n",
    "                Pgi, Fli, Thi, lam1i, lam2i = cvxpylayer0(Xtest_batch, clil, solver_args=ECOS_solver_args)\n",
    "                Pgis = Pgi.T\n",
    "                Flis = Fli.T.to(torch.float32)\n",
    "                \n",
    "                #=============== Without correction layer\n",
    "                Pgis = Pg_guess.T.to(torch.float32)\n",
    "                Flis = Fl_guess.to(torch.float32)\n",
    "\n",
    "                #================ compute cost and infeasibility\n",
    "                infeasibility_batch = torch.sum(lam1i) + torch.sum(lam2i)\n",
    "\n",
    "                batch_cost = torch.matmul(gencost, Pgis)\n",
    "                batch_relco = ((batch_cost - cost_scopf_batch) / cost_scopf_batch) * 100\n",
    "\n",
    "                #===================== compute contingency flows for each sample\n",
    "                imbalancetest_batch = 0\n",
    "\n",
    "                for b in range(tot_lodf_batches):\n",
    "                    lodf_batch = lodf_batch_size\n",
    "                    if b == (tot_lodf_batches - 1): \n",
    "                        lodf_batch = last_batch_size\n",
    "                    left_ones = torch.ones((lodf_batch, 1)).to(device)\n",
    "                    right_ones = torch.ones((int(l * batch_size), 1)).to(device)\n",
    "                    Fli_0 = Flis.expand(lodf_batch, -1, -1).detach()\n",
    "\n",
    "                    index1 = LODF_dict[b][0].detach()\n",
    "                    index2 = LODF_dict[b][1].detach()\n",
    "                    indices = torch.stack((index1, index2)).detach()\n",
    "                    values = LODF_dict[b][2].detach()\n",
    "                    shape = (l, int(l * lodf_batch))\n",
    "                    lodf = torch.sparse_coo_tensor(indices, values, size=shape).T.to(device)\n",
    "\n",
    "                    imbalancetest_batch_loop = (F.relu(torch.abs(Fli_0 + torch.sparse.mm(lodf, Flis).reshape((lodf_batch, l, batch_size))) - max_f))\n",
    "                    imbalancetest_batch += torch.sum(imbalancetest_batch_loop) / int(batch_size)\n",
    "\n",
    "        \n",
    "                relcotest_epoch[i,:] = batch_relco.detach()\n",
    "                imbalancetest_epoch[i,:] = imbalancetest_batch.detach()\n",
    "                infeasibilitytest_epoch[i,:] = infeasibility_batch.detach()\n",
    "            \n",
    "    time_nnSCS = time.time() - time_nnSCS  \n",
    "\n",
    "    imbalancetrain = torch.mean(imbalancetrain_epoch).detach()\n",
    "    imbalancetest = torch.mean(imbalancetest_epoch).detach()\n",
    "    \n",
    "    infeasibilitytrain = torch.mean(infeasibilitytrain_epoch).detach()\n",
    "    infeasibilitytest = torch.mean(infeasibilitytest_epoch).detach()\n",
    "    \n",
    "    relcotrain = torch.mean(relcotrain_epoch).detach()\n",
    "    relcotest = torch.mean(relcotest_epoch).detach()\n",
    "    \n",
    "    #====== convergence criteria\n",
    "    infeasible_cases_tot = torch.sum(infeasibility_count)\n",
    "    line_violation_tot = torch.sum(line_violation_count)\n",
    "    crit_violation_tot = torch.sum(crit_violation_count)\n",
    "    #tot_avg_violation = torch.sum(avg_violation_list).detach()\n",
    "    count_violation_tot = torch.sum(count_violation_list).detach()\n",
    "    \n",
    "    tot_violation_percentage = torch.mean(violation_percentage)\n",
    "    tot_time_contflow = torch.sum(time_contflow)\n",
    "    tot_memory_contflow = torch.mean(memory_contflow)\n",
    "    tot_backward_contflow = torch.sum(backward_contflow)\n",
    "    \n",
    "    #============= \n",
    "    error[epoch,:] = torch.stack((imbalancetrain,imbalancetest)).detach().numpy()\n",
    "    avg_infeasibility[epoch,:] =  torch.stack((infeasibilitytrain,infeasibilitytest)).detach().numpy()\n",
    "    relcost[epoch,:] =  torch.stack((relcotrain,relcotest)).detach().numpy()\n",
    "    \n",
    "    #================= computational graph\n",
    "    time_CF[epoch,:] = tot_time_contflow\n",
    "    memory_CF[epoch,:] = tot_memory_contflow \n",
    "    percentage_CF[epoch,:] = tot_violation_percentage\n",
    "    backward_CF[epoch,:] = tot_backward_contflow\n",
    "\n",
    "    print(epoch, \" | \", round(time_nnSCS,0), \" | \",np.round(error[epoch,:],3), \" | \", np.round(torch.mean(clil[:]).cpu().detach().numpy(),3), \" | \", np.round(avg_infeasibility[epoch,:],3), \" | \", np.round(relcost[epoch,:], 3) )\n",
    "    \n",
    "    #======================= Convergence check infeasible cases\n",
    "    print('current infeasibility:', (infeasible_cases_tot/int(train*Nsamples))*100, '%', 'goal:', 1, '%')\n",
    "    \n",
    "    print('infeasible contingency cases:', line_violation_tot, (line_violation_tot/(Ncontingencies*Nsamples))*100, '%')\n",
    "    print('10%+ contingency cases:', crit_violation_tot, (crit_violation_tot/(Ncontingencies*Nsamples))*100, '%')\n",
    "    print('number of violations:', count_violation_tot)\n",
    "    print('percentage lines not in violation:', tot_violation_percentage, '%')\n",
    "    \n",
    "    print('flow penalty:', flow_penalty*imbalancetrain_batch)\n",
    "    print('infeasible penalty:', infeasibility_penalty*infeasibility_loss)\n",
    "    print('cost penalty:', cost_penalty*cost_loss)\n",
    "    print('soft ineq loss:', soft_ineq_penalty*Fl_base)\n",
    "    print('soft eq loss:', soft_eq_penalty*eq_imbalance)\n",
    "    #print('extra;', loss_term)\n",
    "    \n",
    "        \n",
    "end = time.time()\n",
    "print('RAM Used (GB):', psutil.virtual_memory()[3]/1000000000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6281bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if case == 'N-1':\n",
    "    file_path = 'trained models/correction/without correct/ieee39_N1.pt'\n",
    "if case == 'N-2':\n",
    "    file_path = 'trained models/correction/without correct/ieee39_N2.pt'\n",
    "if case == 'N-3':\n",
    "    file_path = 'trained models/correction/without correct/ieee39_N3.pt'\n",
    "if case == 'N-4':\n",
    "    file_path = 'trained models/correction/without correct/ieee39_N4.pt'\n",
    "if case == 'N-5':\n",
    "    file_path = 'trained models/correction/without correct/ieee39_N5.pt'\n",
    "\n",
    "# Save the model\n",
    "#torch.save(model.state_dict(), file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34b75455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM Used (GB): 6.10349056\n"
     ]
    }
   ],
   "source": [
    "if case == 'N-1':\n",
    "    file_path = 'trained models/correction/ieee39_N1_MC_200.pt'\n",
    "if case == 'N-2':\n",
    "    file_path = 'trained models/correction/ieee39_N2_MC_500.pt'\n",
    "if case == 'N-3':\n",
    "    file_path = 'trained models/correction/ieee39_N3_MC_500.pt'\n",
    "if case == 'N-4':\n",
    "    file_path = 'trained models/correction/ieee39_N4_MC_500.pt'\n",
    "if case == 'N-5':\n",
    "    file_path = 'trained models/correction/ieee39_N5_MC_500.pt'\n",
    "\n",
    "# Load the saved model state dictionary\n",
    "model.load_state_dict(torch.load(file_path))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "print('RAM Used (GB):', psutil.virtual_memory()[3]/1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de99ed41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#======== get values for computational graph\n",
    "comp_graph = np.stack((percentage_CF, time_CF, memory_CF, backward_CF))\n",
    "\n",
    "if case == 'N-1':\n",
    "    file_path = 'Computational graph data/IEEE39/reduced graph/Comp_graphN1.pkl'\n",
    "if case == 'N-2':\n",
    "    file_path = 'Computational graph data/IEEE39/reduced graph/Comp_graphN2.pkl'\n",
    "if case == 'N-3':\n",
    "    file_path = 'Computational graph data/IEEE39/reduced graph/Comp_graphN3.pkl'\n",
    "if case == 'N-4':\n",
    "    file_path = 'Computational graph data/IEEE39/reduced graph/Comp_graphN4.pkl'\n",
    "if case == 'N-5':\n",
    "    file_path = 'Computational graph data/IEEE39/reduced graph/Comp_graphN5.pkl'\n",
    "\n",
    "# Save the dictionary to the specified file path using pickle\n",
    "with open(file_path, 'wb') as f:\n",
    "    pickle.dump(comp_graph, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b1c43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "totaltime = start - end\n",
    "print(\"total time:\", totaltime)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(error[:l,0], 'r', label='Training error')\n",
    "plt.plot(error[:l,1], 'b', label='Testing error')\n",
    "#new_list = 10*range(0, 10,l+1)\n",
    "#plt.xticks(new_list)\n",
    "plt.ylabel('Errors')\n",
    "plt.xlabel('Iterations')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed1cd80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nsamples = 1000\n",
    "#X = np.array(data['load']['p'])/Sbase*ls.kumaraswamymontecarlo(1.6, 2.8, 0.75, LB, UB, Nsamples).T\n",
    "file_path = 'trained models/training data/Xtest1.pkl' \n",
    "with open(file_path, 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "Xtrain = torch.tensor(X[0:int(Nsamples),:],dtype = torch.float32)        \n",
    "Xtrain_transpose = Xtrain.transpose(0,1)\n",
    "#Xtest = torch.tensor(X[int(train*Nsamples):,:],dtype = torch.float64)\n",
    "#standardise\n",
    "Xmin, Xmax,Xmean,Xstd = np.min(X, axis = 0),np.max(X, axis = 0),np.mean(X, axis = 0),np.std(X, axis = 0)\n",
    "Xscal = torch.tensor(( X - Xmean ) / Xstd, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2249343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:05<00:00, 187.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3478, 199, 3185, 2586, 3162, 2563, 3262, 3244, 3440, 2608, 3187, 3156, 2557, 2580, 662, 1904, 3437, 197, 2859, 2857]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:44<00:00, 22.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3478, 199, 3185, 2586, 3162, 2563, 3262, 3244, 3440, 2608, 3187, 3156, 2557, 2580, 662, 1904, 3437, 197, 2859, 2857, 35, 39, 53, 55, 40, 36, 32, 37, 38, 45, 47, 33, 1066, 1271, 1295, 1054, 1067, 1272, 51, 46]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|███████████████████▊                                                           | 251/1000 [00:24<00:56, 13.22it/s]C:\\Users\\bgiraud\\AppData\\Local\\anaconda3\\envs\\learnSCOPF\\lib\\site-packages\\cvxpy\\problems\\problem.py:1385: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:23<00:00, 11.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3478, 199, 3185, 2586, 3162, 2563, 3262, 3244, 3440, 2608, 3187, 3156, 2557, 2580, 662, 1904, 3437, 197, 2859, 2857, 35, 39, 53, 55, 40, 36, 32, 37, 38, 45, 47, 33, 1066, 1271, 1295, 1054, 1067, 1272, 51, 46, 511, 1858, 1086, 395, 2911, 2866, 2936]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:42<00:00,  9.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contingency screening time: 245.28280377388\n"
     ]
    }
   ],
   "source": [
    "#=============== Benchmark #1 contingency screening N-k\n",
    "benchmark = 'SCOPF3screening' #'SCOPF'\n",
    "\n",
    "if benchmark == 'SCOPF':\n",
    "    lcontingencies = range(l)\n",
    "    islanding_cases = [15,20]\n",
    "    lcontingencies_filtered = [l for l in lcontingencies if l not in islanding_cases]\n",
    "    lcontingencies = lcontingencies_filtered\n",
    "    Nk_contingencies = lcontingencies\n",
    "elif benchmark == 'SCOPF1screening':\n",
    "    Nk_contingencies = np.array(list(itertools.combinations(range(Nlines),1)))\n",
    "elif benchmark == 'SCOPF2screening':\n",
    "    Nk_contingencies = np.array(list(itertools.combinations(range(Nlines),2)))\n",
    "elif benchmark == 'SCOPF3screening':\n",
    "    Nk_contingencies = np.array(list(itertools.combinations(range(Nlines),3)))\n",
    "elif benchmark == 'SCOPF4screening':\n",
    "    Nk_contingencies = np.array(list(itertools.combinations(range(Nlines),4)))\n",
    "elif benchmark == 'SCOPF5screening':\n",
    "    Nk_contingencies = np.array(list(itertools.combinations(range(Nlines),5)))\n",
    "\n",
    "Pgisscopf = torch.zeros(Ngens,int(Nsamples),dtype = torch.float32)\n",
    "Fl0sscopf = torch.zeros(Nlines,int(Nsamples),dtype = torch.float32)\n",
    "#Flcs = torch.zeros(Nlines,len(Nk_contingencies),int(Nsamples),dtype = torch.float32)\n",
    "lam1s = torch.zeros(Nbus,int(Nsamples),dtype = torch.float32)\n",
    "lam2s = torch.zeros(Nbus,int(Nsamples),dtype = torch.float32)\n",
    "#lam1sc = torch.zeros(Nbus,len(Nk_contingencies),int(Nsamples),dtype = torch.float32)\n",
    "#lam2sc = torch.zeros(Nbus,len(Nk_contingencies),int(Nsamples),dtype = torch.float32)\n",
    "Th0 = torch.zeros(Nbus,int(Nsamples),dtype = torch.float32)\n",
    "\n",
    "#======== remove islanding cases from contingency list\n",
    "if benchmark != 'SCOPF':\n",
    "    lcontingencies = [0]\n",
    "screening_iterations = 4\n",
    "\n",
    "time_scopfs = time.time()\n",
    "\n",
    "for i in range(screening_iterations):\n",
    "    imbalance_list = []\n",
    "    Nk_zeros = list(range(len(Nk_contingencies)))\n",
    "    zero_indices = [l for l in Nk_zeros if l not in lcontingencies]\n",
    "    print(lcontingencies)\n",
    "\n",
    "    if benchmark == 'SCOPF':\n",
    "        problem0scopf, Pd0scopf, cg0scopf, Pgiscopf, Fl0scopf, Th0scopf, Flc, lam1, lam2, lam1c, lam2c = cs.create_scopf(data, lcontingencies)\n",
    "    elif benchmark == 'SCOPF1screening':\n",
    "        problem0scopf, Pd0scopf, cg0scopf, Pgiscopf, Fl0scopf, Th0scopf, Flc, lam1, lam2, lam1c, lam2c = cs.create_scopf1_screening(data, lcontingencies)\n",
    "    elif benchmark == 'SCOPF2screening':\n",
    "        problem0scopf, Pd0scopf, cg0scopf, Pgiscopf, Fl0scopf, Th0scopf, Flc, lam1, lam2, lam1c, lam2c = cs.create_scopf2_screening(data, lcontingencies)\n",
    "    elif benchmark == 'SCOPF3screening':\n",
    "        problem0scopf, Pd0scopf, cg0scopf, Pgiscopf, Fl0scopf, Th0scopf, Flc, lam1, lam2, lam1c, lam2c = cs.create_scopf3_screening(data, lcontingencies)\n",
    "    elif benchmark == 'SCOPF4screening':\n",
    "        problem0scopf, Pd0scopf, cg0scopf, Pgiscopf, Fl0scopf, Th0scopf, Flc, lam1, lam2, lam1c, lam2c = cs.create_scopf4_screening(data, lcontingencies)\n",
    "    elif benchmark == 'SCOPF5screening':\n",
    "        problem0scopf, Pd0scopf, cg0scopf, Pgiscopf, Fl0scopf, Th0scopf, Flc, lam1, lam2, lam1c, lam2c = cs.create_scopf5_screening(data, lcontingencies)\n",
    "   \n",
    "\n",
    "    cg0scopf.value = np.array(data['gen']['cost'])\n",
    "    for entry in tqdm(range(int(Nsamples)),position=0, leave=True):   \n",
    "        Pd0scopf.value = X[entry,:] #np.array(data['load']['p'])/Sbase    \n",
    "        try:\n",
    "            solution  = problem0scopf.solve(solver=cp.ECOS)    # cp.MOSEK\n",
    "            if problem0scopf.status in [\"infeasible\", \"unbounded\"]:\n",
    "                # Otherwise, problem.value is inf or -inf, respectively.\n",
    "                print(\"Problem infeasible or unbounded\")\n",
    "                #print(\"Optimal value: %s\" % problem0scopf.value)\n",
    "            Pgisscopf[:,entry] = torch.tensor(Pgiscopf.value)\n",
    "            Fl0sscopf[:,entry] = torch.tensor(Fl0scopf.value)\n",
    "            Th0[:,entry] = torch.tensor(Th0scopf.value)\n",
    "\n",
    "            lam1s[:,entry] = torch.tensor(lam1.value)\n",
    "            lam2s[:,entry] = torch.tensor(lam2.value)\n",
    "        except cp.error.SolverError:\n",
    "            Pgisscopf[:,entry] = Pgisscopf[:,(entry-1)]\n",
    "            Fl0sscopf[:,entry] = Fl0sscopf[:,(entry-1)]\n",
    "            Th0[:,entry] = Th0[:,(entry-1)]\n",
    "\n",
    "            lam1s[:,entry] = lam1s[:,(entry-1)]\n",
    "            lam2s[:,entry] = lam2s[:,(entry-1)]\n",
    "            \n",
    "            if i == (screening_iterations - 1):\n",
    "                Xtrain[entry,:] = Xtrain[(entry-1),:]\n",
    "            \n",
    "            print(\"Solver unable to solve for entry:\", entry)\n",
    "            continue\n",
    "        \n",
    "    \n",
    "    cost_scopf = torch.matmul(gencost,Pgisscopf)\n",
    "    \n",
    "    if benchmark == 'SCOPF':\n",
    "        break\n",
    "\n",
    "    #============ compute Nk flows\n",
    "    Flis = Fl0sscopf\n",
    "                \n",
    "    for b in (range(tot_lodf_batches)):\n",
    "\n",
    "        lodf_batch = lodf_batch_size\n",
    "        if b == (tot_lodf_batches - 1):\n",
    "            lodf_batch = last_batch_size\n",
    "        left_ones = torch.ones((lodf_batch,1))\n",
    "        right_ones = torch.ones((int(l*batch_size),1))\n",
    "        Fli_0 = Flis.expand(lodf_batch, -1, -1)\n",
    "\n",
    "        summed_tensor = torch.zeros(lodf_batch).to(device)\n",
    "\n",
    "        index1 = LODF_dict[b][0].detach()\n",
    "        index2 = LODF_dict[b][1].detach()\n",
    "        indices = torch.stack((index1,index2)).detach()\n",
    "        values = LODF_dict[b][2].detach()\n",
    "        shape = (l,int(l*lodf_batch))\n",
    "        lodf = torch.sparse_coo_tensor(indices,values,size = shape, requires_grad = False).T.to(device).detach()\n",
    "\n",
    "        for k in range(int(Nsamples/batch_size)):\n",
    "            start = k*batch_size\n",
    "            end = (k+1)*batch_size\n",
    "            imbalancetrain_batch_loop = (F.relu(torch.abs(Fli_0[:,:,start:end] + torch.sparse.mm(lodf,Flis[:,start:end]).reshape((lodf_batch,l,batch_size)))- max_f)).detach()\n",
    "\n",
    "            #======= get indices of contingencies of highest violations\n",
    "            summed_tensor += torch.sum(torch.sum(imbalancetrain_batch_loop, dim=1), dim=1)\n",
    "\n",
    "        if b == 0:\n",
    "            desired_length = len(summed_tensor)\n",
    "\n",
    "        padding_length = desired_length - len(summed_tensor)\n",
    "        padded_tensor = F.pad(summed_tensor, (0, padding_length), value=0)\n",
    "        imbalance_list.extend(padded_tensor)\n",
    "\n",
    "\n",
    "    indices_per_iteration = 20 # len(summed_tensor)\n",
    "\n",
    "    stacked_imbalance = torch.stack(imbalance_list, dim=0)\n",
    "    highest_indices = (np.argsort(stacked_imbalance.cpu())[-indices_per_iteration:]).tolist()\n",
    "    new_indices = [idx for idx in highest_indices if idx not in lcontingencies] # Filter out the indices that are already in the list\n",
    "    lcontingencies.extend(new_indices) # Append the new indices to the list\n",
    "\n",
    "    \n",
    "time_scopfs = time.time() - time_scopfs\n",
    "print('contingency screening time:', time_scopfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfc4f987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infeasible base cases: tensor(0.)\n",
      "percentage base infeasibility: tensor(0.) %\n",
      "total average cost: tensor(2144.4783)\n",
      "RAM Used (GB): 6.25453056\n"
     ]
    }
   ],
   "source": [
    "##============= check infeasibility\n",
    "infeasibility_base = (lam1s + lam2s).permute(1,0)\n",
    "infeasibility_base[abs(infeasibility_base) < 1e-4] = 0\n",
    "infeasible_buses_base = (abs(infeasibility_base) > 1e-4) #0.001*load_profile) # 10% of total load available for generation/load shedding\n",
    "scopf_cost_average = cost_scopf.mean()\n",
    "\n",
    "infeasible_cases_base = torch.zeros((Nsamples,1))\n",
    "row_has_true_base = torch.any(infeasible_buses_base, axis=1)\n",
    "infeasible_cases_base[row_has_true_base] = True\n",
    "infeasible_cases_tot_base = infeasible_cases_base.sum()\n",
    "print('infeasible base cases:', infeasible_cases_tot_base)\n",
    "print('percentage base infeasibility:', (infeasible_cases_tot_base/(Nsamples))*100, '%')\n",
    "print('total average cost:', scopf_cost_average)\n",
    "\n",
    "print('RAM Used (GB):', psutil.virtual_memory()[3]/1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b8f6254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infeasible N-k contingency cases: tensor(179971) tensor(3.2986) %\n",
      "10%+ contingency cases: tensor(43209) tensor(0.7920) %\n"
     ]
    }
   ],
   "source": [
    "#=========== check how many violations N-k\n",
    "Flis = Fl0sscopf\n",
    "    \n",
    "count_crit_violation = 0\n",
    "count_line_violation = 0\n",
    "\n",
    "for b in (range(tot_lodf_batches)):\n",
    "    lodf_batch = lodf_batch_size\n",
    "    if b == (tot_lodf_batches - 1): \n",
    "        lodf_batch = last_batch_size\n",
    "    left_ones = torch.ones((lodf_batch,1))\n",
    "    right_ones = torch.ones((int(l*batch_size),1))\n",
    "    Fli_0 = Flis.expand(lodf_batch, -1, -1)\n",
    "\n",
    "    index1 = LODF_dict[b][0].detach()\n",
    "    index2 = LODF_dict[b][1].detach()\n",
    "    indices = torch.stack((index1,index2)).detach()\n",
    "    values = LODF_dict[b][2].detach()\n",
    "    shape = (l,int(l*lodf_batch))\n",
    "    lodf = torch.sparse_coo_tensor(indices,values,size = shape, requires_grad = False).T.to(device).detach()\n",
    "\n",
    "    # lodf[(l*Ncont),l] x flis[l,batch_size]\n",
    "    imbalancetrain_batch_loop = (F.relu(torch.abs(Fli_0 + torch.sparse.mm(lodf,Flis).reshape((lodf_batch,l,Nsamples)))- max_f)).detach()\n",
    "    count_crit_violation_loop = torch.sum(imbalancetrain_batch_loop > tollerance_crit*max_f, dim=1)\n",
    "    count_line_violation_loop = torch.sum(imbalancetrain_batch_loop > 0, dim=1)\n",
    "    violation_indices_scopf = torch.nonzero(count_line_violation_loop, as_tuple=False)\n",
    "    \n",
    "\n",
    "    count_crit_violation += torch.count_nonzero(count_crit_violation_loop)\n",
    "    count_line_violation += torch.count_nonzero(count_line_violation_loop)\n",
    "\n",
    "crit_violation_tot = count_crit_violation\n",
    "line_violation_tot = count_line_violation\n",
    "\n",
    "print('infeasible N-k contingency cases:', line_violation_tot, (line_violation_tot/(Ncontingencies*Nsamples))*100, '%')\n",
    "print('10%+ contingency cases:', crit_violation_tot, (crit_violation_tot/(Ncontingencies*Nsamples))*100, '%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb85d397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It |   Time   |    Imbalance    | Mod out  |   Infeasibility   |  Rel cost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:06<00:00,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  |  6.0  |  [168.42]  |  0.478  |  [-0.]  |  [0.554]\n",
      "method time: 6.014103889465332\n",
      "current infeasibility: tensor(0.) % goal: 1 %\n",
      "infeasible contingency cases: tensor(117624.) tensor(2.1559) %\n",
      "10%+ infeasible cases: tensor(50286.) tensor(0.9217) %\n",
      "number of violations: tensor(145663.)\n",
      "total cost: 2155.0712890625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_test = time.time()\n",
    "#============== Initialize training testing data\n",
    "testing_dataset = Data.TensorDataset(Xscal, cost_scopf)\n",
    "test_loader = Data.DataLoader(dataset=testing_dataset, batch_size=batch_size, shuffle=False)  # Note: shuffle=False for testing\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#================ Initialize tensors\n",
    "error = np.zeros(shape=(Niterations,1))\n",
    "avg_infeasibility = np.zeros(shape=(Niterations,1))\n",
    "relcost = np.zeros(shape=(Niterations,1))\n",
    "cost = np.zeros(shape=(Niterations,1))\n",
    "\n",
    "Pgis = torch.zeros(size = (Ngens,int(batch_size)),dtype = torch.float32)\n",
    "Flis = torch.zeros(size = (Nlines,int(batch_size)),dtype = torch.float32)\n",
    "clil = torch.zeros(size = (Ngens,int(batch_size)),dtype = torch.float32)\n",
    "infeasibility = torch.zeros(int(batch_size), requires_grad = False)\n",
    "\n",
    "relcotest_epoch = torch.zeros(size = (int(Nsamples/batch_size),int(batch_size)),dtype = torch.float32, requires_grad = False)\n",
    "cost_epoch = torch.zeros(size = (int(Nsamples/batch_size),int(batch_size)),dtype = torch.float32, requires_grad = False)\n",
    "\n",
    "imbalancetest_epoch = torch.zeros(size = (round((Nsamples)/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "\n",
    "infeasibilitytest_epoch = torch.zeros(size = (int(Nsamples/batch_size),Nbus),dtype = torch.float32, requires_grad = False)\n",
    "infeasibility_count = torch.zeros(size = (int(Nsamples/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "\n",
    "line_violation_count = torch.zeros(size = (int(Nsamples/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "crit_violation_count = torch.zeros(size = (int(Nsamples/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "avg_violation_list = torch.zeros(size = (int(Nsamples/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "count_violation_list = torch.zeros(size = (int(Nsamples/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "\n",
    "print(\"It |   Time   |    Imbalance    | Mod out  |   Infeasibility   |  Rel cost\")\n",
    "for epoch in range(1):    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for i, (Xbatch, cost_scopf_batch) in enumerate(tqdm(test_loader)):\n",
    "                \n",
    "                model.eval() # deactivate dropout, when I changed this, testing error stayed same\n",
    "\n",
    "                Xbatch = Xbatch.clone().detach()\n",
    "                cost_scopf_batch = cost_scopf_batch.clone().detach()\n",
    "\n",
    "                Xtest_batch = ((Xbatch * Xstd)+Xmean).to(torch.float32)\n",
    "\n",
    "                #================== Perform forward pass\n",
    "                #clil = (model(Xbatch))\n",
    "                clil = (model(Xbatch)+1)/2\n",
    "                \n",
    "                #============= cvxpylayer                \n",
    "                Pgi, Fli, Thi, lam1i, lam2i = cvxpylayer0(Xtest_batch, clil, solver_args=ECOS_solver_args)\n",
    "                Pgis = Pgi.T\n",
    "                Flis = Fli.T.to(torch.float32)\n",
    "                \n",
    "                #================ compute cost and infeasibility\n",
    "                infeasibility_batch = sum(lam1i.detach().numpy()) + sum(lam2i.detach().numpy())\n",
    "                infeasibility_batch = torch.tensor(infeasibility_batch)\n",
    "                infeasibility_tensor = lam1i + lam2i\n",
    "                infeasibility_tensor[abs(infeasibility_tensor) < 1e-5 ] = 0\n",
    "                infeasible_buses = (abs(infeasibility_tensor) > 1e-5)\n",
    "                \n",
    "                infeasible_cases = torch.zeros((batch_size,1))\n",
    "                row_has_true = torch.any(infeasible_buses, axis=1)\n",
    "                infeasible_cases[row_has_true] = True\n",
    "                infeasible_cases = infeasible_cases.sum()\n",
    "                               \n",
    "                batch_cost = torch.matmul(gencost,Pgis)\n",
    "                batch_relco = ((batch_cost-cost_scopf_batch)/cost_scopf_batch)*100\n",
    "\n",
    "                #===================== compute contingency flows for each sample\n",
    "                imbalancetest_batch = 0\n",
    "                count_crit_violation = 0\n",
    "                count_line_violation = 0\n",
    "                count_violation = 0\n",
    "\n",
    "                for b in (range(tot_lodf_batches)):\n",
    "                    lodf_batch = lodf_batch_size\n",
    "                    if b == (tot_lodf_batches - 1): \n",
    "                        lodf_batch = last_batch_size\n",
    "                    left_ones = torch.ones((lodf_batch_size,1))\n",
    "                    right_ones = torch.ones((int(l*batch_size),1))\n",
    "                    Fli_0 = Flis.expand(lodf_batch, -1, -1)#.contiguous().view(-1, len(Flis[1])).detach()#.to(torch.float32).clone()\n",
    "\n",
    "                    index1 = LODF_dict[b][0].detach()\n",
    "                    index2 = LODF_dict[b][1].detach()\n",
    "                    indices = torch.stack((index1,index2)).detach()\n",
    "                    values = LODF_dict[b][2].detach()\n",
    "                    shape = (l,int(l*lodf_batch))\n",
    "                    lodf = torch.sparse_coo_tensor(indices,values,size = shape).T.to(device)\n",
    "\n",
    "                    imbalancetest_batch_loop = (F.relu(torch.abs(Fli_0 + torch.sparse.mm(lodf,Flis).reshape((lodf_batch,l,batch_size)))- max_f))\n",
    "                    imbalancetest_batch_loop[abs(imbalancetest_batch_loop) < 1e-5] = 0\n",
    "                    \n",
    "                    count_crit_violation_loop = torch.sum(imbalancetest_batch_loop > tollerance_crit*max_f, dim=1)\n",
    "                    count_line_violation_loop = torch.sum(imbalancetest_batch_loop > 0, dim=1)\n",
    "                    batch_nonzero_indices = torch.nonzero(count_line_violation_loop, as_tuple=False)\n",
    "                    batch_nonzero_indices[:, 1] += i*100\n",
    "                    violation_indices_nn = torch.cat((violation_indices_nn, batch_nonzero_indices), dim=0)\n",
    "                    \n",
    "                    imbalancetest_batch_loop = imbalancetest_batch_loop.reshape(lodf_batch,int(l*batch_size))\n",
    "                    count_violation_loop = torch.count_nonzero(imbalancetest_batch_loop)\n",
    "                \n",
    "                    imbalancetest_batch += torch.sum(imbalancetest_batch_loop)/int(batch_size)\n",
    "                \n",
    "                    count_violation += count_violation_loop\n",
    "                    count_crit_violation += torch.count_nonzero(count_crit_violation_loop.detach())\n",
    "                    count_line_violation += torch.count_nonzero(count_line_violation_loop.detach())\n",
    "        \n",
    "                relcotest_epoch[i,:] = batch_relco.detach()\n",
    "                cost_epoch[i,:] = batch_cost.detach()\n",
    "                imbalancetest_epoch[i,:] = imbalancetest_batch.detach()\n",
    "                infeasibilitytest_epoch[i,:] = infeasibility_batch.detach()\n",
    "                \n",
    "                infeasibility_count[i,:] = infeasible_cases.detach()\n",
    "                line_violation_count[i,:] = count_line_violation.detach()\n",
    "                crit_violation_count[i,:] = count_crit_violation.detach()\n",
    "                #avg_violation_list[i,:] = avg_violation.detach()\n",
    "                count_violation_list[i,:] = count_violation\n",
    "             \n",
    "\n",
    "    imbalancetest = torch.mean(imbalancetest_epoch).detach()\n",
    "\n",
    "    infeasibilitytest = torch.mean(infeasibilitytest_epoch).detach()\n",
    "\n",
    "    relcotest = torch.mean(relcotest_epoch).detach()\n",
    "    costtest = torch.mean(cost_epoch).detach()\n",
    "    \n",
    "    #====== convergence criteria\n",
    "    infeasible_cases_tot = torch.sum(infeasibility_count).detach()\n",
    "    line_violation_tot = torch.sum(line_violation_count)\n",
    "    crit_violation_tot = torch.sum(crit_violation_count)\n",
    "    #tot_avg_violation = torch.sum(avg_violation_list).detach()\n",
    "    count_violation_tot = torch.sum(count_violation_list).detach()\n",
    "    \n",
    "    error[epoch,:] = imbalancetest.detach().numpy()\n",
    "    avg_infeasibility[epoch,:] =  infeasibilitytest.detach().numpy()\n",
    "    relcost[epoch,:] =  relcotest.detach().numpy()\n",
    "    cost[epoch,:] =  costtest.detach().numpy()\n",
    "    end_test = time.time()\n",
    "    time_test = end_test-start_test\n",
    "\n",
    "    print(epoch, \" | \", round(time_test,0), \" | \",np.round(error[epoch,:],3), \" | \", np.round(torch.mean(clil).detach().numpy(),3), \" | \", np.round(avg_infeasibility[epoch,:],3), \" | \", np.round(relcost[epoch,:], 3) )\n",
    "        \n",
    "\n",
    "print('method time:', time_test)\n",
    "\n",
    "print('current infeasibility:', (infeasible_cases_tot/Nsamples)*100, '%', 'goal:', 1, '%')\n",
    "    \n",
    "#print('current avg violation:', (tot_avg_violation/initial_violation_count),'pu', 'goal', 0.01*max_f, 'pu')\n",
    "print('infeasible contingency cases:', line_violation_tot, (line_violation_tot/(Ncontingencies*Nsamples))*100, '%')\n",
    "print('10%+ infeasible cases:', crit_violation_tot, (crit_violation_tot/(Ncontingencies*Nsamples))*100, '%')\n",
    "print('number of violations:', count_violation_tot)\n",
    "print('total cost:', cost.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cfb67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#============== Create new training data (with high variability from RES)\n",
    "file_path = 'trained models/training data/Xtest2.pkl' \n",
    "with open(file_path, 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "Xtrain = torch.tensor(X[0:int(Nsamples),:],dtype = torch.float32)        \n",
    "Xtrain_transpose = Xtrain.transpose(0,1)\n",
    "#Xtest = torch.tensor(X[int(train*Nsamples):,:],dtype = torch.float64)\n",
    "#standardise\n",
    "Xmin, Xmax,Xmean,Xstd = np.min(X, axis = 0),np.max(X, axis = 0),np.mean(X, axis = 0),np.std(X, axis = 0)\n",
    "Xscal = torch.tensor(( X - Xmean ) / Xstd, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d036af4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============== Benchmark #2 heuristic approach\n",
    "if case == 'N-1':\n",
    "    benchmark = 'SCOPF1screening'\n",
    "\n",
    "if benchmark == 'SCOPF1screening':\n",
    "    Nk_contingencies = np.array(list(itertools.combinations(range(Nlines),1)))\n",
    "elif benchmark == 'SCOPF2screening':\n",
    "    Nk_contingencies = np.array(list(itertools.combinations(range(Nlines),2)))\n",
    "elif benchmark == 'SCOPF3screening':\n",
    "    Nk_contingencies = np.array(list(itertools.combinations(range(Nlines),3)))\n",
    "elif benchmark == 'SCOPF4screening':\n",
    "    Nk_contingencies = np.array(list(itertools.combinations(range(Nlines),4)))\n",
    "\n",
    "Pgisscopf = torch.zeros(Ngens,int(Nsamples),dtype = torch.float32)\n",
    "Fl0sscopf = torch.zeros(Nlines,int(Nsamples),dtype = torch.float32)\n",
    "#Flcs = torch.zeros(Nlines,len(Nk_contingencies),int(Nsamples),dtype = torch.float32)\n",
    "lam1s = torch.zeros(Nbus,int(Nsamples),dtype = torch.float32)\n",
    "lam2s = torch.zeros(Nbus,int(Nsamples),dtype = torch.float32)\n",
    "#lam1sc = torch.zeros(Nbus,len(Nk_contingencies),int(Nsamples),dtype = torch.float32)\n",
    "#lam2sc = torch.zeros(Nbus,len(Nk_contingencies),int(Nsamples),dtype = torch.float32)\n",
    "Th0 = torch.zeros(Nbus,int(Nsamples),dtype = torch.float32)\n",
    "\n",
    "time_scopfs = time.time()\n",
    "\n",
    "Nk_zeros = list(range(len(Nk_contingencies)))\n",
    "zero_indices = [l for l in Nk_zeros if l not in lcontingencies]\n",
    "print(lcontingencies)\n",
    "\n",
    "\n",
    "if benchmark == 'SCOPF1screening':\n",
    "    problem0scopf, Pd0scopf, cg0scopf, Pgiscopf, Fl0scopf, Th0scopf, Flc, lam1, lam2, lam1c, lam2c = cs.create_scopf1_screening(data, lcontingencies)\n",
    "elif benchmark == 'SCOPF2screening':\n",
    "    problem0scopf, Pd0scopf, cg0scopf, Pgiscopf, Fl0scopf, Th0scopf, Flc, lam1, lam2, lam1c, lam2c = cs.create_scopf2_screening(data, lcontingencies)\n",
    "elif benchmark == 'SCOPF3screening':\n",
    "    problem0scopf, Pd0scopf, cg0scopf, Pgiscopf, Fl0scopf, Th0scopf, Flc, lam1, lam2, lam1c, lam2c = cs.create_scopf3_screening(data, lcontingencies)\n",
    "elif benchmark == 'SCOPF4screening':\n",
    "    problem0scopf, Pd0scopf, cg0scopf, Pgiscopf, Fl0scopf, Th0scopf, Flc, lam1, lam2, lam1c, lam2c = cs.create_scopf4_screening(data, lcontingencies)\n",
    "\n",
    "cg0scopf.value = np.array(data['gen']['cost'])\n",
    "for entry in tqdm(range(int(Nsamples)),position=0, leave=True):   \n",
    "    Pd0scopf.value = X[entry,:] #np.array(data['load']['p'])/Sbase    \n",
    "    try:\n",
    "        solution  = problem0scopf.solve(solver=cp.ECOS)    # cp.MOSEK\n",
    "        if problem0scopf.status in [\"infeasible\", \"unbounded\"]:\n",
    "            # Otherwise, problem.value is inf or -inf, respectively.\n",
    "            print(\"Problem infeasible or unbounded\")\n",
    "            #print(\"Optimal value: %s\" % problem0scopf.value)\n",
    "        Pgisscopf[:,entry] = torch.tensor(Pgiscopf.value)\n",
    "        Fl0sscopf[:,entry] = torch.tensor(Fl0scopf.value)\n",
    "        Th0[:,entry] = torch.tensor(Th0scopf.value)\n",
    "\n",
    "        lam1s[:,entry] = torch.tensor(lam1.value)\n",
    "        lam2s[:,entry] = torch.tensor(lam2.value)\n",
    "    except cp.error.SolverError:\n",
    "        Pgisscopf[:,entry] = Pgisscopf[:,(entry-1)]\n",
    "        Fl0sscopf[:,entry] = Fl0sscopf[:,(entry-1)]\n",
    "        Th0[:,entry] = Th0[:,(entry-1)]\n",
    "\n",
    "        lam1s[:,entry] = lam1s[:,(entry-1)]\n",
    "        lam2s[:,entry] = lam2s[:,(entry-1)]\n",
    "\n",
    "        Xtrain[entry,:] = Xtrain[(entry-1),:]\n",
    "\n",
    "        print(\"Solver unable to solve for entry:\", entry)\n",
    "        continue\n",
    "\n",
    "cost_scopf = torch.matmul(gencost,Pgisscopf)\n",
    "\n",
    "#============ compute Nk flows\n",
    "Flis = Fl0sscopf\n",
    "\n",
    "count_crit_violation = 0\n",
    "count_line_violation = 0\n",
    "\n",
    "for b in (range(tot_lodf_batches)):\n",
    "\n",
    "        lodf_batch = lodf_batch_size\n",
    "        if b == (tot_lodf_batches - 1):\n",
    "            lodf_batch = last_batch_size\n",
    "        left_ones = torch.ones((lodf_batch,1))\n",
    "        right_ones = torch.ones((int(l*batch_size),1))\n",
    "        Fli_0 = Flis.expand(lodf_batch, -1, -1)\n",
    "\n",
    "        summed_tensor = torch.zeros(lodf_batch).to(device)\n",
    "\n",
    "        index1 = LODF_dict[b][0].detach()\n",
    "        index2 = LODF_dict[b][1].detach()\n",
    "        indices = torch.stack((index1,index2)).detach()\n",
    "        values = LODF_dict[b][2].detach()\n",
    "        shape = (l,int(l*lodf_batch))\n",
    "        lodf = torch.sparse_coo_tensor(indices,values,size = shape, requires_grad = False).T.to(device).detach()\n",
    "\n",
    "        for k in range(int(Nsamples/batch_size)):\n",
    "            start = k*batch_size\n",
    "            end = (k+1)*batch_size\n",
    "            imbalancetrain_batch_loop = (F.relu(torch.abs(Fli_0[:,:,start:end] + torch.sparse.mm(lodf,Flis[:,start:end]).reshape((lodf_batch,l,batch_size)))- max_f)).detach()\n",
    "\n",
    "            count_crit_violation_loop = torch.sum(imbalancetrain_batch_loop > tollerance_crit*max_f, dim=1)\n",
    "            count_line_violation_loop = torch.sum(imbalancetrain_batch_loop > 0, dim=1)\n",
    "\n",
    "            count_crit_violation += torch.count_nonzero(count_crit_violation_loop)\n",
    "            count_line_violation += torch.count_nonzero(count_line_violation_loop)\n",
    "\n",
    "crit_violation_tot = count_crit_violation\n",
    "line_violation_tot = count_line_violation\n",
    "\n",
    "print('infeasible N-k contingency cases:', line_violation_tot, (line_violation_tot/(Ncontingencies*Nsamples))*100, '%')\n",
    "print('10%+ contingency cases:', crit_violation_tot, (crit_violation_tot/(Ncontingencies*Nsamples))*100, '%')\n",
    "    \n",
    "time_scopfs = time.time() - time_scopfs\n",
    "print('heuristic approach time:', time_scopfs)\n",
    "\n",
    "##============= check infeasibility\n",
    "infeasibility_base = (lam1s + lam2s).permute(1,0)\n",
    "infeasibility_base[abs(infeasibility_base) < 1e-4] = 0\n",
    "infeasible_buses_base = (abs(infeasibility_base) > 1e-4) #0.001*load_profile) # 10% of total load available for generation/load shedding\n",
    "scopf_cost_average = cost_scopf.mean()\n",
    "\n",
    "infeasible_cases_base = torch.zeros((Nsamples,1))\n",
    "row_has_true_base = torch.any(infeasible_buses_base, axis=1)\n",
    "infeasible_cases_base[row_has_true_base] = True\n",
    "infeasible_cases_tot_base = infeasible_cases_base.sum()\n",
    "print('infeasible base cases:', infeasible_cases_tot_base)\n",
    "print('percentage base infeasibility:', (infeasible_cases_tot_base/(Nsamples))*100, '%')\n",
    "print('total average cost:', scopf_cost_average)\n",
    "\n",
    "print('RAM Used (GB):', psutil.virtual_memory()[3]/1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d097e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_test = time.time()\n",
    "#============== Initialize training testing data\n",
    "testing_dataset = Data.TensorDataset(Xscal, cost_scopf)\n",
    "test_loader = Data.DataLoader(dataset=testing_dataset, batch_size=batch_size, shuffle=False)  # Note: shuffle=False for testing\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#================ Initialize tensors\n",
    "error = np.zeros(shape=(Niterations,1))\n",
    "avg_infeasibility = np.zeros(shape=(Niterations,1))\n",
    "relcost = np.zeros(shape=(Niterations,1))\n",
    "cost = np.zeros(shape=(Niterations,1))\n",
    "\n",
    "Pgis = torch.zeros(size = (Ngens,int(batch_size)),dtype = torch.float32)\n",
    "Flis = torch.zeros(size = (Nlines,int(batch_size)),dtype = torch.float32)\n",
    "clil = torch.zeros(size = (Ngens,int(batch_size)),dtype = torch.float32)\n",
    "infeasibility = torch.zeros(int(batch_size), requires_grad = False)\n",
    "\n",
    "relcotest_epoch = torch.zeros(size = (int(Nsamples/batch_size),int(batch_size)),dtype = torch.float32, requires_grad = False)\n",
    "cost_epoch = torch.zeros(size = (int(Nsamples/batch_size),int(batch_size)),dtype = torch.float32, requires_grad = False)\n",
    "\n",
    "imbalancetest_epoch = torch.zeros(size = (round((Nsamples)/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "\n",
    "infeasibilitytest_epoch = torch.zeros(size = (int(Nsamples/batch_size),Nbus),dtype = torch.float32, requires_grad = False)\n",
    "infeasibility_count = torch.zeros(size = (int(Nsamples/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "\n",
    "line_violation_count = torch.zeros(size = (int(Nsamples/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "crit_violation_count = torch.zeros(size = (int(Nsamples/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "avg_violation_list = torch.zeros(size = (int(Nsamples/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "count_violation_list = torch.zeros(size = (int(Nsamples/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "\n",
    "print(\"It |   Time   |    Imbalance    | Mod out  |   Infeasibility   |  Rel cost\")\n",
    "for epoch in range(1):    \n",
    "    with torch.no_grad():\n",
    "        for i, (Xbatch, cost_scopf_batch) in enumerate(tqdm(test_loader)):\n",
    "                \n",
    "                model.eval() # deactivate dropout, when I changed this, testing error stayed same\n",
    "\n",
    "                Xbatch = Xbatch.clone().detach()\n",
    "                cost_scopf_batch = cost_scopf_batch.clone().detach()\n",
    "\n",
    "                Xtest_batch = ((Xbatch * Xstd)+Xmean).to(torch.float32)\n",
    "            \n",
    "                #================== Perform forward pass\n",
    "                #clil = (model(Xbatch))\n",
    "                clil = (model(Xbatch)+1)/2\n",
    "                \n",
    "                Pgi, Fli, Thi, lam1i, lam2i = cvxpylayer0(Xtest_batch, clil, solver_args=ECOS_solver_args)\n",
    "                Pgis = Pgi.T\n",
    "                Flis = Fli.T.to(torch.float32)\n",
    "                \n",
    "                #================ compute cost and infeasibility\n",
    "                infeasibility_batch = sum(lam1i.detach().numpy()) + sum(lam2i.detach().numpy())\n",
    "                infeasibility_batch = torch.tensor(infeasibility_batch)\n",
    "                infeasibility_tensor = lam1i + lam2i\n",
    "                infeasibility_tensor[abs(infeasibility_tensor) < 1e-5 ] = 0\n",
    "                infeasible_buses = (abs(infeasibility_tensor) > 1e-5)\n",
    "                \n",
    "                infeasible_cases = torch.zeros((batch_size,1))\n",
    "                row_has_true = torch.any(infeasible_buses, axis=1)\n",
    "                infeasible_cases[row_has_true] = True\n",
    "                infeasible_cases = infeasible_cases.sum()\n",
    "                \n",
    "                batch_cost = torch.matmul(gencost,Pgis)\n",
    "                batch_relco = ((batch_cost-cost_scopf_batch)/cost_scopf_batch)*100\n",
    "\n",
    "                #===================== compute contingency flows for each sample\n",
    "                imbalancetest_batch = 0\n",
    "                count_crit_violation = 0\n",
    "                count_line_violation = 0\n",
    "                count_violation = 0\n",
    "\n",
    "                for b in (range(tot_lodf_batches)):\n",
    "                    lodf_batch = lodf_batch_size\n",
    "                    if b == (tot_lodf_batches - 1): \n",
    "                        lodf_batch = last_batch_size\n",
    "                    left_ones = torch.ones((lodf_batch_size,1))\n",
    "                    right_ones = torch.ones((int(l*batch_size),1))\n",
    "                    Fli_0 = Flis.expand(lodf_batch, -1, -1)#.contiguous().view(-1, len(Flis[1])).detach()#.to(torch.float32).clone()\n",
    "                    #mask = mask_base(b, lodf_batch)\n",
    "                    #Fli_0[~mask] = 0\n",
    "\n",
    "                    index1 = LODF_dict[b][0].detach()\n",
    "                    index2 = LODF_dict[b][1].detach()\n",
    "                    indices = torch.stack((index1,index2)).detach()\n",
    "                    values = LODF_dict[b][2].detach()\n",
    "                    shape = (l,int(l*lodf_batch))\n",
    "                    lodf = torch.sparse_coo_tensor(indices,values,size = shape).T.to(device)\n",
    "\n",
    "                    imbalancetest_batch_loop = (F.relu(torch.abs(Fli_0 + torch.sparse.mm(lodf,Flis).reshape((lodf_batch,l,batch_size)))- max_f))\n",
    "                    imbalancetest_batch_loop[abs(imbalancetest_batch_loop) < 1e-5] = 0\n",
    "                    \n",
    "                    count_crit_violation_loop = torch.sum(imbalancetest_batch_loop > tollerance_crit*max_f, dim=1)\n",
    "                    count_line_violation_loop = torch.sum(imbalancetest_batch_loop > 0, dim=1)\n",
    "                    imbalancetest_batch_loop = imbalancetest_batch_loop.reshape(lodf_batch,int(l*batch_size))\n",
    "                    count_violation_loop = torch.count_nonzero(imbalancetest_batch_loop)\n",
    "                \n",
    "                    imbalancetest_batch += torch.sum(imbalancetest_batch_loop)/int(batch_size)\n",
    "                \n",
    "                    count_violation += count_violation_loop\n",
    "                    count_crit_violation += torch.count_nonzero(count_crit_violation_loop.detach())\n",
    "                    count_line_violation += torch.count_nonzero(count_line_violation_loop.detach())\n",
    "        \n",
    "                relcotest_epoch[i,:] = batch_relco.detach()\n",
    "                cost_epoch[i,:] = batch_cost.detach()\n",
    "                imbalancetest_epoch[i,:] = imbalancetest_batch.detach()\n",
    "                infeasibilitytest_epoch[i,:] = infeasibility_batch.detach()\n",
    "                \n",
    "                infeasibility_count[i,:] = infeasible_cases.detach()\n",
    "                line_violation_count[i,:] = count_line_violation.detach()\n",
    "                crit_violation_count[i,:] = count_crit_violation.detach()\n",
    "                #avg_violation_list[i,:] = avg_violation.detach()\n",
    "                count_violation_list[i,:] = count_violation\n",
    "             \n",
    "\n",
    "    imbalancetest = torch.mean(imbalancetest_epoch).detach()\n",
    "\n",
    "    infeasibilitytest = torch.mean(infeasibilitytest_epoch).detach()\n",
    "\n",
    "    relcotest = torch.mean(relcotest_epoch).detach()\n",
    "    costtest = torch.mean(cost_epoch).detach()\n",
    "    \n",
    "    #====== convergence criteria\n",
    "    infeasible_cases_tot = torch.sum(infeasibility_count).detach()\n",
    "    line_violation_tot = torch.sum(line_violation_count)\n",
    "    crit_violation_tot = torch.sum(crit_violation_count)\n",
    "    #tot_avg_violation = torch.sum(avg_violation_list).detach()\n",
    "    count_violation_tot = torch.sum(count_violation_list).detach()\n",
    "    \n",
    "    error[epoch,:] = imbalancetest.detach().numpy()\n",
    "    avg_infeasibility[epoch,:] =  infeasibilitytest.detach().numpy()\n",
    "    relcost[epoch,:] =  relcotest.detach().numpy()\n",
    "    cost[epoch,:] =  costtest.detach().numpy()\n",
    "    end_test = time.time()\n",
    "    time_test = end_test-start_test\n",
    "\n",
    "    print(epoch, \" | \", round(time_test,0), \" | \",np.round(error[epoch,:],3), \" | \", np.round(torch.mean(clil).detach().numpy(),3), \" | \", np.round(avg_infeasibility[epoch,:],3), \" | \", np.round(relcost[epoch,:], 3) )\n",
    "        \n",
    "\n",
    "print('method time:', time_test)\n",
    "\n",
    "print('current infeasibility:', (infeasible_cases_tot/Nsamples)*100, '%', 'goal:', 1, '%')\n",
    "    \n",
    "#print('current avg violation:', (tot_avg_violation/initial_violation_count),'pu', 'goal', 0.01*max_f, 'pu')\n",
    "print('infeasible contingency cases:', line_violation_tot, (line_violation_tot/(Ncontingencies*Nsamples))*100, '%')\n",
    "print('10%+ infeasible cases:', crit_violation_tot, (crit_violation_tot/(Ncontingencies*Nsamples))*100, '%')\n",
    "print('number of violations:', count_violation_tot)\n",
    "print('total cost:', cost.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde65dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87526521",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cea26a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
