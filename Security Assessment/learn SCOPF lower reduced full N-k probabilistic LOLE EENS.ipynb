{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a6fec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'C:\\Users\\bgiraud\\Documents\\Thesis Bastien\\I - Coding\\01 - learning SCOPFs\\01 - GLODF')\n",
    "\n",
    "import cvxpy as cp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import cvxpy_dcopf as cd\n",
    "import cvxpy_scopf as cs\n",
    "#import cvxpy_imb as ci\n",
    "from cvxpylayers.torch import CvxpyLayer\n",
    "import torch\n",
    "import torchvision\n",
    "import timeit\n",
    "import precontingency as pc\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data as Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import psutil\n",
    "\n",
    "import loadsampling as ls\n",
    "import pickle\n",
    "\n",
    "#SOLVER SETTINGS FOR SCS\n",
    "# used as solver for cvxpylayer in NN\n",
    "# SCS is standard solver for CVXPYlayer, you can specify other solvers\n",
    "\n",
    "SCS_solver_args={'use_indirect': False,\n",
    "         'gpu': False,\n",
    "         'verbose': False, # False\n",
    "         'normalize': True, #True heuristic data rescaling\n",
    "         'max_iters': 10000, #2500 giving the maximum number of iterations\n",
    "         'scale': 100, #1 if normalized, rescales by this factor\n",
    "         'eps':1e-3, #1e-3 convergence tolerance\n",
    "         'cg_rate': 2, #2 for indirect, tolerance goes down like 1/iter^cg_rate\n",
    "         'alpha': 1.5, #1.5 relaxation parameter\n",
    "         'rho_x':1e-3, #1e-3 x equality constraint scaling\n",
    "         'acceleration_lookback': 10, #10\n",
    "         'write_data_filename':None}\n",
    "\n",
    "\n",
    "SCS_solver_args2={'use_indirect': False,\n",
    "         'gpu': False,\n",
    "         'verbose': False, # False\n",
    "         'normalize': True, #True heuristic data rescaling\n",
    "         'max_iters': 20000, #2500 giving the maximum number of iterations\n",
    "         'scale': 1, #1 if normalized, rescales by this factor\n",
    "         'eps':1e-3, #1e-3 convergence tolerance\n",
    "         'cg_rate': 2, #2 for indirect, tolerance goes down like 1/iter^cg_rate\n",
    "         'alpha': 1.5, #1.5 relaxation parameter\n",
    "         'rho_x':1e-3, #1e-3 x equality constraint scaling\n",
    "         'acceleration_lookback': 10, #10\n",
    "         'write_data_filename':None}\n",
    "\n",
    "\n",
    "SCS_solver_args3={#'use_indirect': False,\n",
    "         #'gpu': False,\n",
    "         'verbose': False, # False\n",
    "         'normalize': False, #True heuristic data rescaling\n",
    "         'max_iters': 10000, #2500 giving the maximum number of iterations\n",
    "         'scale': 1, #1 if normalized, rescales by this factor\n",
    "         'eps':1e-3, #1e-3 convergence tolerance ????????\n",
    "         #'cg_rate': 2, #2 for indirect, tolerance goes down like 1/iter^cg_rate ????????\n",
    "         'alpha': 1.5, #1.5 relaxation parameter\n",
    "         'rho_x':1e-3, #1e-3 x equality constraint scaling\n",
    "         'acceleration_lookback': 10, #10\n",
    "         'write_data_filename':None}\n",
    "\n",
    "ECOS_solver_args = {\"solve_method\":\"ECOS\",\n",
    "    'verbose': False, # False\n",
    "    'max_iters': 50000, # Maximum number of iterations\n",
    "    'reltol': 1e-3, # Convergence tolerance\n",
    "    # Add other ECOS solver arguments as needed\n",
    "}\n",
    "\n",
    "\n",
    "data = pd.read_excel(r'C:\\Users\\bgiraud\\Documents\\Thesis Bastien\\I - Coding\\01 - learning SCOPFs\\01 - GLODF\\IEEE27mod.xlsx',sheet_name=None)\n",
    "Sbase = data['par']['base'][0]\n",
    "Nloads = len(data['load'])\n",
    "load_loc = torch.from_numpy(np.array(data['load']['bus']))\n",
    "Ngens = len(data['gen'])\n",
    "gen_loc = torch.from_numpy(np.array(data['gen']['bus']))\n",
    "Nlines = len(data['line'])\n",
    "Nbus = len(data['bus'])\n",
    "print(data[\"line\"]['max_f'][0]/Sbase)\n",
    "\n",
    "Nsamples = 1000\n",
    "Niterations = 200\n",
    "batch_size = 100\n",
    "train = 0.8\n",
    "tollerance_crit = 0.1\n",
    "learning_rate = 0.01\n",
    "wd = 0.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42304390",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.parallel import DataParallel\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # GPU is available\n",
    "    device = torch.device('cuda')\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    # GPU is not available\n",
    "    device = torch.device('cpu')\n",
    "    print(\"GPU is not available\")\n",
    "\n",
    "\n",
    "if torch.cuda.device_count() >= 1:\n",
    "    device_ids = list(range(torch.cuda.device_count()))\n",
    "else:\n",
    "    device_ids = None\n",
    "    \n",
    "num_workers = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72387614",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "#X = np.array(data['load']['p'])/Sbase*np.random.uniform(low=0.75, high=1.25, size = (Nsamples,Nloads))\n",
    "\n",
    "#LB = 0.75*np.ones(Nloads)\n",
    "#UB = 1.25*np.ones(Nloads)\n",
    "#X = np.array(data['load']['p'])/Sbase*ls.kumaraswamymontecarlo(1.6, 2.8, 0.75, LB, UB, Nsamples).T\n",
    "\n",
    "file_path = r'C:\\Users\\bgiraud\\Documents\\Thesis Bastien\\I - Coding\\01 - learning SCOPFs\\01 - GLODF\\trained models\\training data\\Xtrain.pkl' \n",
    "with open(file_path, 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "\n",
    "Xtrain = torch.tensor(X[0:int(Nsamples),:],dtype = torch.float32)        \n",
    "Xtrain_transpose = Xtrain.transpose(0,1)\n",
    "#Xtest = torch.tensor(X[int(train*Nsamples):,:],dtype = torch.float64)\n",
    "#standardise\n",
    "Xmin, Xmax,Xmean,Xstd = np.min(X, axis = 0),np.max(X, axis = 0),np.mean(X, axis = 0),np.std(X, axis = 0)\n",
    "Xscal = torch.tensor(( X - Xmean ) / Xstd, dtype=torch.float32)\n",
    "#Xscaltrain = Xscal[0:int(train*Nsamples),:]\n",
    "#Xscaltest = Xscal[int(train*Nsamples):,:]\n",
    "\n",
    "\n",
    "c_max = np.max(np.array(data['gen']['cost']))\n",
    "c_min = np.min(np.array(data['gen']['cost']))\n",
    "\n",
    "gencost = torch.tensor(np.array(data['gen']['cost']),dtype = torch.float32)  \n",
    "c_delta = c_max - c_min\n",
    "cgi0 = torch.tensor((np.array(data['gen']['cost'])-c_min)/c_delta,dtype = torch.float32)\n",
    "alpha = 1\n",
    "\n",
    "PTDF = torch.from_numpy(pc.compptdfs(data)).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6e0289",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timeit.default_timer()\n",
    "#problem0, Pd0, cg0, cll0, Pgi, Fl0, Th0, lam10, lam20 = cd.create_dcopf_lower(data)\n",
    "problem0, Pd0, cll0, Pgi, Fl0, Th0, lam10, lam20 = cd.create_dcopf_correction(data)\n",
    "assert problem0.is_dpp()\n",
    "\n",
    "#cvxpylayer0 = CvxpyLayer(problem0, parameters=[Pd0, cg0, cll0], variables=[Pgi, Fl0, Th0, lam10, lam20])\n",
    "cvxpylayer0 = CvxpyLayer(problem0, parameters=[Pd0, cll0], variables=[Pgi, Fl0, Th0, lam10, lam20])\n",
    "time1 = timeit.default_timer() - start\n",
    "print(\"Time to create model: \", time1)\n",
    "\n",
    "max_f = data[\"line\"]['max_f'][0]/Sbase\n",
    "l = len(data['line'])\n",
    "print('RAM Used (GB):', psutil.virtual_memory()[3]/1000000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62978df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "dict_method = 'pickle'\n",
    "\n",
    "start_load = time.time()\n",
    "\n",
    "if dict_method == 'pickle': \n",
    "    file_path = r'C:\\Users\\bgiraud\\Documents\\Thesis Bastien\\I - Coding\\01 - learning SCOPFs\\01 - GLODF\\LODF_list\\IEEE27\\LODF_listN1.pkl'  \n",
    "    with open(file_path, 'rb') as f:\n",
    "        LODF_dict_N1 = pickle.load(f)\n",
    "\n",
    "Ncontingencies_N1 = int(l) \n",
    "tot_lodf_batches_N1 = len(LODF_dict_N1)\n",
    "lodf_batch_size_N1 = Ncontingencies_N1\n",
    "last_batch_size_N1 = Ncontingencies_N1 - (lodf_batch_size_N1*(tot_lodf_batches_N1-1))\n",
    "\n",
    "if dict_method == 'pickle': \n",
    "    file_path = r'C:\\Users\\bgiraud\\Documents\\Thesis Bastien\\I - Coding\\01 - learning SCOPFs\\01 - GLODF\\LODF_list\\IEEE27\\LODF_listN2.pkl'  \n",
    "    with open(file_path, 'rb') as f:\n",
    "        LODF_dict_N2 = pickle.load(f)\n",
    "\n",
    "Ncontingencies_N2 = int(l*(l-1)/2)\n",
    "tot_lodf_batches_N2 = len(LODF_dict_N2)\n",
    "lodf_batch_size_N2 = Ncontingencies_N2\n",
    "last_batch_size_N2 = Ncontingencies_N2 - (lodf_batch_size_N2*(tot_lodf_batches_N2-1))\n",
    "\n",
    "if dict_method == 'pickle': \n",
    "    file_path = r'C:\\Users\\bgiraud\\Documents\\Thesis Bastien\\I - Coding\\01 - learning SCOPFs\\01 - GLODF\\LODF_list\\IEEE27\\LODF_listN3.pkl'  \n",
    "    with open(file_path, 'rb') as f:\n",
    "        LODF_dict_N3 = pickle.load(f)\n",
    "\n",
    "Ncontingencies_N3 = int(l*(l-1)/2*(l-2)/3)\n",
    "tot_lodf_batches_N3 = len(LODF_dict_N3)\n",
    "lodf_batch_size_N3 = Ncontingencies_N3 #20000\n",
    "last_batch_size_N3 = Ncontingencies_N3 - (lodf_batch_size_N3*(tot_lodf_batches_N3-1))\n",
    "\n",
    "if dict_method == 'pickle': \n",
    "    file_path = r'C:\\Users\\bgiraud\\Documents\\Thesis Bastien\\I - Coding\\01 - learning SCOPFs\\01 - GLODF\\LODF_list\\IEEE27\\LODF_listN4.pkl'  \n",
    "    with open(file_path, 'rb') as f:\n",
    "        LODF_dict_N4 = pickle.load(f)\n",
    "\n",
    "Ncontingencies_N4 = int(l*(l-1)/2*(l-2)/3*(l-3)/4)\n",
    "tot_lodf_batches_N4 = len(LODF_dict_N4)\n",
    "lodf_batch_size_N4 = 20000\n",
    "last_batch_size_N4 = Ncontingencies_N4 - (lodf_batch_size_N4*(tot_lodf_batches_N4-1))\n",
    "\n",
    "if dict_method == 'pickle': \n",
    "    file_path = r'C:\\Users\\bgiraud\\Documents\\Thesis Bastien\\I - Coding\\01 - learning SCOPFs\\01 - GLODF\\LODF_list\\IEEE27\\LODF_listN5.pkl'  \n",
    "    with open(file_path, 'rb') as f:\n",
    "        LODF_dict_N5 = pickle.load(f)\n",
    "\n",
    "Ncontingencies_N5 = int(l*(l-1)/2*(l-2)/3*(l-3)/4*(l-4)/5)\n",
    "tot_lodf_batches_N5 = len(LODF_dict_N5)\n",
    "lodf_batch_size_N5 = 20000 # Ncontingencies #20000\n",
    "last_batch_size_N5 = Ncontingencies_N5 - (lodf_batch_size_N5*(tot_lodf_batches_N5-1))\n",
    "        \n",
    "    \n",
    "end_load = time.time()\n",
    "\n",
    "print('Load LODF time:', end_load - start_load)\n",
    "print('RAM Used (GB):', psutil.virtual_memory()[3]/1000000000)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36ea33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = 'basecase'\n",
    "\n",
    "if scenario == 'basecase':\n",
    "    #===================== Base case probabilties\n",
    "    file_path = 'probabilities/IEEE39/probabilities_N1.pkl'\n",
    "    with open(file_path, 'rb') as f:\n",
    "        probabilities_N1 = pickle.load(f)\n",
    "\n",
    "    file_path = 'probabilities/IEEE39/probabilities_N2.pkl'\n",
    "    with open(file_path, 'rb') as f:\n",
    "        probabilities_N2 = pickle.load(f)\n",
    "\n",
    "    file_path = 'probabilities/IEEE39/probabilities_N3.pkl'\n",
    "    with open(file_path, 'rb') as f:\n",
    "        probabilities_N3 = pickle.load(f)\n",
    "\n",
    "elif scenario == 'stormy1':\n",
    "#===================== Stormy node 1 probabilties\n",
    "    file_path = 'probabilities/IEEE39/Storm/probabilities_N1_bigstorm2.pkl'\n",
    "    with open(file_path, 'rb') as f:\n",
    "        probabilities_N1 = pickle.load(f)\n",
    "\n",
    "    file_path = 'probabilities/IEEE39/Storm/probabilities_N2_bigstorm2.pkl'\n",
    "    with open(file_path, 'rb') as f:\n",
    "        probabilities_N2 = pickle.load(f)\n",
    "\n",
    "    file_path = 'probabilities/IEEE39/Storm/probabilities_N3_bigstorm2.pkl'\n",
    "    with open(file_path, 'rb') as f:\n",
    "        probabilities_N3 = pickle.load(f)\n",
    "        \n",
    "file_path = 'probabilities/IEEE39/line_lengths.pkl'\n",
    "with open(file_path, 'rb') as f:\n",
    "    line_len = pickle.load(f)\n",
    "    \n",
    "\n",
    "line_lengths = torch.tensor(line_len).unsqueeze(1)\n",
    "print(torch.mean(line_lengths))\n",
    "    \n",
    "print(probabilities_N1.sum()/l, probabilities_N1.shape, probabilities_N1.max(), probabilities_N1.min())\n",
    "print(probabilities_N2.sum()/l, probabilities_N2.shape, probabilities_N2.max(), probabilities_N2.min())\n",
    "print(probabilities_N3.sum()/l, probabilities_N3.shape, probabilities_N3.max(), probabilities_N3.min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690b68e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Benchmark either DCOPF or SCOPF\n",
    "benchmark = 'SCOPF' #'SCOPF'\n",
    "\n",
    "Pgisscopf = torch.zeros(Ngens,int(Nsamples),dtype = torch.float32)\n",
    "Fl0sscopf = torch.zeros(Nlines,int(Nsamples),dtype = torch.float32)\n",
    "Flcs = torch.zeros(Nlines,l,int(Nsamples),dtype = torch.float32)\n",
    "lam1s = torch.zeros(Nbus,int(Nsamples),dtype = torch.float32)\n",
    "lam2s = torch.zeros(Nbus,int(Nsamples),dtype = torch.float32)\n",
    "lam1sc = torch.zeros(Nbus,l,int(Nsamples),dtype = torch.float32)\n",
    "lam2sc = torch.zeros(Nbus,l,int(Nsamples),dtype = torch.float32)\n",
    "Th0 = torch.zeros(Nbus,int(Nsamples),dtype = torch.float32)\n",
    "\n",
    "#======== remove islanding cases from contingency list\n",
    "lcontingencies = range(l)\n",
    "islanding_cases = [15,20]\n",
    "lcontingencies_filtered = [l for l in lcontingencies if l not in islanding_cases]\n",
    "lcontingencies = lcontingencies_filtered\n",
    "\n",
    "time_scopfs = time.time()\n",
    "\n",
    "#======== determine infeasible cases\n",
    "# add loads\n",
    "load_profile = torch.zeros((Nsamples,Nbus)).to(torch.float32)\n",
    "load_profile[:, load_loc] = torch.from_numpy(X).to(torch.float32)\n",
    "\n",
    "if benchmark == 'SCOPF':\n",
    "    problem0scopf, Pd0scopf, cg0scopf, Pgiscopf, Fl0scopf, Th0scopf, Flc, lam1, lam2, lam1c, lam2c = cs.create_scopf(data, lcontingencies)\n",
    "\n",
    "elif benchmark == 'SCOPF2':\n",
    "    problem0scopf, Pd0scopf, cg0scopf, Pgiscopf, Fl0scopf, Th0scopf, Flc = cs.create_scopf2(data, lcontingencies)\n",
    "\n",
    "elif benchmark == 'DCOPF': \n",
    "    #problem0scopf, Pd0scopf, cg0scopf, cl0scopf, Pgiscopf, Fl0scopf, Th0scopf, lam1dc, lam2dc = cd.create_dcopf_upper(data)\n",
    "    problem0scopf, Pd0scopf, cg0scopf, cll0scopf, Pgiscopf, Fl0scopf, Th0scopf, lam1, lam2 = cd.create_dcopf_lower(data)\n",
    "    cll0scopf.value = np.zeros(Ngens)\n",
    "\n",
    "cg0scopf.value = np.array(data['gen']['cost'])\n",
    "for entry in tqdm(range(int(Nsamples)),position=0, leave=True):   \n",
    "    Pd0scopf.value = X[entry,:] #np.array(data['load']['p'])/Sbase    \n",
    "    solution  = problem0scopf.solve(solver=cp.ECOS)    # cp.MOSEK\n",
    "    if problem0scopf.status in [\"infeasible\", \"unbounded\"]:\n",
    "         # Otherwise, problem.value is inf or -inf, respectively.\n",
    "         print(\"Problem infeasible or unbounded\")\n",
    "         #print(\"Optimal value: %s\" % problem0scopf.value)\n",
    "    Pgisscopf[:,entry] = torch.tensor(Pgiscopf.value)\n",
    "    Fl0sscopf[:,entry] = torch.tensor(Fl0scopf.value)\n",
    "    #Flcs[:,:,entry] = insert_zero_columns(torch.tensor(Flc.value),islanding_cases)\n",
    "    Th0[:,entry] = torch.tensor(Th0scopf.value)\n",
    "    \n",
    "    lam1s[:,entry] = torch.tensor(lam1.value)\n",
    "    lam2s[:,entry] = torch.tensor(lam2.value)\n",
    "    #lam1sc[:,:,entry] = insert_zero_columns(torch.tensor(lam1c.value),islanding_cases)\n",
    "    #lam2sc[:,:,entry] = insert_zero_columns(torch.tensor(lam2c.value),islanding_cases)\n",
    "    \n",
    "time_scopfs = time.time() - time_scopfs\n",
    "cost_scopf = torch.matmul(gencost,Pgisscopf)\n",
    "\n",
    "# add gens\n",
    "load_profile[:, gen_loc] += Pgisscopf.T\n",
    "\n",
    "# add smallest values to zero elements\n",
    "nonzero_values, _ = torch.min(load_profile.masked_fill(load_profile == 0, float('inf')), dim=1, keepdim=True)\n",
    "mask = load_profile == 0\n",
    "load_profile = torch.where(mask, nonzero_values, load_profile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f5ccfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.set_grad_enabled(True):\n",
    "    N, D_in, D_out = Nsamples, Nloads, Ngens\n",
    "    H1, H2, H3 = 8,8,8\n",
    "   \n",
    "    #very large network\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(D_in, H1),\n",
    "        torch.nn.ReLU(),\n",
    "        nn.Dropout(p=0.2),\n",
    "        torch.nn.Linear(H1, H2),\n",
    "        torch.nn.ReLU(),\n",
    "        nn.Dropout(p=0.2),\n",
    "        torch.nn.Linear(H2, H3),\n",
    "        torch.nn.ReLU(),\n",
    "        nn.Dropout(p=0.2),\n",
    "        torch.nn.Linear(H3, D_out), #before 23/12 when everything was working!\n",
    "        #torch.nn.Sigmoid(),\n",
    "        torch.nn.Tanh(),\n",
    "        #torch.nn.Softmax(dim=0),\n",
    "    )\n",
    "    \n",
    "    #model = torch.nn.RNN(input_size = D_in, hidden_size = D_out, num_layers=3, nonlinearity = 'tanh', batch_first = True)\n",
    "    model.double()\n",
    "    \n",
    "    def init_weights(u):\n",
    "        if type(u) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(u.weight)\n",
    "            #u.reset_parameters() # default gaussion initializaiton\n",
    "            torch.nn.init.xavier_uniform_(u.weight, gain=nn.init.calculate_gain('relu'))\n",
    "            u.bias.data.fill_(0)    \n",
    "    model.apply(init_weights)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    # adaptive momentum with weight decay\n",
    "    \n",
    "    model.to(torch.float32)\n",
    "    print(model)\n",
    "\n",
    "print('Load LODF time:', end_load - start_load)\n",
    "print('RAM Used (GB):', psutil.virtual_memory()[3]/1000000000)\n",
    "    \n",
    "num_params_full = sum(p.numel() for p in model.parameters())\n",
    "#num_params_TT = sum(p.numel() for p in modelTT.parameters())\n",
    "print('')\n",
    "print('FC neural network:', num_params_full)\n",
    "#print('TT neural network:', num_params_TT)\n",
    "print('input loads:', D_in, 'output gens:', D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b8ccce",
   "metadata": {},
   "outputs": [],
   "source": [
    "if device_ids:\n",
    "    model = DataParallel(model, device_ids=device_ids)\n",
    "    model = model.to(device)\n",
    "    print(device_ids)\n",
    "\n",
    "model.to(torch.float32)\n",
    "print(next(model.parameters()).device)\n",
    "print(next(model.parameters()).dtype)\n",
    "\n",
    "# prompt to empty GPU cache: torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e031bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_indices = data['gen']['bus'] \n",
    "generator_nodes = np.zeros(Nbus)\n",
    "for index in gen_indices:\n",
    "    generator_nodes[index] = 1\n",
    "gen_nodes = np.nonzero(generator_nodes)\n",
    "gen_nodes = gen_nodes[0].tolist()\n",
    "print(gen_nodes)\n",
    "\n",
    "load_indices = data['load']['bus'] \n",
    "load_nodes = np.zeros(Nbus)\n",
    "for index in load_indices:\n",
    "    load_nodes[index] = 1\n",
    "load_nodes = np.nonzero(load_nodes)\n",
    "load_nodes = load_nodes[0].tolist()\n",
    "print(load_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de083d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "max_p = torch.from_numpy(np.array(-data[\"gen\"]['min_p']/Sbase))\n",
    "min_p = torch.from_numpy(np.array(data[\"gen\"]['max_p']/Sbase))\n",
    "\n",
    "#============== Initialize training testing data\n",
    "Xscal_train, Xscal_test, cost_scopf_train, cost_scopf_test = train_test_split(Xscal, cost_scopf, test_size=1-train, random_state=42)\n",
    "\n",
    "training_dataset = Data.TensorDataset(Xscal_train, cost_scopf_train)\n",
    "testing_dataset = Data.TensorDataset(Xscal_test, cost_scopf_test)\n",
    "\n",
    "train_loader = Data.DataLoader(dataset=training_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = Data.DataLoader(dataset=testing_dataset, batch_size=batch_size, shuffle=False)  # Note: shuffle=False for testing\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "flow_penalty = 1000\n",
    "crit_penalty = 0\n",
    "infeasibility_penalty = 0\n",
    "cost_penalty = 1\n",
    "soft_penalty = 100\n",
    "Niterations = 10\n",
    "\n",
    "\n",
    "#================ Initialize tensors\n",
    "error = np.zeros(shape=(Niterations,6))\n",
    "exp_inf = np.zeros(shape=(Niterations,6))\n",
    "avg_infeasibility = np.zeros(shape=(Niterations,2))\n",
    "relcost = np.zeros(shape=(Niterations,2))\n",
    "\n",
    "time_CF = np.zeros(shape=(Niterations,1))\n",
    "memory_CF = np.zeros(shape=(Niterations,1))\n",
    "percentage_CF = np.zeros(shape=(Niterations,1))\n",
    "backward_CF = np.zeros(shape=(Niterations,1))\n",
    "\n",
    "Pgis = torch.zeros(size = (Ngens,int(batch_size)),dtype = torch.float32)\n",
    "Flis = torch.zeros(size = (Nlines,int(batch_size)),dtype = torch.float32)\n",
    "clil = torch.zeros(size = (Ngens,int(batch_size)),dtype = torch.float32)\n",
    "infeasibility = torch.zeros(int(batch_size), requires_grad = False)\n",
    "\n",
    "relcotrain_epoch = torch.zeros(size = (int(Nsamples/batch_size),int(batch_size)),dtype = torch.float32, requires_grad = False)\n",
    "relcotest_epoch = torch.zeros(size = (int(Nsamples/batch_size),int(batch_size)),dtype = torch.float32, requires_grad = False)\n",
    "\n",
    "imbalancetrain_epoch = torch.zeros(size = (int(Nsamples*train/batch_size),3),dtype = torch.float32, requires_grad = False)\n",
    "imbalancetest_epoch = torch.zeros(size = (round((Nsamples*(1-train))/batch_size),3),dtype = torch.float32, requires_grad = False)\n",
    "\n",
    "expected_viol_epoch = torch.zeros(size = (int(Nsamples*train/batch_size),3),dtype = torch.float32, requires_grad = False)\n",
    "expected_viol_test_epoch = torch.zeros(size = (round((Nsamples*(1-train))/batch_size),3),dtype = torch.float32, requires_grad = False)\n",
    "\n",
    "infeasibilitytrain_epoch = torch.zeros(size = (int(Nsamples/batch_size),Nbus),dtype = torch.float32, requires_grad = False)\n",
    "infeasibilitytest_epoch = torch.zeros(size = (int(Nsamples/batch_size),Nbus),dtype = torch.float32, requires_grad = False)\n",
    "infeasibility_count = torch.zeros(size = (int(Nsamples/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "\n",
    "line_violation_count = torch.zeros(size = (int(Nsamples/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "crit_violation_count = torch.zeros(size = (int(Nsamples/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "avg_violation_list = torch.zeros(size = (int(Nsamples/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "count_violation_list = torch.zeros(size = (int(Nsamples/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "\n",
    "violation_percentage = torch.zeros(size = (int(Nsamples*train/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "time_contflow = torch.zeros(size = (int(Nsamples*train/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "memory_contflow = torch.zeros(size = (int(Nsamples*train/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "backward_contflow = torch.zeros(size = (int(Nsamples*train/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "\n",
    "#===== N-1\n",
    "crit_violation_count_N1 = torch.zeros(size = (int(Nsamples/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "line_violation_count_N1 = torch.zeros(size = (int(Nsamples/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "\n",
    "#===== N-2\n",
    "crit_violation_count_N2 = torch.zeros(size = (int(Nsamples/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "line_violation_count_N2 = torch.zeros(size = (int(Nsamples/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "\n",
    "#===== N-3\n",
    "crit_violation_count_N3 = torch.zeros(size = (int(Nsamples/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "line_violation_count_N3 = torch.zeros(size = (int(Nsamples/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "\n",
    "\n",
    "print(\"It |   Time   |    Imbalance    | Mod out  |   Infeasibility   |  Rel cost\")\n",
    "for epoch in range(Niterations):\n",
    "    expected_infeasible = 0\n",
    "    expected_infeasible_test = 0\n",
    "    \n",
    "    time_nnSCS = time.time()\n",
    "    for i, (Xbatch, cost_scopf_batch) in enumerate(tqdm(train_loader)):\n",
    "            start_forward = time.time()\n",
    "            start_forward_memory = psutil.virtual_memory()[3] / 1000000000            \n",
    "            \n",
    "            #============= initialize\n",
    "            model.train()\n",
    "            optimizer.zero_grad() \n",
    "            model.zero_grad()\n",
    "            Xbatch = Xbatch.clone().detach().to(device)\n",
    "            cost_scopf_batch = cost_scopf_batch.clone().detach().to(device)\n",
    "\n",
    "            Xtrain_batch = ((Xbatch.cpu() * Xstd) + Xmean).to(torch.float32).to(device)\n",
    "            load_profile = torch.zeros((batch_size, Nbus)).to(device)\n",
    "            load_profile[:, load_loc] = Xtrain_batch\n",
    "\n",
    "            #================== Perform forward pass\n",
    "            start_FP = time.time()\n",
    "            clil = (model(Xbatch) + 1) / 2\n",
    "            end_FP = time.time()\n",
    "            \n",
    "            #============= soft violation base case\n",
    "            Pg_guess = torch.mul(max_p,clil) \n",
    "            \n",
    "            Pg_train = torch.zeros((batch_size,Nbus)).to(torch.float32)\n",
    "            Pd_train = torch.zeros((batch_size,Nbus)).to(torch.float32)\n",
    "\n",
    "            Pg_train[:,gen_nodes] = Pg_guess.to(torch.float32)\n",
    "            Pd_train[:,load_nodes] = Xtrain_batch.to(torch.float32)\n",
    "                \n",
    "            Fl_base = F.relu(torch.abs(PTDF@(Pg_train-Pd_train).T)-max_f).sum()/int(batch_size)\n",
    "            \n",
    "            #============= cvxpylayer\n",
    "            start_cvxpy = time.time()\n",
    "            #Pgi, Fli, Thi, lam1i, lam2i = cvxpylayer0(Xtrain_batch, gencost, clil, solver_args=ECOS_solver_args)\n",
    "            Pgi, Fli, Thi, lam1i, lam2i = cvxpylayer0(Xtrain_batch, clil, solver_args=ECOS_solver_args)\n",
    "            Pgis = Pgi.T\n",
    "            Flis = Fli.T.to(torch.float32)\n",
    "            end_cvxpy = time.time()\n",
    "            \n",
    "            #=============== modify load profile with generator outputs\n",
    "            load_profile[:, gen_loc] += Pgi\n",
    "            nonzero_values, _ = torch.min(load_profile.masked_fill(load_profile == 0, float('inf')), dim=1, keepdim=True)\n",
    "            mask = load_profile == 0\n",
    "            load_profile = torch.where(mask, nonzero_values, load_profile)\n",
    "\n",
    "            #================ infeasibility\n",
    "            infeasibility_tensor = (lam1i + lam2i)\n",
    "            infeasibility_loss = infeasibility_tensor.sum() / int(batch_size)\n",
    "            infeasibility_batch = infeasibility_tensor.sum(dim=0).detach()\n",
    "\n",
    "            infeasibility_tensor[abs(infeasibility_tensor) < 1e-5] = 0\n",
    "            infeasible_buses = (abs(infeasibility_tensor) > 1e-5)\n",
    "\n",
    "            infeasible_cases = torch.zeros((batch_size, 1)).to(device)\n",
    "            row_has_true = torch.any(infeasible_buses, axis=1)\n",
    "            infeasible_cases[row_has_true] = True\n",
    "            infeasible_cases = infeasible_cases.sum()\n",
    "\n",
    "            #==================== cost\n",
    "            batch_cost = torch.matmul(gencost, Pgis)\n",
    "            batch_relco = ((batch_cost - cost_scopf_batch) / cost_scopf_batch) * 100\n",
    "            relco_avg = torch.mean(batch_relco)\n",
    "\n",
    "            cost_loss = torch.mean(batch_cost) #/ int(batch_size)\n",
    "\n",
    "            #===================== compute contingency flows for each sample  \n",
    "            imbalancetrain_batch_N1 = 0\n",
    "            imbalancetrain_batch_N2 = 0\n",
    "            imbalancetrain_batch_N3 = 0\n",
    "            imbalancetrain_batch = 0            \n",
    "            \n",
    "            count_crit_violation_N1 = 0\n",
    "            count_line_violation_N1 = 0\n",
    "            count_violation_N1 = 0\n",
    "            \n",
    "            count_crit_violation_N2 = 0\n",
    "            count_line_violation_N2 = 0\n",
    "            count_violation_N2 = 0\n",
    "            \n",
    "            count_crit_violation_N3 = 0\n",
    "            count_line_violation_N3 = 0\n",
    "            count_violation_N3 = 0\n",
    "            \n",
    "            expected_flows = torch.zeros(size = (l,batch_size),dtype = torch.float32, requires_grad = True)\n",
    "            expected_flows_N1 = torch.zeros(size = (l,batch_size),dtype = torch.float32, requires_grad = True)\n",
    "            expected_flows_N2 = torch.zeros(size = (l,batch_size),dtype = torch.float32, requires_grad = True)\n",
    "            expected_flows_N3 = torch.zeros(size = (l,batch_size),dtype = torch.float32, requires_grad = True)\n",
    "\n",
    "            #============ N-1\n",
    "            for b in (range(tot_lodf_batches_N1)):\n",
    "                #print('RAM Used (GB):', psutil.virtual_memory()[3]/1000000000)\n",
    "                lodf_batch = lodf_batch_size_N1\n",
    "                if b == (tot_lodf_batches_N1 - 1): \n",
    "                    lodf_batch = last_batch_size_N1\n",
    "                left_ones = torch.ones((lodf_batch,1))\n",
    "                right_ones = torch.ones((int(l*batch_size),1))\n",
    "                Fli_0 = Flis.expand(lodf_batch, -1, -1)\n",
    "                \n",
    "                if b != (tot_lodf_batches_N1 - 1): \n",
    "                    start_prob = b*lodf_batch*l\n",
    "                    end_prob = (b+1)*lodf_batch*l\n",
    "                    P_outage = probabilities_N1[start_prob:end_prob]\n",
    "                if b == (tot_lodf_batches_N1 - 1): \n",
    "                    end_prob = Ncontingencies_N1*l\n",
    "                    start_prob = (Ncontingencies_N1*l) - (last_batch_size_N1*l)\n",
    "                    P_outage = probabilities_N1[-(last_batch_size_N1*l):]*1000\n",
    "                                \n",
    "                index1 = LODF_dict_N1[b][0].detach()\n",
    "                index2 = LODF_dict_N1[b][1].detach()\n",
    "                indices = torch.stack((index1,index2)).detach()\n",
    "                values = LODF_dict_N1[b][2].detach()\n",
    "                shape = (l,int(l*lodf_batch))\n",
    "                lodf = torch.sparse_coo_tensor(indices,values,size = shape, requires_grad = False).T.to(device).detach()\n",
    "\n",
    "                imbalancetrain_batch_loop = (F.relu(torch.abs(Fli_0 + torch.sparse.mm(lodf, Flis).reshape((lodf_batch, l, batch_size))) - max_f)).detach().to(device)\n",
    "                count_crit_violation_loop = torch.sum(imbalancetrain_batch_loop > tollerance_crit * max_f, dim=1)\n",
    "                count_line_violation_loop = torch.sum(imbalancetrain_batch_loop > 0, dim=1)\n",
    "                imbalancetrain_batch_loop = imbalancetrain_batch_loop.reshape(int(l * lodf_batch), -1)\n",
    "\n",
    "                #========= count the number of rows that have all zeros\n",
    "                mask = torch.all(imbalancetrain_batch_loop == 0, dim=1) \n",
    "                indices_remove = mask.nonzero().squeeze(1)\n",
    "                num_zero_rows = torch.sum(mask)\n",
    "                \n",
    "                indices_tot = torch.arange(start_prob, end_prob)\n",
    "                mask_keep = ~torch.isin(indices_tot, indices_remove)\n",
    "                indices_keep = indices_tot[mask_keep]\n",
    "                indices_loc = indices_keep % l\n",
    "\n",
    "                #============ create new lodf\n",
    "                mask_flow = torch.ones(imbalancetrain_batch_loop.shape[0], dtype=torch.bool).to(device)\n",
    "                mask_flow[indices_remove] = False\n",
    "                lodf_test = lodf.to_dense()[mask_flow]\n",
    "                lodf_test = lodf_test.clone().detach()\n",
    "                lodf = lodf_test.to_sparse_coo().requires_grad_().to(torch.float32) \n",
    "                P_outage = P_outage[mask_flow]\n",
    "                \n",
    "                Fli_0 = Flis.repeat(lodf_batch, 1).to(torch.float32).to(device)[mask_flow]\n",
    "                imbalancetrain_batch_loop = (F.relu(torch.abs(Fli_0+torch.sparse.mm(lodf,Flis))- max_f))\n",
    "                count_violation_loop = torch.count_nonzero(imbalancetrain_batch_loop)#/int(batch_size) \n",
    "                expected_violation = torch.mul(P_outage,imbalancetrain_batch_loop).to(torch.float32)\n",
    "                \n",
    "                imbalancetrain_batch_N1 += torch.sum(imbalancetrain_batch_loop)/int(batch_size)\n",
    "                expected_flows_N1 = torch.index_add(expected_flows_N1, 0, indices_loc.long(), expected_violation)\n",
    "                \n",
    "                count_violation_N1 += count_violation_loop\n",
    "                count_crit_violation_N1 += torch.count_nonzero(count_crit_violation_loop.detach())\n",
    "                count_line_violation_N1 += torch.count_nonzero(count_line_violation_loop.detach())\n",
    "                \n",
    "            #============ N-2\n",
    "            for b in (range(tot_lodf_batches_N2)):\n",
    "                #print('RAM Used (GB):', psutil.virtual_memory()[3]/1000000000)\n",
    "                lodf_batch = lodf_batch_size_N2\n",
    "                if b == (tot_lodf_batches_N2 - 1): \n",
    "                    lodf_batch = last_batch_size_N2\n",
    "                left_ones = torch.ones((lodf_batch,1))\n",
    "                right_ones = torch.ones((int(l*batch_size),1))\n",
    "                Fli_0 = Flis.expand(lodf_batch, -1, -1)\n",
    "                \n",
    "                if b != (tot_lodf_batches_N2 - 1): \n",
    "                    start_prob = b*lodf_batch*l\n",
    "                    end_prob = (b+1)*lodf_batch*l\n",
    "                    P_outage = probabilities_N2[start_prob:end_prob]\n",
    "                if b == (tot_lodf_batches_N2 - 1): \n",
    "                    end_prob = Ncontingencies_N2*l\n",
    "                    start_prob = (Ncontingencies_N2*l) - (last_batch_size_N2*l)\n",
    "                    P_outage = probabilities_N2[-(last_batch_size_N2*l):]*1000\n",
    "                                \n",
    "                index1 = LODF_dict_N2[b][0].detach()\n",
    "                index2 = LODF_dict_N2[b][1].detach()\n",
    "                indices = torch.stack((index1,index2)).detach()\n",
    "                values = LODF_dict_N2[b][2].detach()\n",
    "                shape = (l,int(l*lodf_batch))\n",
    "                lodf = torch.sparse_coo_tensor(indices,values,size = shape, requires_grad = False).T.to(device).detach()\n",
    "\n",
    "                imbalancetrain_batch_loop = (F.relu(torch.abs(Fli_0 + torch.sparse.mm(lodf, Flis).reshape((lodf_batch, l, batch_size))) - max_f)).detach().to(device)\n",
    "                count_crit_violation_loop = torch.sum(imbalancetrain_batch_loop > tollerance_crit * max_f, dim=1)\n",
    "                count_line_violation_loop = torch.sum(imbalancetrain_batch_loop > 0, dim=1)\n",
    "                imbalancetrain_batch_loop = imbalancetrain_batch_loop.reshape(int(l * lodf_batch), -1)\n",
    "\n",
    "                #========= count the number of rows that have all zeros\n",
    "                mask = torch.all(imbalancetrain_batch_loop == 0, dim=1) \n",
    "                indices_remove = mask.nonzero().squeeze(1)\n",
    "                num_zero_rows = torch.sum(mask)\n",
    "                \n",
    "                indices_tot = torch.arange(start_prob, end_prob)\n",
    "                mask_keep = ~torch.isin(indices_tot, indices_remove)\n",
    "                indices_keep = indices_tot[mask_keep]\n",
    "                indices_loc = indices_keep % l\n",
    "\n",
    "                #============ create new lodf\n",
    "                mask_flow = torch.ones(imbalancetrain_batch_loop.shape[0], dtype=torch.bool).to(device)\n",
    "                mask_flow[indices_remove] = False\n",
    "                lodf_test = lodf.to_dense()[mask_flow]\n",
    "                lodf_test = lodf_test.clone().detach()\n",
    "                lodf = lodf_test.to_sparse_coo().requires_grad_().to(torch.float32) \n",
    "                P_outage = P_outage[mask_flow]\n",
    "                \n",
    "                Fli_0 = Flis.repeat(lodf_batch, 1).to(torch.float32).to(device)[mask_flow]\n",
    "                imbalancetrain_batch_loop = (F.relu(torch.abs(Fli_0+torch.sparse.mm(lodf,Flis))- max_f))\n",
    "                count_violation_loop = torch.count_nonzero(imbalancetrain_batch_loop)#/int(batch_size) \n",
    "                expected_violation = torch.mul(P_outage,imbalancetrain_batch_loop).to(torch.float32)\n",
    "                \n",
    "                imbalancetrain_batch_N2 += torch.sum(imbalancetrain_batch_loop)/int(batch_size)\n",
    "                expected_flows_N2 = torch.index_add(expected_flows_N2, 0, indices_loc.long(), expected_violation)\n",
    "                \n",
    "                count_violation_N2 += count_violation_loop\n",
    "                count_crit_violation_N2 += torch.count_nonzero(count_crit_violation_loop.detach())\n",
    "                count_line_violation_N2 += torch.count_nonzero(count_line_violation_loop.detach())\n",
    "                \n",
    "                \n",
    "            #============ N-3\n",
    "            for b in (range(tot_lodf_batches_N3)):\n",
    "                #print('RAM Used (GB):', psutil.virtual_memory()[3]/1000000000)\n",
    "                lodf_batch = lodf_batch_size_N3\n",
    "                if b == (tot_lodf_batches_N3 - 1): \n",
    "                    lodf_batch = last_batch_size_N3\n",
    "                left_ones = torch.ones((lodf_batch,1))\n",
    "                right_ones = torch.ones((int(l*batch_size),1))\n",
    "                Fli_0 = Flis.expand(lodf_batch, -1, -1)\n",
    "                \n",
    "                if b != (tot_lodf_batches_N3 - 1): \n",
    "                    start_prob = b*lodf_batch*l\n",
    "                    end_prob = (b+1)*lodf_batch*l\n",
    "                    P_outage = probabilities_N3[start_prob:end_prob]\n",
    "                if b == (tot_lodf_batches_N3 - 1): \n",
    "                    end_prob = Ncontingencies_N3*l\n",
    "                    start_prob = (Ncontingencies_N3*l) - (last_batch_size_N3*l)\n",
    "                    P_outage = probabilities_N3[-(last_batch_size_N3*l):]*1000\n",
    "                                \n",
    "                index1 = LODF_dict_N3[b][0].detach()\n",
    "                index2 = LODF_dict_N3[b][1].detach()\n",
    "                indices = torch.stack((index1,index2)).detach()\n",
    "                values = LODF_dict_N3[b][2].detach()\n",
    "                shape = (l,int(l*lodf_batch))\n",
    "                lodf = torch.sparse_coo_tensor(indices,values,size = shape, requires_grad = False).T.to(device).detach()\n",
    "\n",
    "                imbalancetrain_batch_loop = (F.relu(torch.abs(Fli_0 + torch.sparse.mm(lodf, Flis).reshape((lodf_batch, l, batch_size))) - max_f)).detach().to(device)\n",
    "                count_crit_violation_loop = torch.sum(imbalancetrain_batch_loop > tollerance_crit * max_f, dim=1)\n",
    "                count_line_violation_loop = torch.sum(imbalancetrain_batch_loop > 0, dim=1)\n",
    "                imbalancetrain_batch_loop = imbalancetrain_batch_loop.reshape(int(l * lodf_batch), -1)\n",
    "\n",
    "                #========= count the number of rows that have all zeros\n",
    "                mask = torch.all(imbalancetrain_batch_loop == 0, dim=1) \n",
    "                indices_remove = mask.nonzero().squeeze(1)\n",
    "                num_zero_rows = torch.sum(mask)\n",
    "                \n",
    "                indices_tot = torch.arange(start_prob, end_prob)\n",
    "                mask_keep = ~torch.isin(indices_tot, indices_remove)\n",
    "                indices_keep = indices_tot[mask_keep]\n",
    "                indices_loc = indices_keep % l\n",
    "\n",
    "                #============ create new lodf\n",
    "                mask_flow = torch.ones(imbalancetrain_batch_loop.shape[0], dtype=torch.bool).to(device)\n",
    "                mask_flow[indices_remove] = False\n",
    "                lodf_test = lodf.to_dense()[mask_flow]\n",
    "                lodf_test = lodf_test.clone().detach()\n",
    "                lodf = lodf_test.to_sparse_coo().requires_grad_().to(torch.float32) \n",
    "                P_outage = P_outage[mask_flow]\n",
    "                \n",
    "                Fli_0 = Flis.repeat(lodf_batch, 1).to(torch.float32).to(device)[mask_flow]\n",
    "                imbalancetrain_batch_loop = (F.relu(torch.abs(Fli_0+torch.sparse.mm(lodf,Flis))- max_f))\n",
    "\n",
    "                count_violation_loop = torch.count_nonzero(imbalancetrain_batch_loop)#/int(batch_size) \n",
    "                expected_violation = torch.mul(P_outage,imbalancetrain_batch_loop).to(torch.float32)\n",
    "                \n",
    "                imbalancetrain_batch_N3 += torch.sum(imbalancetrain_batch_loop)/int(batch_size)\n",
    "                expected_flows_N3 = torch.index_add(expected_flows_N3, 0, indices_loc.long(), expected_violation)\n",
    "                \n",
    "                count_violation_N3 += count_violation_loop\n",
    "                count_crit_violation_N3 += torch.count_nonzero(count_crit_violation_loop.detach())\n",
    "                count_line_violation_N3 += torch.count_nonzero(count_line_violation_loop.detach())\n",
    "            \n",
    "            expected_inf_N1 = torch.sum(expected_flows_N1)/int(batch_size)\n",
    "            expected_inf_N2 = torch.sum(expected_flows_N2)/int(batch_size)\n",
    "            expected_inf_N3 = torch.sum(expected_flows_N3)/int(batch_size)\n",
    "\n",
    "            expected_infeasible_N1 = (torch.count_nonzero(expected_flows_N1)/torch.numel(expected_flows_N1))*100\n",
    "            expected_infeasible_N2 = (torch.count_nonzero(expected_flows_N2)/torch.numel(expected_flows_N2))*100\n",
    "            expected_infeasible_N3 = (torch.count_nonzero(expected_flows_N3)/torch.numel(expected_flows_N3))*100\n",
    "            torch.set_printoptions(precision=9)\n",
    "            \n",
    "            imbalancetrain_batch = expected_inf_N1+expected_inf_N2+expected_inf_N3\n",
    "            #imbalancetrain_batch_N1 + imbalancetrain_batch_N2 + imbalancetrain_batch_N3\n",
    "            #print(imbalancetrain_batch_N1, imbalancetrain_batch_N3, imbalancetrain_batch_N3)\n",
    "            \n",
    "            #======================== Perform back pass\n",
    "            loss = flow_penalty * imbalancetrain_batch + infeasibility_penalty * infeasibility_loss + cost_penalty * cost_loss + Fl_base * soft_penalty\n",
    "\n",
    "            start_BP = time.time()\n",
    "            loss.backward()\n",
    "            end_BP = time.time()\n",
    "            backward_batch = end_BP - start_BP\n",
    "            #print('Backward pass:', end_BP - start_BP)\n",
    "            \n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            relcotrain_epoch[i,:] = batch_relco.detach()\n",
    "            imbalancetrain_epoch[i,:] = torch.tensor([expected_inf_N1.detach(),expected_inf_N2.detach(),expected_inf_N3.detach()])\n",
    "            expected_viol_epoch[i,:] = torch.tensor([expected_infeasible_N1.detach(),expected_infeasible_N2.detach(),expected_infeasible_N3.detach()])\n",
    "            infeasibilitytrain_epoch[i,:] = infeasibility_batch.detach()\n",
    "            infeasibility_count[i,:] = infeasible_cases.detach()\n",
    "        \n",
    "            #time_contflow[i,:] = time_contflow_batch\n",
    "            #memory_contflow[i,:] = memory_contflow_batch\n",
    "            #backward_contflow[i,:] = backward_batch\n",
    "            \n",
    "            #====== N-1\n",
    "            crit_violation_count_N1[i,:] = count_crit_violation_N1.detach()\n",
    "            line_violation_count_N1[i,:] = count_line_violation_N1.detach()\n",
    "            \n",
    "            #====== N-2\n",
    "            crit_violation_count_N2[i,:] = count_crit_violation_N2.detach()\n",
    "            line_violation_count_N2[i,:] = count_line_violation_N2.detach()\n",
    "            \n",
    "            #====== N-3\n",
    "            crit_violation_count_N3[i,:] = count_crit_violation_N3.detach()\n",
    "            line_violation_count_N3[i,:] = count_line_violation_N3.detach()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (Xbatch, cost_scopf_batch) in enumerate(tqdm(test_loader)):\n",
    "                \n",
    "                model.eval() # deactivate dropout\n",
    "\n",
    "                Xbatch = Xbatch.clone().detach().to(device)\n",
    "                cost_scopf_batch = cost_scopf_batch.clone().detach().to(device)\n",
    "\n",
    "                Xtest_batch = ((Xbatch.cpu() * Xstd) + Xmean).to(torch.float32).to(device)\n",
    "\n",
    "                #================== Perform forward pass\n",
    "                clil = (model(Xbatch) + 1) / 2\n",
    "\n",
    "                #Pgi, Fli, Thi, lam1i, lam2i = cvxpylayer0(Xtest_batch, gencost, clil, solver_args=ECOS_solver_args)\n",
    "                Pgi, Fli, Thi, lam1i, lam2i = cvxpylayer0(Xtest_batch, clil, solver_args=ECOS_solver_args)\n",
    "                Pgis = Pgi.T\n",
    "                Flis = Fli.T.to(torch.float32)\n",
    "\n",
    "                #================ compute cost and infeasibility\n",
    "                infeasibility_batch = torch.sum(lam1i) + torch.sum(lam2i)\n",
    "\n",
    "                batch_cost = torch.matmul(gencost, Pgis)\n",
    "                batch_relco = ((batch_cost - cost_scopf_batch) / cost_scopf_batch) * 100\n",
    "\n",
    "                #===================== compute contingency flows for each sample\n",
    "                imbalancetest_batch_N1 = 0\n",
    "                imbalancetest_batch_N2 = 0\n",
    "                imbalancetest_batch_N3 = 0\n",
    "                imbalancetest_batch = 0\n",
    "                \n",
    "                expected_flows = torch.zeros(size = (l,batch_size),dtype = torch.float32, requires_grad = False)\n",
    "                expected_flows_N1 = torch.zeros(size = (l,batch_size),dtype = torch.float32, requires_grad = False)\n",
    "                expected_flows_N2 = torch.zeros(size = (l,batch_size),dtype = torch.float32, requires_grad = False)\n",
    "                expected_flows_N3 = torch.zeros(size = (l,batch_size),dtype = torch.float32, requires_grad = False)\n",
    "\n",
    "                #============ N-1\n",
    "                for b in (range(tot_lodf_batches_N1)):\n",
    "                    #print('RAM Used (GB):', psutil.virtual_memory()[3]/1000000000)\n",
    "                    lodf_batch = lodf_batch_size_N1\n",
    "                    if b == (tot_lodf_batches_N1 - 1): \n",
    "                        lodf_batch = last_batch_size_N1\n",
    "                    left_ones = torch.ones((lodf_batch,1))\n",
    "                    right_ones = torch.ones((int(l*batch_size),1))\n",
    "                    Fli_0 = Flis.expand(lodf_batch, -1, -1)\n",
    "                    \n",
    "                    if b != (tot_lodf_batches_N1 - 1): \n",
    "                        start_prob = b*lodf_batch*l\n",
    "                        end_prob = (b+1)*lodf_batch*l\n",
    "                        P_outage = probabilities_N1[start_prob:end_prob]\n",
    "                    if b == (tot_lodf_batches_N1 - 1): \n",
    "                        end_prob = Ncontingencies_N1*l\n",
    "                        start_prob = (Ncontingencies_N1*l) - (last_batch_size_N1*l)\n",
    "                        P_outage = probabilities_N1[-(last_batch_size_N1*l):]\n",
    "\n",
    "                    index1 = LODF_dict_N1[b][0].detach()\n",
    "                    index2 = LODF_dict_N1[b][1].detach()\n",
    "                    indices = torch.stack((index1,index2)).detach()\n",
    "                    values = LODF_dict_N1[b][2].detach()\n",
    "                    shape = (l,int(l*lodf_batch))\n",
    "                    lodf = torch.sparse_coo_tensor(indices,values,size = shape, requires_grad = False).T.to(device).detach()\n",
    "\n",
    "                    imbalancetest_batch_loop = (F.relu(torch.abs(Fli_0 + torch.sparse.mm(lodf,Flis).reshape((lodf_batch,l,batch_size)))- max_f)).detach()\n",
    "                    imbalancetest_batch_loop = imbalancetest_batch_loop.reshape(int(l*lodf_batch), -1)\n",
    "                    expected_flows_N1 += torch.mul(P_outage,imbalancetest_batch_loop).to(torch.float32).reshape((lodf_batch,l,batch_size)).sum(dim=0)\n",
    "\n",
    "                    imbalancetest_batch_N1 += torch.sum(imbalancetest_batch_loop)/int(batch_size)\n",
    "\n",
    "                #============ N-2\n",
    "                for b in (range(tot_lodf_batches_N2)):\n",
    "                    #print('RAM Used (GB):', psutil.virtual_memory()[3]/1000000000)\n",
    "                    lodf_batch = lodf_batch_size_N2\n",
    "                    if b == (tot_lodf_batches_N2 - 1): \n",
    "                        lodf_batch = last_batch_size_N2\n",
    "                    left_ones = torch.ones((lodf_batch,1))\n",
    "                    right_ones = torch.ones((int(l*batch_size),1))\n",
    "                    Fli_0 = Flis.expand(lodf_batch, -1, -1)\n",
    "                    \n",
    "                    if b != (tot_lodf_batches_N2 - 1): \n",
    "                        start_prob = b*lodf_batch*l\n",
    "                        end_prob = (b+1)*lodf_batch*l\n",
    "                        P_outage = probabilities_N2[start_prob:end_prob]\n",
    "                    if b == (tot_lodf_batches_N2 - 1): \n",
    "                        end_prob = Ncontingencies_N2*l\n",
    "                        start_prob = (Ncontingencies_N2*l) - (last_batch_size_N2*l)\n",
    "                        P_outage = probabilities_N2[-(last_batch_size_N2*l):]\n",
    "\n",
    "                    index1 = LODF_dict_N2[b][0].detach()\n",
    "                    index2 = LODF_dict_N2[b][1].detach()\n",
    "                    indices = torch.stack((index1,index2)).detach()\n",
    "                    values = LODF_dict_N2[b][2].detach()\n",
    "                    shape = (l,int(l*lodf_batch))\n",
    "                    lodf = torch.sparse_coo_tensor(indices,values,size = shape, requires_grad = False).T.to(device).detach()\n",
    "\n",
    "                    imbalancetest_batch_loop = (F.relu(torch.abs(Fli_0 + torch.sparse.mm(lodf,Flis).reshape((lodf_batch,l,batch_size)))- max_f)).detach()\n",
    "                    imbalancetest_batch_loop = imbalancetest_batch_loop.reshape(int(l*lodf_batch), -1)\n",
    "                    expected_flows_N2 += torch.mul(P_outage,imbalancetest_batch_loop).to(torch.float32).reshape((lodf_batch,l,batch_size)).sum(dim=0)\n",
    "\n",
    "                    imbalancetest_batch_N2 += torch.sum(imbalancetest_batch_loop)/int(batch_size)\n",
    "\n",
    "\n",
    "                #============ N-3\n",
    "                for b in (range(tot_lodf_batches_N3)):\n",
    "                    #print('RAM Used (GB):', psutil.virtual_memory()[3]/1000000000)\n",
    "                    lodf_batch = lodf_batch_size_N3\n",
    "                    if b == (tot_lodf_batches_N3 - 1): \n",
    "                        lodf_batch = last_batch_size_N3\n",
    "                    left_ones = torch.ones((lodf_batch,1))\n",
    "                    right_ones = torch.ones((int(l*batch_size),1))\n",
    "                    Fli_0 = Flis.expand(lodf_batch, -1, -1)\n",
    "                    \n",
    "                    if b != (tot_lodf_batches_N3 - 1): \n",
    "                        start_prob = b*lodf_batch*l\n",
    "                        end_prob = (b+1)*lodf_batch*l\n",
    "                        P_outage = probabilities_N3[start_prob:end_prob]\n",
    "                    if b == (tot_lodf_batches_N3 - 1): \n",
    "                        end_prob = Ncontingencies_N3*l\n",
    "                        start_prob = (Ncontingencies_N3*l) - (last_batch_size_N3*l)\n",
    "                        P_outage = probabilities_N3[-(last_batch_size_N3*l):]\n",
    "\n",
    "                    index1 = LODF_dict_N3[b][0].detach()\n",
    "                    index2 = LODF_dict_N3[b][1].detach()\n",
    "                    indices = torch.stack((index1,index2)).detach()\n",
    "                    values = LODF_dict_N3[b][2].detach()\n",
    "                    shape = (l,int(l*lodf_batch))\n",
    "                    lodf = torch.sparse_coo_tensor(indices,values,size = shape, requires_grad = False).T.to(device).detach()\n",
    "\n",
    "                    imbalancetest_batch_loop = (F.relu(torch.abs(Fli_0 + torch.sparse.mm(lodf,Flis).reshape((lodf_batch,l,batch_size)))- max_f)).detach()\n",
    "                    imbalancetest_batch_loop = imbalancetest_batch_loop.reshape(int(l*lodf_batch), -1)\n",
    "                    expected_flows_N3 += torch.mul(P_outage,imbalancetest_batch_loop).to(torch.float32).reshape((lodf_batch,l,batch_size)).sum(dim=0)\n",
    "\n",
    "                    imbalancetest_batch_N3 += torch.sum(imbalancetest_batch_loop)/int(batch_size) \n",
    "                \n",
    "                expected_inf_N1 = torch.sum(expected_flows_N1)/int(batch_size)\n",
    "                expected_inf_N2 = torch.sum(expected_flows_N2)/int(batch_size)\n",
    "                expected_inf_N3 = torch.sum(expected_flows_N3)/int(batch_size)\n",
    "\n",
    "                expected_infeasible_N1 = (torch.count_nonzero(expected_flows_N1)/torch.numel(expected_flows_N1))*100\n",
    "                expected_infeasible_N2 = (torch.count_nonzero(expected_flows_N2)/torch.numel(expected_flows_N2))*100\n",
    "                expected_infeasible_N3 = (torch.count_nonzero(expected_flows_N3)/torch.numel(expected_flows_N3))*100\n",
    "                torch.set_printoptions(precision=9)\n",
    "\n",
    "                imbalancetest_batch = expected_inf_N1+expected_inf_N2+expected_inf_N3\n",
    "                #imbalancetest_batch_N1 + imbalancetest_batch_N2 + imbalancetest_batch_N3\n",
    "        \n",
    "                relcotest_epoch[i,:] = batch_relco.detach()\n",
    "                imbalancetest_epoch[i,:] = torch.tensor([expected_inf_N1.detach(),expected_inf_N2.detach(),expected_inf_N3.detach()])\n",
    "                expected_viol_test_epoch[i,:] = torch.tensor([expected_infeasible_N1.detach(),expected_infeasible_N2.detach(),expected_infeasible_N3.detach()])\n",
    "                infeasibilitytest_epoch[i,:] = infeasibility_batch.detach()\n",
    "            \n",
    "    time_nnSCS = time.time() - time_nnSCS  \n",
    "\n",
    "    imbalancetrain = torch.sum(imbalancetrain_epoch, dim=0).detach()\n",
    "    imbalancetest = torch.sum(imbalancetest_epoch, dim=0).detach()\n",
    "    \n",
    "    expected_violations_train = torch.mean(expected_viol_epoch, dim=0).detach()\n",
    "    expected_violations_test = torch.mean(expected_viol_test_epoch, dim=0).detach()\n",
    "    \n",
    "    infeasibilitytrain = torch.mean(infeasibilitytrain_epoch).detach()\n",
    "    infeasibilitytest = torch.mean(infeasibilitytest_epoch).detach()\n",
    "    \n",
    "    relcotrain = torch.mean(relcotrain_epoch).detach()\n",
    "    relcotest = torch.mean(relcotest_epoch).detach()\n",
    "    \n",
    "    #======== N-1\n",
    "    crit_violation_tot_N1 = torch.sum(crit_violation_count_N1).detach()\n",
    "    line_violation_tot_N1 = torch.sum(line_violation_count_N1).detach()\n",
    "    \n",
    "    #======== N-1\n",
    "    crit_violation_tot_N2 = torch.sum(crit_violation_count_N2).detach()\n",
    "    line_violation_tot_N2 = torch.sum(line_violation_count_N2).detach()\n",
    "    \n",
    "    #======== N-1\n",
    "    crit_violation_tot_N3 = torch.sum(crit_violation_count_N3).detach()\n",
    "    line_violation_tot_N3 = torch.sum(line_violation_count_N3).detach()\n",
    "    \n",
    "    #====== convergence criteria\n",
    "    infeasible_cases_tot = torch.sum(infeasibility_count)\n",
    "    \n",
    "    tot_violation_percentage = torch.mean(violation_percentage)\n",
    "    tot_time_contflow = torch.sum(time_contflow)\n",
    "    tot_memory_contflow = torch.mean(memory_contflow)\n",
    "    tot_backward_contflow = torch.sum(backward_contflow)\n",
    "    \n",
    "    #============= \n",
    "    error[epoch,:] = torch.hstack((imbalancetrain,imbalancetest)).detach().numpy()\n",
    "    exp_inf[epoch,:] = torch.hstack((expected_violations_train, expected_violations_test)).detach().numpy()\n",
    "    avg_infeasibility[epoch,:] =  torch.stack((infeasibilitytrain,infeasibilitytest)).detach().numpy()\n",
    "    relcost[epoch,:] =  torch.stack((relcotrain,relcotest)).detach().numpy()\n",
    "    \n",
    "    #================= computational graph\n",
    "    time_CF[epoch,:] = tot_time_contflow\n",
    "    memory_CF[epoch,:] = tot_memory_contflow \n",
    "    percentage_CF[epoch,:] = tot_violation_percentage\n",
    "    backward_CF[epoch,:] = tot_backward_contflow\n",
    "\n",
    "    print(epoch, \" | \", round(time_nnSCS,0), \" | \",np.round(error[epoch,:],3), \" | \",np.round(exp_inf[epoch,:],3), \" | \", np.round(avg_infeasibility[epoch,:],3), \" | \", np.round(relcost[epoch,:], 3) )\n",
    "    \n",
    "    #======================= Convergence check infeasible cases\n",
    "    print('current infeasibility:', (infeasible_cases_tot/int(train*Nsamples))*100, '%', 'goal:', 1, '%')\n",
    "    \n",
    "    print('')\n",
    "    #print('N1 infeasible contingency cases:', line_violation_tot_N1, (line_violation_tot_N1/(Ncontingencies_N1*Nsamples))*100, '%')\n",
    "    #print('N1 10%+ contingency cases:', crit_violation_tot_N1, (crit_violation_tot_N1/(Ncontingencies_N1*Nsamples))*100, '%')\n",
    "    print('N1 expected avg violation:', error[epoch,3])\n",
    "    \n",
    "    print('')\n",
    "    #print('N2 infeasible contingency cases:', line_violation_tot_N2, (line_violation_tot_N2/(Ncontingencies_N2*Nsamples))*100, '%')\n",
    "    #print('N2 10%+ contingency cases:', crit_violation_tot_N2, (crit_violation_tot_N2/(Ncontingencies_N2*Nsamples))*100, '%')\n",
    "    print('N2 expected avg violation:', error[epoch,4])\n",
    "    \n",
    "    print('')\n",
    "    #print('N3 infeasible contingency cases:', line_violation_tot_N3, (line_violation_tot_N3/(Ncontingencies_N3*Nsamples))*100, '%')\n",
    "    #print('N3 10%+ contingency cases:', crit_violation_tot_N3, (crit_violation_tot_N3/(Ncontingencies_N3*Nsamples))*100, '%')\n",
    "    print('N3 expected avg violation:', error[epoch,5])\n",
    "    print('')\n",
    "\n",
    "    print('flow penalty:', flow_penalty*imbalancetrain_batch)\n",
    "    print('infeasible penalty:', infeasibility_penalty*infeasibility_loss)\n",
    "    print('cost penalty:', cost_penalty*cost_loss)\n",
    "    print('soft loss:', soft_penalty*Fl_base)\n",
    "    #print('extra;', loss_term)\n",
    "    \n",
    "        \n",
    "end = time.time()\n",
    "print('RAM Used (GB):', psutil.virtual_memory()[3]/1000000000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc36031",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_path = r'C:\\Users\\bgiraud\\Documents\\Thesis Bastien\\I - Coding\\01 - learning SCOPFs\\01 - GLODF\\trained models\\Nk models\\ieee39_Nk_25k_1_soft.pt'\n",
    "file_path = r'C:\\Users\\bgiraud\\Documents\\Thesis Bastien\\I - Coding\\01 - learning SCOPFs\\01 - GLODF\\trained models\\Nk models\\ieee39_new_prob.pt'\n",
    "\n",
    "\n",
    "# Save the model\n",
    "#torch.save(model.state_dict(), file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b5c5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_path = r'C:\\Users\\bgiraud\\Documents\\Thesis Bastien\\I - Coding\\01 - learning SCOPFs\\01 - GLODF\\trained models\\Nk models\\ieee39_Nk_25000_1.pt'\n",
    "\n",
    "# Load the saved model state dictionary\n",
    "model.load_state_dict(torch.load(file_path))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "print('RAM Used (GB):', psutil.virtual_memory()[3]/1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b1c43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#totaltime = start - end\n",
    "#print(\"total time:\", totaltime)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(error[:l, 0], 'r', label='N-1 train')\n",
    "plt.plot(error[:l, 1], 'g', label='N-2 train')\n",
    "plt.plot(error[:l, 2], 'b', label='N-3 train')\n",
    "\n",
    "# Plot the N-1 test, N-2 test, and N-3 test columns\n",
    "plt.plot(error[:l, 3], 'r--', label='N-1 test')\n",
    "plt.plot(error[:l, 4], 'g--', label='N-2 test')\n",
    "plt.plot(error[:l, 5], 'b--', label='N-3 test')\n",
    "#new_list = 10*range(0, 10,l+1)\n",
    "#plt.xticks(new_list)\n",
    "plt.ylabel('Errors')\n",
    "plt.xlabel('Iterations')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.plot(exp_inf[:l, 0], 'r', label='N-1 train')\n",
    "plt.plot(exp_inf[:l, 1], 'g', label='N-2 train')\n",
    "plt.plot(exp_inf[:l, 2], 'b', label='N-3 train')\n",
    "\n",
    "# Plot the N-1 test, N-2 test, and N-3 test columns\n",
    "plt.plot(exp_inf[:l, 3], 'r--', label='N-1 test')\n",
    "plt.plot(exp_inf[:l, 4], 'g--', label='N-2 test')\n",
    "plt.plot(exp_inf[:l, 5], 'b--', label='N-3 test')\n",
    "#new_list = 10*range(0, 10,l+1)\n",
    "#plt.xticks(new_list)\n",
    "plt.ylabel('Percentage')\n",
    "plt.xlabel('Iterations')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1cd80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nsamples = 1000\n",
    "#X = np.array(data['load']['p'])/Sbase*ls.kumaraswamymontecarlo(1.6, 2.8, 0.75, LB, UB, Nsamples).T\n",
    "file_path = r'C:\\Users\\bgiraud\\Documents\\Thesis Bastien\\I - Coding\\01 - learning SCOPFs\\01 - GLODF\\trained models\\training data\\Xtest1.pkl' \n",
    "with open(file_path, 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "Xtrain = torch.tensor(X[0:int(Nsamples),:],dtype = torch.float32)        \n",
    "Xtrain_transpose = Xtrain.transpose(0,1)\n",
    "#Xtest = torch.tensor(X[int(train*Nsamples):,:],dtype = torch.float64)\n",
    "#standardise\n",
    "Xmin, Xmax,Xmean,Xstd = np.min(X, axis = 0),np.max(X, axis = 0),np.mean(X, axis = 0),np.std(X, axis = 0)\n",
    "Xscal = torch.tensor(( X - Xmean ) / Xstd, dtype=torch.float32)\n",
    "\n",
    "frequency = 0.0051\n",
    "repair_time = 44\n",
    "line_avg = torch.mean(line_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2249343",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============== Benchmark #1 contingency screening N-k\n",
    "benchmark = 'risk_based' #'SCOPF'\n",
    "\n",
    "if benchmark == 'SCOPF':\n",
    "    lcontingencies = range(l)\n",
    "    islanding_cases = [15,20]\n",
    "    lcontingencies_filtered = [l for l in lcontingencies if l not in islanding_cases]\n",
    "    lcontingencies = lcontingencies_filtered\n",
    "    N1_contingencies = lcontingencies\n",
    "elif benchmark == 'SCOPF1screening':\n",
    "    Nk_contingencies = np.array(list(itertools.combinations(range(Nlines),1)))\n",
    "elif benchmark == 'SCOPF2screening':\n",
    "    Nk_contingencies = np.array(list(itertools.combinations(range(Nlines),2)))\n",
    "elif benchmark == 'SCOPF3screening':\n",
    "    Nk_contingencies = np.array(list(itertools.combinations(range(Nlines),3)))\n",
    "elif benchmark == 'risk_based':\n",
    "    N1_contingencies = np.array(list(itertools.combinations(range(Nlines),1)))\n",
    "    N2_contingencies = np.array(list(itertools.combinations(range(Nlines),2)))\n",
    "\n",
    "Pgisscopf = torch.zeros(Ngens,int(Nsamples),dtype = torch.float32)\n",
    "Fl0sscopf = torch.zeros(Nlines,int(Nsamples),dtype = torch.float32)\n",
    "#Flcs = torch.zeros(Nlines,len(Nk_contingencies),int(Nsamples),dtype = torch.float32)\n",
    "lam1s = torch.zeros(Nbus,int(Nsamples),dtype = torch.float32)\n",
    "lam2s = torch.zeros(Nbus,int(Nsamples),dtype = torch.float32)\n",
    "lam1sc = torch.zeros(Nbus,52,int(Nsamples),dtype = torch.float32)\n",
    "lam2sc = torch.zeros(Nbus,52,int(Nsamples),dtype = torch.float32)\n",
    "Th0 = torch.zeros(Nbus,int(Nsamples),dtype = torch.float32)\n",
    "\n",
    "#======== remove islanding cases from contingency list\n",
    "if benchmark != 'SCOPF':\n",
    "    lcontingencies = [0]\n",
    "    l2contingencies = [1]\n",
    "screening_iterations = 2\n",
    "\n",
    "time_scopfs = time.time()\n",
    "\n",
    "for i in range(screening_iterations):\n",
    "    imbalance_list1 = []\n",
    "    imbalance_list2 = []\n",
    "    \n",
    "    N1_zeros = list(range(len(N1_contingencies)))\n",
    "    zero_indices = [l for l in N1_zeros if l not in lcontingencies]\n",
    "    \n",
    "    islanding_cases = [15,20]\n",
    "    lcontingencies_filtered = [l for l in lcontingencies if l not in islanding_cases]\n",
    "    lcontingencies = lcontingencies_filtered\n",
    "    print(lcontingencies)\n",
    "    \n",
    "    if benchmark != 'SCOPF':\n",
    "        islanding_cases2 = [0, 12, 13, 14, 19, 43, 44, 45, 50, 75, 80, 104, 109, 113, 117, 118, 132, 137, 159, 164, 167\\\n",
    "                            , 185, 190, 210, 215, 234, 239, 257, 262, 276, 279, 284, 300, 301, 305, 320, 325, 338, 339, 344,\\\n",
    "                            357, 362, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 395,\\\n",
    "                            408, 409, 410, 423, 424, 437, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 465,\\\n",
    "                            466, 467, 475, 476, 477, 495, 496, 500, 501, 507, 518, 525, 526, 527]\n",
    "        lcontingencies_filtered2 = [l for l in l2contingencies if l not in islanding_cases2]\n",
    "        l2contingencies = lcontingencies_filtered2\n",
    "        print(l2contingencies)\n",
    "    \n",
    "\n",
    "    if benchmark == 'SCOPF':\n",
    "        problem0scopf, Pd0scopf, cg0scopf, Pgiscopf, Fl0scopf, Th0scopf, Flc, lam1, lam2, lam1c, lam2c = cs.create_scopf(data, lcontingencies)\n",
    "    elif benchmark == 'SCOPF1screening':\n",
    "        problem0scopf, Pd0scopf, cg0scopf, Pgiscopf, Fl0scopf, Th0scopf, Flc, lam1, lam2, lam1c, lam2c = cs.create_scopf1_screening(data, lcontingencies)\n",
    "    elif benchmark == 'SCOPF2screening':\n",
    "        problem0scopf, Pd0scopf, cg0scopf, Pgiscopf, Fl0scopf, Th0scopf, Flc, lam1, lam2, lam1c, lam2c = cs.create_scopf2_screening(data, lcontingencies)\n",
    "    elif benchmark == 'SCOPF3screening':\n",
    "        problem0scopf, Pd0scopf, cg0scopf, Pgiscopf, Fl0scopf, Th0scopf, Flc, lam1, lam2, lam1c, lam2c = cs.create_scopf3_screening(data, lcontingencies)\n",
    "    elif benchmark == 'risk_based':\n",
    "        problem0scopf, Pd0scopf, cg0scopf, Pgiscopf, Fl0scopf, Th0scopf, Flc, lam1, lam2, lam1c, lam2c = cs.create_scopf_risk_based(data, lcontingencies, l2contingencies)\n",
    "    \n",
    "\n",
    "    cg0scopf.value = np.array(data['gen']['cost'])\n",
    "    for entry in tqdm(range(int(Nsamples)),position=0, leave=True):   \n",
    "        Pd0scopf.value = X[entry,:] #np.array(data['load']['p'])/Sbase    \n",
    "        try:\n",
    "            solution  = problem0scopf.solve(solver=cp.ECOS)    # cp.MOSEK\n",
    "            if problem0scopf.status in [\"infeasible\", \"unbounded\"]:\n",
    "                # Otherwise, problem.value is inf or -inf, respectively.\n",
    "                print(\"Problem infeasible or unbounded\")\n",
    "                #print(\"Optimal value: %s\" % problem0scopf.value)\n",
    "            Pgisscopf[:,entry] = torch.tensor(Pgiscopf.value)\n",
    "            Fl0sscopf[:,entry] = torch.tensor(Fl0scopf.value)\n",
    "            Th0[:,entry] = torch.tensor(Th0scopf.value)\n",
    "\n",
    "            lam1s[:,entry] = torch.tensor(lam1.value)\n",
    "            lam2s[:,entry] = torch.tensor(lam2.value)\n",
    "            if i != 0:\n",
    "                lam1sc[:,:,entry] = torch.tensor(lam1c.value)\n",
    "                lam2sc[:,:,entry] = torch.tensor(lam2c.value)\n",
    "        except cp.error.SolverError:\n",
    "            Pgisscopf[:,entry] = Pgisscopf[:,(entry-1)]\n",
    "            Fl0sscopf[:,entry] = Fl0sscopf[:,(entry-1)]\n",
    "            Th0[:,entry] = Th0[:,(entry-1)]\n",
    "\n",
    "            lam1s[:,entry] = lam1s[:,(entry-1)]\n",
    "            lam2s[:,entry] = lam2s[:,(entry-1)]\n",
    "            lam1sc[:,:,entry] = lam1sc[:,:,(entry-1)]\n",
    "            lam2sc[:,:,entry] = lam1sc[:,:,(entry-1)]\n",
    "            \n",
    "            print(\"Solver unable to solve for entry:\", entry)\n",
    "            continue\n",
    "        \n",
    "    \n",
    "    cost_scopf = torch.matmul(gencost,Pgisscopf)\n",
    "    \n",
    "    if benchmark == 'SCOPF':\n",
    "        break\n",
    "        \n",
    "    if i == (screening_iterations - 1):\n",
    "        break\n",
    "\n",
    "    #============ compute N1 contingencies\n",
    "    Flis = Fl0sscopf\n",
    "                \n",
    "    for b in (range(tot_lodf_batches_N1)):\n",
    "        lodf_batch = lodf_batch_size_N1\n",
    "        if b == (tot_lodf_batches_N1 - 1):\n",
    "            lodf_batch = last_batch_size_N1\n",
    "        left_ones = torch.ones((lodf_batch,1))\n",
    "        right_ones = torch.ones((int(l*batch_size),1))\n",
    "        Fli_0 = Flis.expand(lodf_batch, -1, -1)\n",
    "\n",
    "        index1 = LODF_dict_N1[b][0].detach()\n",
    "        index2 = LODF_dict_N1[b][1].detach()\n",
    "        indices = torch.stack((index1,index2)).detach()\n",
    "        values = LODF_dict_N1[b][2].detach()\n",
    "        shape = (l,int(l*lodf_batch))\n",
    "        lodf = torch.sparse_coo_tensor(indices,values,size = shape, requires_grad = False).T.to(device).detach()\n",
    "\n",
    "        imbalancetrain_batch_loop = (F.relu(torch.abs(Fli_0 + torch.sparse.mm(lodf,Flis).reshape((lodf_batch,l,Nsamples)))- max_f)).detach()\n",
    "\n",
    "        #======= get indices of contingencies of highest violations\n",
    "        summed_tensor = torch.sum(torch.sum(imbalancetrain_batch_loop, dim=1), dim=1)\n",
    "\n",
    "        if b == 0:\n",
    "            desired_length = len(summed_tensor)\n",
    "\n",
    "        padding_length = desired_length - len(summed_tensor)\n",
    "        padded_tensor = F.pad(summed_tensor, (0, padding_length), value=0)\n",
    "        imbalance_list1.extend(padded_tensor)\n",
    "\n",
    "\n",
    "    indices_per_iteration = 32 # len(summed_tensor)\n",
    "\n",
    "    stacked_imbalance = torch.stack(imbalance_list1, dim=0)\n",
    "    highest_indices = (np.argsort(stacked_imbalance.cpu())[-indices_per_iteration:]).tolist()\n",
    "    new_indices = [idx for idx in highest_indices if idx not in lcontingencies] # Filter out the indices that are already in the list\n",
    "    lcontingencies.extend(new_indices) # Append the new indices to the list\n",
    "        \n",
    "    if benchmark == 'risk_based': \n",
    "    #============ compute N2 contingencies\n",
    "    \n",
    "        for b in (range(tot_lodf_batches_N2)):\n",
    "            #print('RAM Used (GB):', psutil.virtual_memory()[3]/1000000000)\n",
    "            lodf_batch = lodf_batch_size_N2\n",
    "            if b == (tot_lodf_batches_N2 - 1): \n",
    "                lodf_batch = last_batch_size_N2\n",
    "            left_ones = torch.ones((lodf_batch,1))\n",
    "            right_ones = torch.ones((int(l*batch_size),1))\n",
    "            Fli_0 = Flis.expand(lodf_batch, -1, -1)\n",
    "\n",
    "            if b != (tot_lodf_batches_N2 - 1): \n",
    "                start_prob = b*lodf_batch*l\n",
    "                end_prob = (b+1)*lodf_batch*l\n",
    "                P_outage = probabilities_N2[start_prob:end_prob]\n",
    "            if b == (tot_lodf_batches_N2 - 1): \n",
    "                end_prob = Ncontingencies_N2*l\n",
    "                start_prob = (Ncontingencies_N2*l) - (last_batch_size_N2*l)\n",
    "                P_outage = probabilities_N2[-(last_batch_size_N2*l):]\n",
    "\n",
    "            index1 = LODF_dict_N2[b][0].detach()\n",
    "            index2 = LODF_dict_N2[b][1].detach()\n",
    "            indices = torch.stack((index1,index2)).detach()\n",
    "            values = LODF_dict_N2[b][2].detach()\n",
    "            shape = (l,int(l*lodf_batch))\n",
    "            lodf = torch.sparse_coo_tensor(indices,values,size = shape, requires_grad = False).T.to(device).detach()\n",
    "\n",
    "            imbalancetrain_batch_loop = (F.relu(torch.abs(Fli_0 + torch.sparse.mm(lodf, Flis).reshape((lodf_batch, l, Nsamples))) - max_f)).detach().to(device)\n",
    "            #imbalancetrain_batch_loop = imbalancetrain_batch_loop.reshape(int(l * lodf_batch), -1)\n",
    "            #expected_violation = torch.mul(P_outage,imbalancetrain_batch_loop).to(torch.float32).reshape((lodf_batch, l, Nsamples))\n",
    "     \n",
    "            #======= get indices of contingencies of highest violations\n",
    "            summed_tensor = torch.sum(torch.sum(imbalancetrain_batch_loop, dim=1), dim=1)\n",
    "\n",
    "            if b == 0:\n",
    "                desired_length = len(summed_tensor)\n",
    "\n",
    "            padding_length = desired_length - len(summed_tensor)\n",
    "            padded_tensor = F.pad(summed_tensor, (0, padding_length), value=0)\n",
    "            imbalance_list2.extend(padded_tensor)\n",
    "\n",
    "\n",
    "        indices_per_iteration = 20 # len(summed_tensor)\n",
    "\n",
    "        stacked_imbalance = torch.stack(imbalance_list2, dim=0)\n",
    "        highest_indices = (np.argsort(stacked_imbalance.cpu())[-indices_per_iteration:]).tolist()\n",
    "        new_indices = [idx for idx in highest_indices if idx not in l2contingencies] # Filter out the indices that are already in the list\n",
    "        l2contingencies.extend(new_indices) # Append the new indices to the list\n",
    "\n",
    "    \n",
    "time_scopfs = time.time() - time_scopfs\n",
    "print('contingency screening time:', time_scopfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfaf171",
   "metadata": {},
   "outputs": [],
   "source": [
    "#============== infeasible n-1 contingency cases\n",
    "infeasibility_cont = lam1sc + lam2sc\n",
    "infeasibility_cont_N1 = infeasibility_cont[:,0:31,:]\n",
    "N1_counter = 0\n",
    "\n",
    "for i in range(len(infeasibility_cont_N1[0,:,0])):\n",
    "    intermediate = infeasibility_cont_N1[:,i,:]\n",
    "    intermediate[abs(intermediate) < 1e-5] = 0\n",
    "    intermediate_buses = (abs(intermediate) > 0) \n",
    "    intermediate_cases = torch.zeros((Nsamples,1))\n",
    "    row_has_true_inter = torch.any(intermediate_buses, axis=0)\n",
    "    intermediate_cases[row_has_true_inter] = True\n",
    "    N1_counter += intermediate_cases.sum()\n",
    "    \n",
    "\n",
    "print(N1_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc4f987",
   "metadata": {},
   "outputs": [],
   "source": [
    "##============= check infeasibility\n",
    "infeasibility_base = (lam1s + lam2s).permute(1,0)\n",
    "infeasibility_base[abs(infeasibility_base) < 1e-5] = 0\n",
    "infeasible_buses_base = (abs(infeasibility_base) > 1e-5) #0.001*load_profile) # 10% of total load available for generation/load shedding\n",
    "scopf_cost_average = cost_scopf.mean()\n",
    "\n",
    "infeasible_cases_base = torch.zeros((Nsamples,1))\n",
    "row_has_true_base = torch.any(infeasible_buses_base, axis=1)\n",
    "infeasible_cases_base[row_has_true_base] = True\n",
    "infeasible_cases_tot_base = infeasible_cases_base.sum()\n",
    "print('infeasible base cases:', infeasible_cases_tot_base)\n",
    "print('percentage base infeasibility:', (infeasible_cases_tot_base/(Nsamples))*100, '%')\n",
    "print('total average cost:', scopf_cost_average)\n",
    "\n",
    "print('RAM Used (GB):', psutil.virtual_memory()[3]/1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fb8e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flis = Fl0sscopf\n",
    "\n",
    "imbalancetrain_batch_N1 = 0\n",
    "imbalancetrain_batch_N2 = 0\n",
    "imbalancetrain_batch_N3 = 0\n",
    "imbalancetrain_batch = 0            \n",
    "\n",
    "count_crit_violation_N1 = 0\n",
    "count_line_violation_N1 = 0\n",
    "count_violation_N1 = 0\n",
    "\n",
    "count_crit_violation_N2 = 0\n",
    "count_line_violation_N2 = 0\n",
    "count_violation_N2 = 0\n",
    "\n",
    "count_crit_violation_N3 = 0\n",
    "count_line_violation_N3 = 0\n",
    "count_violation_N3 = 0\n",
    "\n",
    "expected_flows = torch.zeros(size = (l,Nsamples),dtype = torch.float32, requires_grad = True)\n",
    "expected_flows_N1 = torch.zeros(size = (l,Nsamples),dtype = torch.float32, requires_grad = True)\n",
    "expected_flows_N2 = torch.zeros(size = (l,Nsamples),dtype = torch.float32, requires_grad = True)\n",
    "expected_flows_N3 = torch.zeros(size = (l,Nsamples),dtype = torch.float32, requires_grad = True)\n",
    "\n",
    "#============ N-1\n",
    "for b in (range(tot_lodf_batches_N1)):\n",
    "    #print('RAM Used (GB):', psutil.virtual_memory()[3]/1000000000)\n",
    "    lodf_batch = lodf_batch_size_N1\n",
    "    if b == (tot_lodf_batches_N1 - 1): \n",
    "        lodf_batch = last_batch_size_N1\n",
    "    left_ones = torch.ones((lodf_batch,1))\n",
    "    right_ones = torch.ones((int(l*batch_size),1))\n",
    "    Fli_0 = Flis.expand(lodf_batch, -1, -1)\n",
    "\n",
    "    if b != (tot_lodf_batches_N1 - 1): \n",
    "        start_prob = b*lodf_batch*l\n",
    "        end_prob = (b+1)*lodf_batch*l\n",
    "        P_outage = probabilities_N1[start_prob:end_prob]\n",
    "    if b == (tot_lodf_batches_N1 - 1): \n",
    "        end_prob = Ncontingencies_N1*l\n",
    "        start_prob = (Ncontingencies_N1*l) - (last_batch_size_N1*l)\n",
    "        P_outage = probabilities_N1[-(last_batch_size_N1*l):]\n",
    "\n",
    "    index1 = LODF_dict_N1[b][0].detach()\n",
    "    index2 = LODF_dict_N1[b][1].detach()\n",
    "    indices = torch.stack((index1,index2)).detach()\n",
    "    values = LODF_dict_N1[b][2].detach()\n",
    "    shape = (l,int(l*lodf_batch))\n",
    "    lodf = torch.sparse_coo_tensor(indices,values,size = shape, requires_grad = False).T.to(device).detach()\n",
    "\n",
    "    imbalancetrain_batch_loop = (F.relu(torch.abs(Fli_0 + torch.sparse.mm(lodf, Flis).reshape((lodf_batch, l, Nsamples))) - 1.1*max_f)).detach().to(device)\n",
    "    count_crit_violation_loop = torch.sum(imbalancetrain_batch_loop > 0, dim=1)\n",
    "    count_line_violation_loop = torch.sum(imbalancetrain_batch_loop > 0, dim=1)   \n",
    "    LOLP_N1 = torch.where(count_line_violation_loop != 0, torch.tensor(1, dtype=count_line_violation_loop.dtype), count_line_violation_loop)\n",
    "    LOLP_N1 = torch.mul(P_outage[0:Ncontingencies_N1],LOLP_N1)\n",
    "    \n",
    "    imbalancetrain_batch_loop = imbalancetrain_batch_loop.reshape(int(l * lodf_batch), -1)\n",
    "\n",
    "    #========= count the number of rows that have all zeros\n",
    "    mask = torch.all(imbalancetrain_batch_loop == 0, dim=1) \n",
    "    indices_remove = mask.nonzero().squeeze(1)\n",
    "    num_zero_rows = torch.sum(mask)\n",
    "\n",
    "    indices_tot = torch.arange(start_prob, end_prob)\n",
    "    mask_keep = ~torch.isin(indices_tot, indices_remove)\n",
    "    indices_keep = indices_tot[mask_keep]\n",
    "    indices_loc = indices_keep % l\n",
    "\n",
    "    #============ create new lodf\n",
    "    mask_flow = torch.ones(imbalancetrain_batch_loop.shape[0], dtype=torch.bool).to(device)\n",
    "    mask_flow[indices_remove] = False\n",
    "    lodf_test = lodf.to_dense()[mask_flow]\n",
    "    lodf_test = lodf_test.clone().detach()\n",
    "    lodf = lodf_test.to_sparse_coo().requires_grad_().to(torch.float32) \n",
    "    P_outage = P_outage[mask_flow]\n",
    "\n",
    "    Fli_0 = Flis.repeat(lodf_batch, 1).to(torch.float32).to(device)[mask_flow]\n",
    "    imbalancetrain_batch_loop = (F.relu(torch.abs(Fli_0+torch.sparse.mm(lodf,Flis))- max_f))\n",
    "    count_violation_loop = torch.count_nonzero(imbalancetrain_batch_loop)#/int(batch_size) \n",
    "    expected_violation = torch.mul(P_outage,imbalancetrain_batch_loop).to(torch.float32)\n",
    "\n",
    "    imbalancetrain_batch_N1 += torch.sum(imbalancetrain_batch_loop)/int(Nsamples)\n",
    "    expected_flows_N1 = torch.index_add(expected_flows_N1, 0, indices_loc.long(), expected_violation)\n",
    "\n",
    "    count_violation_N1 += count_violation_loop\n",
    "    count_crit_violation_N1 += torch.count_nonzero(count_crit_violation_loop.detach())\n",
    "    count_line_violation_N1 += torch.count_nonzero(count_line_violation_loop.detach())\n",
    "\n",
    "#============ N-2\n",
    "for b in (range(tot_lodf_batches_N2)):\n",
    "    #print('RAM Used (GB):', psutil.virtual_memory()[3]/1000000000)\n",
    "    lodf_batch = lodf_batch_size_N2\n",
    "    if b == (tot_lodf_batches_N2 - 1): \n",
    "        lodf_batch = last_batch_size_N2\n",
    "    left_ones = torch.ones((lodf_batch,1))\n",
    "    right_ones = torch.ones((int(l*batch_size),1))\n",
    "    Fli_0 = Flis.expand(lodf_batch, -1, -1)\n",
    "\n",
    "    if b != (tot_lodf_batches_N2 - 1): \n",
    "        start_prob = b*lodf_batch*l\n",
    "        end_prob = (b+1)*lodf_batch*l\n",
    "        P_outage = probabilities_N2[start_prob:end_prob]\n",
    "    if b == (tot_lodf_batches_N2 - 1): \n",
    "        end_prob = Ncontingencies_N2*l\n",
    "        start_prob = (Ncontingencies_N2*l) - (last_batch_size_N2*l)\n",
    "        P_outage = probabilities_N2[-(last_batch_size_N2*l):]\n",
    "\n",
    "    index1 = LODF_dict_N2[b][0].detach()\n",
    "    index2 = LODF_dict_N2[b][1].detach()\n",
    "    indices = torch.stack((index1,index2)).detach()\n",
    "    values = LODF_dict_N2[b][2].detach()\n",
    "    shape = (l,int(l*lodf_batch))\n",
    "    lodf = torch.sparse_coo_tensor(indices,values,size = shape, requires_grad = False).T.to(device).detach()\n",
    "\n",
    "    imbalancetrain_batch_loop = (F.relu(torch.abs(Fli_0 + torch.sparse.mm(lodf, Flis).reshape((lodf_batch, l, Nsamples))) - 1.1*max_f)).detach().to(device)\n",
    "    count_crit_violation_loop = torch.sum(imbalancetrain_batch_loop > 0, dim=1)\n",
    "    count_line_violation_loop = torch.sum(imbalancetrain_batch_loop > 0, dim=1)\n",
    "    LOLP_N2 = torch.where(count_line_violation_loop != 0, torch.tensor(1, dtype=count_line_violation_loop.dtype), count_line_violation_loop)\n",
    "    LOLP_N2 = torch.mul(P_outage[0:Ncontingencies_N2],LOLP_N2)\n",
    "    \n",
    "    imbalancetrain_batch_loop = imbalancetrain_batch_loop.reshape(int(l * lodf_batch), -1)\n",
    "\n",
    "    #========= count the number of rows that have all zeros\n",
    "    mask = torch.all(imbalancetrain_batch_loop == 0, dim=1) \n",
    "    indices_remove = mask.nonzero().squeeze(1)\n",
    "    num_zero_rows = torch.sum(mask)\n",
    "\n",
    "    indices_tot = torch.arange(start_prob, end_prob)\n",
    "    mask_keep = ~torch.isin(indices_tot, indices_remove)\n",
    "    indices_keep = indices_tot[mask_keep]\n",
    "    indices_loc = indices_keep % l\n",
    "\n",
    "    #============ create new lodf\n",
    "    mask_flow = torch.ones(imbalancetrain_batch_loop.shape[0], dtype=torch.bool).to(device)\n",
    "    mask_flow[indices_remove] = False\n",
    "    lodf_test = lodf.to_dense()[mask_flow]\n",
    "    lodf_test = lodf_test.clone().detach()\n",
    "    lodf = lodf_test.to_sparse_coo().requires_grad_().to(torch.float32) \n",
    "    P_outage = P_outage[mask_flow]\n",
    "\n",
    "    Fli_0 = Flis.repeat(lodf_batch, 1).to(torch.float32).to(device)[mask_flow]\n",
    "    imbalancetrain_batch_loop = (F.relu(torch.abs(Fli_0+torch.sparse.mm(lodf,Flis))- max_f))\n",
    "    count_violation_loop = torch.count_nonzero(imbalancetrain_batch_loop)#/int(batch_size) \n",
    "    expected_violation = torch.mul(P_outage,imbalancetrain_batch_loop).to(torch.float32)\n",
    "\n",
    "    imbalancetrain_batch_N2 += torch.sum(imbalancetrain_batch_loop)/int(Nsamples)\n",
    "    expected_flows_N2 = torch.index_add(expected_flows_N2, 0, indices_loc.long(), expected_violation)\n",
    "\n",
    "    count_violation_N2 += count_violation_loop\n",
    "    count_crit_violation_N2 += torch.count_nonzero(count_crit_violation_loop.detach())\n",
    "    count_line_violation_N2 += torch.count_nonzero(count_line_violation_loop.detach())\n",
    "\n",
    "\n",
    "#============ N-3\n",
    "for b in (range(tot_lodf_batches_N3)):\n",
    "    #print('RAM Used (GB):', psutil.virtual_memory()[3]/1000000000)\n",
    "    lodf_batch = lodf_batch_size_N3\n",
    "    if b == (tot_lodf_batches_N3 - 1): \n",
    "        lodf_batch = last_batch_size_N3\n",
    "    left_ones = torch.ones((lodf_batch,1))\n",
    "    right_ones = torch.ones((int(l*batch_size),1))\n",
    "    Fli_0 = Flis.expand(lodf_batch, -1, -1)\n",
    "\n",
    "    if b != (tot_lodf_batches_N3 - 1): \n",
    "        start_prob = b*lodf_batch*l\n",
    "        end_prob = (b+1)*lodf_batch*l\n",
    "        P_outage = probabilities_N3[start_prob:end_prob]\n",
    "    if b == (tot_lodf_batches_N3 - 1): \n",
    "        end_prob = Ncontingencies_N3*l\n",
    "        start_prob = (Ncontingencies_N3*l) - (last_batch_size_N3*l)\n",
    "        P_outage = probabilities_N3[-(last_batch_size_N3*l):]\n",
    "\n",
    "    index1 = LODF_dict_N3[b][0].detach()\n",
    "    index2 = LODF_dict_N3[b][1].detach()\n",
    "    indices = torch.stack((index1,index2)).detach()\n",
    "    values = LODF_dict_N3[b][2].detach()\n",
    "    shape = (l,int(l*lodf_batch))\n",
    "    lodf = torch.sparse_coo_tensor(indices,values,size = shape, requires_grad = False).T.to(device).detach()\n",
    "\n",
    "    imbalancetrain_batch_loop = (F.relu(torch.abs(Fli_0 + torch.sparse.mm(lodf, Flis).reshape((lodf_batch, l, Nsamples))) - 1.1*max_f)).detach().to(device)\n",
    "    count_crit_violation_loop = torch.sum(imbalancetrain_batch_loop > 0, dim=1)\n",
    "    count_line_violation_loop = torch.sum(imbalancetrain_batch_loop > 0, dim=1)\n",
    "    LOLP_N3 = torch.where(count_line_violation_loop != 0, torch.tensor(1, dtype=count_line_violation_loop.dtype), count_line_violation_loop)\n",
    "    LOLP_N3 = torch.mul(P_outage[0:Ncontingencies_N3],LOLP_N3)\n",
    "    \n",
    "    imbalancetrain_batch_loop = imbalancetrain_batch_loop.reshape(int(l * lodf_batch), -1)\n",
    "\n",
    "    #========= count the number of rows that have all zeros\n",
    "    mask = torch.all(imbalancetrain_batch_loop == 0, dim=1) \n",
    "    indices_remove = mask.nonzero().squeeze(1)\n",
    "    num_zero_rows = torch.sum(mask)\n",
    "\n",
    "    indices_tot = torch.arange(start_prob, end_prob)\n",
    "    mask_keep = ~torch.isin(indices_tot, indices_remove)\n",
    "    indices_keep = indices_tot[mask_keep]\n",
    "    indices_loc = indices_keep % l\n",
    "\n",
    "    #============ create new lodf\n",
    "    mask_flow = torch.ones(imbalancetrain_batch_loop.shape[0], dtype=torch.bool).to(device)\n",
    "    mask_flow[indices_remove] = False\n",
    "    lodf_test = lodf.to_dense()[mask_flow]\n",
    "    lodf_test = lodf_test.clone().detach()\n",
    "    lodf = lodf_test.to_sparse_coo().requires_grad_().to(torch.float32) \n",
    "    P_outage = P_outage[mask_flow]\n",
    "\n",
    "    Fli_0 = Flis.repeat(lodf_batch, 1).to(torch.float32).to(device)[mask_flow]\n",
    "    imbalancetrain_batch_loop = (F.relu(torch.abs(Fli_0+torch.sparse.mm(lodf,Flis))- max_f))\n",
    "\n",
    "    count_violation_loop = torch.count_nonzero(imbalancetrain_batch_loop)#/int(batch_size) \n",
    "    expected_violation = torch.mul(P_outage,imbalancetrain_batch_loop).to(torch.float32)\n",
    "\n",
    "    imbalancetrain_batch_N3 += torch.sum(imbalancetrain_batch_loop)/int(Nsamples)\n",
    "    expected_flows_N3 = torch.index_add(expected_flows_N3, 0, indices_loc.long(), expected_violation)\n",
    "\n",
    "    count_violation_N3 += count_violation_loop\n",
    "    count_crit_violation_N3 += torch.count_nonzero(count_crit_violation_loop.detach())\n",
    "    count_line_violation_N3 += torch.count_nonzero(count_line_violation_loop.detach())\n",
    "\n",
    "expected_inf_N1 = torch.sum(expected_flows_N1)/int(Nsamples)\n",
    "expected_inf_N2 = torch.sum(expected_flows_N2)/int(Nsamples)\n",
    "expected_inf_N3 = torch.sum(expected_flows_N3)/int(Nsamples)\n",
    "\n",
    "expected_infeasible_N1 = (torch.count_nonzero(expected_flows_N1)/torch.numel(expected_flows_N1))*100\n",
    "expected_infeasible_N2 = (torch.count_nonzero(expected_flows_N2)/torch.numel(expected_flows_N2))*100\n",
    "expected_infeasible_N3 = (torch.count_nonzero(expected_flows_N3)/torch.numel(expected_flows_N3))*100\n",
    "\n",
    "print('')\n",
    "#print('N1 infeasible contingency cases:', count_line_violation_N1, (count_line_violation_N1/(Ncontingencies_N1*Nsamples))*100, '%')\n",
    "print('LOLP N1:', count_crit_violation_N1, (count_crit_violation_N1/(Ncontingencies_N1*Nsamples))*100, '%')\n",
    "print('LOLP N1:', LOLP_N1.sum()/Nsamples)\n",
    "print('LOLE h/y N1:', LOLP_N1.sum()/Nsamples*8760)\n",
    "print('EENS N1:', expected_inf_N1*Sbase*frequency*repair_time*line_avg)\n",
    "#print('N1 expected avg violation:', expected_inf_N1)\n",
    "#print('N1 number violated lines:', expected_infeasible_N1)\n",
    "\n",
    "print('')\n",
    "print('N2 infeasible contingency cases:', count_line_violation_N2, (count_line_violation_N2/(Ncontingencies_N2*Nsamples))*100, '%')\n",
    "print('LOLP N2:', count_crit_violation_N2, (count_crit_violation_N2/(Ncontingencies_N2*Nsamples))*100, '%')\n",
    "print('LOLP N2:', LOLP_N2.sum()/Nsamples)\n",
    "print('LOLE h/y N2:', LOLP_N2.sum()/Nsamples*8760)\n",
    "print('EENS N2:', expected_inf_N2*Sbase*frequency*repair_time*2*line_avg*2)\n",
    "#print('N2 expected avg violation:', expected_inf_N2)\n",
    "#print('N2 number violated lines:', expected_infeasible_N2)\n",
    "\n",
    "print('')\n",
    "print('N3 infeasible contingency cases:', count_line_violation_N3, (count_line_violation_N3/(Ncontingencies_N3*Nsamples))*100, '%')\n",
    "#print('LOLP N3:', count_crit_violation_N3, (count_crit_violation_N3/(Ncontingencies_N3*Nsamples))*100, '%')\n",
    "print('LOLP N3:', LOLP_N3.sum()/Nsamples)\n",
    "print('LOLE h/y N3:', LOLP_N3.sum()/Nsamples*8760)\n",
    "print('EENS N3 MWh/y:', expected_inf_N3*Sbase*frequency*repair_time*3*line_avg*3)\n",
    "#print('N3 expected avg violation:', expected_inf_N3)\n",
    "#print('N3 number violated lines:', expected_infeasible_N3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb85d397",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_test = time.time()\n",
    "#============== Initialize training testing data\n",
    "testing_dataset = Data.TensorDataset(Xscal, cost_scopf)\n",
    "test_loader = Data.DataLoader(dataset=testing_dataset, batch_size=batch_size, shuffle=False)  # Note: shuffle=False for testing\n",
    "\n",
    "#================ Initialize tensors\n",
    "error = np.zeros(shape=(Niterations,3))\n",
    "exp_inf = np.zeros(shape=(Niterations,3))\n",
    "avg_infeasibility = np.zeros(shape=(Niterations,1))\n",
    "relcost = np.zeros(shape=(Niterations,1))\n",
    "cost = np.zeros(shape=(Niterations,1))\n",
    "\n",
    "Pgis = torch.zeros(size = (Ngens,int(batch_size)),dtype = torch.float32)\n",
    "Flis = torch.zeros(size = (Nlines,int(batch_size)),dtype = torch.float32)\n",
    "clil = torch.zeros(size = (Ngens,int(batch_size)),dtype = torch.float32)\n",
    "infeasibility = torch.zeros(int(batch_size), requires_grad = False)\n",
    "\n",
    "relcotest_epoch = torch.zeros(size = (int(Nsamples/batch_size),int(batch_size)),dtype = torch.float32, requires_grad = False)\n",
    "cost_epoch = torch.zeros(size = (int(Nsamples/batch_size),int(batch_size)),dtype = torch.float32, requires_grad = False)\n",
    "\n",
    "imbalancetest_epoch = torch.zeros(size = (round((Nsamples)/batch_size),3),dtype = torch.float32, requires_grad = False)\n",
    "expected_viol_test_epoch = torch.zeros(size = (round((Nsamples)/batch_size),3),dtype = torch.float32, requires_grad = False)\n",
    "\n",
    "infeasibilitytest_epoch = torch.zeros(size = (int(Nsamples/batch_size),Nbus),dtype = torch.float32, requires_grad = False)\n",
    "infeasibility_count = torch.zeros(size = (int(Nsamples/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "\n",
    "#===== N-1\n",
    "crit_violation_count_N1 = torch.zeros(size = (int(Nsamples/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "line_violation_count_N1 = torch.zeros(size = (int(Nsamples/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "\n",
    "#===== N-2\n",
    "crit_violation_count_N2 = torch.zeros(size = (int(Nsamples/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "line_violation_count_N2 = torch.zeros(size = (int(Nsamples/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "\n",
    "#===== N-3\n",
    "crit_violation_count_N3 = torch.zeros(size = (int(Nsamples/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "line_violation_count_N3 = torch.zeros(size = (int(Nsamples/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "\n",
    "print(\"It |   Time   |    Imbalance    | Mod out  |   Infeasibility   |  Rel cost\")\n",
    "for epoch in range(1):    \n",
    "    with torch.no_grad():\n",
    "        for i, (Xbatch, cost_scopf_batch) in enumerate(tqdm(test_loader)):\n",
    "                \n",
    "                model.eval() # deactivate dropout, when I changed this, testing error stayed same\n",
    "\n",
    "                Xbatch = Xbatch.clone().detach()\n",
    "                cost_scopf_batch = cost_scopf_batch.clone().detach()\n",
    "\n",
    "                Xtest_batch = ((Xbatch.cpu() * Xstd)+Xmean).to(torch.float32).to(device)\n",
    "                load_profile = torch.zeros((batch_size,Nbus)).to(device)\n",
    "                load_profile[:, load_loc] = Xtest_batch\n",
    "\n",
    "                #================== Perform forward pass\n",
    "                #clil = (model(Xbatch))\n",
    "                clil = (model(Xbatch)+1)/2\n",
    "                \n",
    "                #Pgi, Fli, Thi, lam1i, lam2i = cvxpylayer0(Xtest_batch, gencost, clil, solver_args=ECOS_solver_args)\n",
    "                Pgi, Fli, Thi, lam1i, lam2i = cvxpylayer0(Xtest_batch, clil, solver_args=ECOS_solver_args)\n",
    "                Pgis = Pgi.T\n",
    "                Flis = Fli.T.to(torch.float32)\n",
    "                \n",
    "                load_profile[:, gen_loc] += Pgi\n",
    "                nonzero_values, _ = torch.min(load_profile.masked_fill(load_profile == 0, float('inf')), dim=1, keepdim=True)\n",
    "                mask = load_profile == 0\n",
    "                load_profile = torch.where(mask, nonzero_values, load_profile)\n",
    "\n",
    "                #================ compute cost and infeasibility\n",
    "                infeasibility_batch = sum(lam1i.cpu().detach().numpy()) + sum(lam2i.cpu().detach().numpy())\n",
    "                infeasibility_batch = torch.tensor(infeasibility_batch)\n",
    "                infeasibility_tensor = lam1i + lam2i\n",
    "                infeasibility_tensor[abs(infeasibility_tensor) < 1e-5 ] = 0\n",
    "                infeasible_buses = (abs(infeasibility_tensor) > 1e-5)\n",
    "                \n",
    "                infeasible_cases = torch.zeros((batch_size,1))\n",
    "                row_has_true = torch.any(infeasible_buses, axis=1)\n",
    "                infeasible_cases[row_has_true] = True\n",
    "                infeasible_cases = infeasible_cases.sum()\n",
    "                \n",
    "                batch_cost = torch.matmul(gencost,Pgis).cpu()\n",
    "                batch_relco = ((batch_cost-cost_scopf_batch)/cost_scopf_batch)*100\n",
    "\n",
    "                #===================== compute contingency flows for each sample\n",
    "                imbalancetrain_batch_N1 = 0\n",
    "                imbalancetrain_batch_N2 = 0\n",
    "                imbalancetrain_batch_N3 = 0\n",
    "                imbalancetrain_batch = 0   \n",
    "\n",
    "                count_crit_violation_N1 = 0\n",
    "                count_line_violation_N1 = 0\n",
    "                count_violation_N1 = 0\n",
    "                LOLP_N1_tot = 0\n",
    "\n",
    "                count_crit_violation_N2 = 0\n",
    "                count_line_violation_N2 = 0\n",
    "                count_violation_N2 = 0\n",
    "                LOLP_N2_tot = 0\n",
    "\n",
    "                count_crit_violation_N3 = 0\n",
    "                count_line_violation_N3 = 0\n",
    "                count_violation_N3 = 0\n",
    "                LOLP_N3_tot = 0\n",
    "                \n",
    "                expected_flows = torch.zeros(size = (l,batch_size),dtype = torch.float32, requires_grad = True)\n",
    "                expected_flows_N1 = torch.zeros(size = (l,batch_size),dtype = torch.float32, requires_grad = True)\n",
    "                expected_flows_N2 = torch.zeros(size = (l,batch_size),dtype = torch.float32, requires_grad = True)\n",
    "                expected_flows_N3 = torch.zeros(size = (l,batch_size),dtype = torch.float32, requires_grad = True)\n",
    "\n",
    "                #============ N-1\n",
    "                for b in (range(tot_lodf_batches_N1)):\n",
    "                    #print('RAM Used (GB):', psutil.virtual_memory()[3]/1000000000)\n",
    "                    lodf_batch = lodf_batch_size_N1\n",
    "                    if b == (tot_lodf_batches_N1 - 1): \n",
    "                        lodf_batch = last_batch_size_N1\n",
    "                    left_ones = torch.ones((lodf_batch,1))\n",
    "                    right_ones = torch.ones((int(l*batch_size),1))\n",
    "                    Fli_0 = Flis.expand(lodf_batch, -1, -1)\n",
    "\n",
    "                    if b != (tot_lodf_batches_N1 - 1): \n",
    "                        start_prob = b*lodf_batch*l\n",
    "                        end_prob = (b+1)*lodf_batch*l\n",
    "                        P_outage = probabilities_N1[start_prob:end_prob]\n",
    "                    if b == (tot_lodf_batches_N1 - 1): \n",
    "                        end_prob = Ncontingencies_N1*l\n",
    "                        start_prob = (Ncontingencies_N1*l) - (last_batch_size_N1*l)\n",
    "                        P_outage = probabilities_N1[-(last_batch_size_N1*l):]\n",
    "\n",
    "                    index1 = LODF_dict_N1[b][0].detach()\n",
    "                    index2 = LODF_dict_N1[b][1].detach()\n",
    "                    indices = torch.stack((index1,index2)).detach()\n",
    "                    values = LODF_dict_N1[b][2].detach()\n",
    "                    shape = (l,int(l*lodf_batch))\n",
    "                    lodf = torch.sparse_coo_tensor(indices,values,size = shape, requires_grad = False).T.to(device).detach()\n",
    "\n",
    "                    imbalancetrain_batch_loop = (F.relu(torch.abs(Fli_0 + torch.sparse.mm(lodf, Flis).reshape((lodf_batch, l, batch_size))) - 1.1*max_f)).detach().to(device)\n",
    "                    count_crit_violation_loop = torch.sum(imbalancetrain_batch_loop > 0, dim=1)\n",
    "                    count_line_violation_loop = torch.sum(imbalancetrain_batch_loop > 0, dim=1)\n",
    "                    LOLP_N1 = torch.where(count_line_violation_loop != 0, torch.tensor(1, dtype=count_line_violation_loop.dtype), count_line_violation_loop)\n",
    "                    LOLP_N1 = torch.mul(P_outage[0:Ncontingencies_N1],LOLP_N1)\n",
    "                    LOLP_N1_tot += LOLP_N1.sum()/int(batch_size)\n",
    "                    \n",
    "                    imbalancetrain_batch_loop = imbalancetrain_batch_loop.reshape(int(l * lodf_batch), -1)\n",
    "\n",
    "                    #========= count the number of rows that have all zeros\n",
    "                    mask = torch.all(imbalancetrain_batch_loop == 0, dim=1) \n",
    "                    indices_remove = mask.nonzero().squeeze(1)\n",
    "                    num_zero_rows = torch.sum(mask)\n",
    "\n",
    "                    indices_tot = torch.arange(start_prob, end_prob)\n",
    "                    mask_keep = ~torch.isin(indices_tot, indices_remove)\n",
    "                    indices_keep = indices_tot[mask_keep]\n",
    "                    indices_loc = indices_keep % l\n",
    "\n",
    "                    #============ create new lodf\n",
    "                    mask_flow = torch.ones(imbalancetrain_batch_loop.shape[0], dtype=torch.bool).to(device)\n",
    "                    mask_flow[indices_remove] = False\n",
    "                    lodf_test = lodf.to_dense()[mask_flow]\n",
    "                    lodf_test = lodf_test.clone().detach()\n",
    "                    lodf = lodf_test.to_sparse_coo().requires_grad_().to(torch.float32) \n",
    "                    P_outage = P_outage[mask_flow]\n",
    "\n",
    "                    Fli_0 = Flis.repeat(lodf_batch, 1).to(torch.float32).to(device)[mask_flow]\n",
    "                    imbalancetrain_batch_loop = (F.relu(torch.abs(Fli_0+torch.sparse.mm(lodf,Flis))- max_f))\n",
    "                    count_violation_loop = torch.count_nonzero(imbalancetrain_batch_loop)#/int(batch_size) \n",
    "                    expected_violation = torch.mul(P_outage,imbalancetrain_batch_loop).to(torch.float32)\n",
    "\n",
    "                    imbalancetrain_batch_N1 += torch.sum(imbalancetrain_batch_loop)/int(batch_size)\n",
    "                    expected_flows_N1 = torch.index_add(expected_flows_N1, 0, indices_loc.long(), expected_violation)\n",
    "\n",
    "                    count_violation_N1 += count_violation_loop\n",
    "                    count_crit_violation_N1 += torch.count_nonzero(count_crit_violation_loop.detach())\n",
    "                    count_line_violation_N1 += torch.count_nonzero(count_line_violation_loop.detach())\n",
    "\n",
    "                #============ N-2\n",
    "                for b in (range(tot_lodf_batches_N2)):\n",
    "                    #print('RAM Used (GB):', psutil.virtual_memory()[3]/1000000000)\n",
    "                    lodf_batch = lodf_batch_size_N2\n",
    "                    if b == (tot_lodf_batches_N2 - 1): \n",
    "                        lodf_batch = last_batch_size_N2\n",
    "                    left_ones = torch.ones((lodf_batch,1))\n",
    "                    right_ones = torch.ones((int(l*batch_size),1))\n",
    "                    Fli_0 = Flis.expand(lodf_batch, -1, -1)\n",
    "\n",
    "                    if b != (tot_lodf_batches_N2 - 1): \n",
    "                        start_prob = b*lodf_batch*l\n",
    "                        end_prob = (b+1)*lodf_batch*l\n",
    "                        P_outage = probabilities_N2[start_prob:end_prob]\n",
    "                    if b == (tot_lodf_batches_N2 - 1): \n",
    "                        end_prob = Ncontingencies_N2*l\n",
    "                        start_prob = (Ncontingencies_N2*l) - (last_batch_size_N2*l)\n",
    "                        P_outage = probabilities_N2[-(last_batch_size_N2*l):]\n",
    "\n",
    "                    index1 = LODF_dict_N2[b][0].detach()\n",
    "                    index2 = LODF_dict_N2[b][1].detach()\n",
    "                    indices = torch.stack((index1,index2)).detach()\n",
    "                    values = LODF_dict_N2[b][2].detach()\n",
    "                    shape = (l,int(l*lodf_batch))\n",
    "                    lodf = torch.sparse_coo_tensor(indices,values,size = shape, requires_grad = False).T.to(device).detach()\n",
    "\n",
    "                    imbalancetrain_batch_loop = (F.relu(torch.abs(Fli_0 + torch.sparse.mm(lodf, Flis).reshape((lodf_batch, l, batch_size))) - 1.1*max_f)).detach().to(device)\n",
    "                    count_crit_violation_loop = torch.sum(imbalancetrain_batch_loop > 0, dim=1)\n",
    "                    count_line_violation_loop = torch.sum(imbalancetrain_batch_loop > 0, dim=1)\n",
    "                    LOLP_N2 = torch.where(count_line_violation_loop != 0, torch.tensor(1, dtype=count_line_violation_loop.dtype), count_line_violation_loop)\n",
    "                    LOLP_N2 = torch.mul(P_outage[0:Ncontingencies_N2],LOLP_N2)\n",
    "                    LOLP_N2_tot += LOLP_N2.sum()/int(batch_size)\n",
    "                    \n",
    "                    imbalancetrain_batch_loop = imbalancetrain_batch_loop.reshape(int(l * lodf_batch), -1)\n",
    "\n",
    "                    #========= count the number of rows that have all zeros\n",
    "                    mask = torch.all(imbalancetrain_batch_loop == 0, dim=1) \n",
    "                    indices_remove = mask.nonzero().squeeze(1)\n",
    "                    num_zero_rows = torch.sum(mask)\n",
    "\n",
    "                    indices_tot = torch.arange(start_prob, end_prob)\n",
    "                    mask_keep = ~torch.isin(indices_tot, indices_remove)\n",
    "                    indices_keep = indices_tot[mask_keep]\n",
    "                    indices_loc = indices_keep % l\n",
    "\n",
    "                    #============ create new lodf\n",
    "                    mask_flow = torch.ones(imbalancetrain_batch_loop.shape[0], dtype=torch.bool).to(device)\n",
    "                    mask_flow[indices_remove] = False\n",
    "                    lodf_test = lodf.to_dense()[mask_flow]\n",
    "                    lodf_test = lodf_test.clone().detach()\n",
    "                    lodf = lodf_test.to_sparse_coo().requires_grad_().to(torch.float32) \n",
    "                    P_outage = P_outage[mask_flow]\n",
    "\n",
    "                    Fli_0 = Flis.repeat(lodf_batch, 1).to(torch.float32).to(device)[mask_flow]\n",
    "                    imbalancetrain_batch_loop = (F.relu(torch.abs(Fli_0+torch.sparse.mm(lodf,Flis))- max_f))\n",
    "                    count_violation_loop = torch.count_nonzero(imbalancetrain_batch_loop)#/int(batch_size) \n",
    "                    expected_violation = torch.mul(P_outage,imbalancetrain_batch_loop).to(torch.float32)\n",
    "\n",
    "                    imbalancetrain_batch_N2 += torch.sum(imbalancetrain_batch_loop)/int(batch_size)\n",
    "                    expected_flows_N2 = torch.index_add(expected_flows_N2, 0, indices_loc.long(), expected_violation)\n",
    "\n",
    "                    count_violation_N2 += count_violation_loop\n",
    "                    count_crit_violation_N2 += torch.count_nonzero(count_crit_violation_loop.detach())\n",
    "                    count_line_violation_N2 += torch.count_nonzero(count_line_violation_loop.detach())\n",
    "\n",
    "\n",
    "                #============ N-3\n",
    "                for b in (range(tot_lodf_batches_N3)):\n",
    "                    #print('RAM Used (GB):', psutil.virtual_memory()[3]/1000000000)\n",
    "                    lodf_batch = lodf_batch_size_N3\n",
    "                    if b == (tot_lodf_batches_N3 - 1): \n",
    "                        lodf_batch = last_batch_size_N3\n",
    "                    left_ones = torch.ones((lodf_batch,1))\n",
    "                    right_ones = torch.ones((int(l*batch_size),1))\n",
    "                    Fli_0 = Flis.expand(lodf_batch, -1, -1)\n",
    "\n",
    "                    if b != (tot_lodf_batches_N3 - 1): \n",
    "                        start_prob = b*lodf_batch*l\n",
    "                        end_prob = (b+1)*lodf_batch*l\n",
    "                        P_outage = probabilities_N3[start_prob:end_prob]\n",
    "                    if b == (tot_lodf_batches_N3 - 1): \n",
    "                        end_prob = Ncontingencies_N3*l\n",
    "                        start_prob = (Ncontingencies_N3*l) - (last_batch_size_N3*l)\n",
    "                        P_outage = probabilities_N3[-(last_batch_size_N3*l):]\n",
    "\n",
    "                    index1 = LODF_dict_N3[b][0].detach()\n",
    "                    index2 = LODF_dict_N3[b][1].detach()\n",
    "                    indices = torch.stack((index1,index2)).detach()\n",
    "                    values = LODF_dict_N3[b][2].detach()\n",
    "                    shape = (l,int(l*lodf_batch))\n",
    "                    lodf = torch.sparse_coo_tensor(indices,values,size = shape, requires_grad = False).T.to(device).detach()\n",
    "\n",
    "                    imbalancetrain_batch_loop = (F.relu(torch.abs(Fli_0 + torch.sparse.mm(lodf, Flis).reshape((lodf_batch, l, batch_size))) - 1.1*max_f)).detach().to(device)\n",
    "                    count_crit_violation_loop = torch.sum(imbalancetrain_batch_loop > 0, dim=1)\n",
    "                    count_line_violation_loop = torch.sum(imbalancetrain_batch_loop > 0, dim=1)\n",
    "                    LOLP_N3 = torch.where(count_line_violation_loop != 0, torch.tensor(1, dtype=count_line_violation_loop.dtype), count_line_violation_loop)\n",
    "                    LOLP_N3 = torch.mul(P_outage[0:Ncontingencies_N3],LOLP_N3)\n",
    "                    LOLP_N3_tot += LOLP_N3.sum()/int(batch_size)\n",
    "                    \n",
    "                    imbalancetrain_batch_loop = imbalancetrain_batch_loop.reshape(int(l * lodf_batch), -1)\n",
    "\n",
    "                    #========= count the number of rows that have all zeros\n",
    "                    mask = torch.all(imbalancetrain_batch_loop == 0, dim=1) \n",
    "                    indices_remove = mask.nonzero().squeeze(1)\n",
    "                    num_zero_rows = torch.sum(mask)\n",
    "\n",
    "                    indices_tot = torch.arange(start_prob, end_prob)\n",
    "                    mask_keep = ~torch.isin(indices_tot, indices_remove)\n",
    "                    indices_keep = indices_tot[mask_keep]\n",
    "                    indices_loc = indices_keep % l\n",
    "\n",
    "                    #============ create new lodf\n",
    "                    mask_flow = torch.ones(imbalancetrain_batch_loop.shape[0], dtype=torch.bool).to(device)\n",
    "                    mask_flow[indices_remove] = False\n",
    "                    lodf_test = lodf.to_dense()[mask_flow]\n",
    "                    lodf_test = lodf_test.clone().detach()\n",
    "                    lodf = lodf_test.to_sparse_coo().requires_grad_().to(torch.float32) \n",
    "                    P_outage = P_outage[mask_flow]\n",
    "\n",
    "                    Fli_0 = Flis.repeat(lodf_batch, 1).to(torch.float32).to(device)[mask_flow]\n",
    "                    imbalancetrain_batch_loop = (F.relu(torch.abs(Fli_0+torch.sparse.mm(lodf,Flis))- max_f))\n",
    "\n",
    "                    count_violation_loop = torch.count_nonzero(imbalancetrain_batch_loop)#/int(batch_size) \n",
    "                    expected_violation = torch.mul(P_outage,imbalancetrain_batch_loop).to(torch.float32)\n",
    "\n",
    "                    imbalancetrain_batch_N3 += torch.sum(imbalancetrain_batch_loop)/int(batch_size)\n",
    "                    expected_flows_N3 = torch.index_add(expected_flows_N3, 0, indices_loc.long(), expected_violation)\n",
    "\n",
    "                    count_violation_N3 += count_violation_loop\n",
    "                    count_crit_violation_N3 += torch.count_nonzero(count_crit_violation_loop.detach())\n",
    "                    count_line_violation_N3 += torch.count_nonzero(count_line_violation_loop.detach())\n",
    "\n",
    "                expected_inf_N1 = torch.sum(expected_flows_N1)/int(batch_size)\n",
    "                expected_inf_N2 = torch.sum(expected_flows_N2)/int(batch_size)\n",
    "                expected_inf_N3 = torch.sum(expected_flows_N3)/int(batch_size)\n",
    "\n",
    "                expected_infeasible_N1 = (torch.count_nonzero(expected_flows_N1)/torch.numel(expected_flows_N1))*100\n",
    "                expected_infeasible_N2 = (torch.count_nonzero(expected_flows_N2)/torch.numel(expected_flows_N2))*100\n",
    "                expected_infeasible_N3 = (torch.count_nonzero(expected_flows_N3)/torch.numel(expected_flows_N3))*100\n",
    "                torch.set_printoptions(precision=9)\n",
    "                    \n",
    "                imbalancetest_batch = expected_inf_N1+expected_inf_N2+expected_inf_N3\n",
    "        \n",
    "                relcotest_epoch[i,:] = batch_relco.detach()\n",
    "                cost_epoch[i,:] = batch_cost.detach()\n",
    "                imbalancetest_epoch[i,:] = torch.tensor([expected_inf_N1.detach(),expected_inf_N2.detach(),expected_inf_N3.detach()])\n",
    "                expected_viol_test_epoch[i,:] = torch.tensor([expected_infeasible_N1.detach(),expected_infeasible_N2.detach(),expected_infeasible_N3.detach()])\n",
    "                \n",
    "                infeasibilitytest_epoch[i,:] = infeasibility_batch.detach()\n",
    "                \n",
    "                infeasibility_count[i,:] = infeasible_cases.detach()\n",
    "                \n",
    "                #====== N-1\n",
    "                crit_violation_count_N1[i,:] = count_crit_violation_N1.detach()\n",
    "                line_violation_count_N1[i,:] = count_line_violation_N1.detach()\n",
    "\n",
    "                #====== N-2\n",
    "                crit_violation_count_N2[i,:] = count_crit_violation_N2.detach()\n",
    "                line_violation_count_N2[i,:] = count_line_violation_N2.detach()\n",
    "\n",
    "                #====== N-3\n",
    "                crit_violation_count_N3[i,:] = count_crit_violation_N3.detach()\n",
    "                line_violation_count_N3[i,:] = count_line_violation_N3.detach()\n",
    "            \n",
    "\n",
    "    imbalancetest = torch.mean(imbalancetest_epoch, dim=0).detach()\n",
    "    expected_violations_test = torch.mean(expected_viol_test_epoch, dim=0).detach()\n",
    "\n",
    "    infeasibilitytest = torch.mean(infeasibilitytest_epoch).detach()\n",
    "\n",
    "    relcotest = torch.mean(relcotest_epoch).detach()\n",
    "    costtest = torch.mean(cost_epoch).detach()\n",
    "    \n",
    "    #======== N-1\n",
    "    crit_violation_tot_N1 = torch.sum(crit_violation_count_N1).detach()\n",
    "    line_violation_tot_N1 = torch.sum(line_violation_count_N1).detach()\n",
    "    \n",
    "    #======== N-1\n",
    "    crit_violation_tot_N2 = torch.sum(crit_violation_count_N2).detach()\n",
    "    line_violation_tot_N2 = torch.sum(line_violation_count_N2).detach()\n",
    "    \n",
    "    #======== N-1\n",
    "    crit_violation_tot_N3 = torch.sum(crit_violation_count_N3).detach()\n",
    "    line_violation_tot_N3 = torch.sum(line_violation_count_N3).detach()\n",
    "    \n",
    "    #====== convergence criteria\n",
    "    infeasible_cases_tot = torch.sum(infeasibility_count).detach()\n",
    "    \n",
    "    error[epoch,:] = imbalancetest.detach().numpy()\n",
    "    exp_inf[epoch,:] = expected_violations_test.detach().numpy()\n",
    "    avg_infeasibility[epoch,:] =  infeasibilitytest.detach().numpy()\n",
    "    relcost[epoch,:] =  relcotest.detach().numpy()\n",
    "    cost[epoch,:] =  costtest.detach().numpy()\n",
    "    end_test = time.time()\n",
    "    time_test = end_test-start_test\n",
    "\n",
    "    print(epoch, \" | \", round(time_test,0), \" | \",np.round(error[epoch,:],3), \" | \",np.round(exp_inf[epoch,:],3), \" | \", np.round(avg_infeasibility[epoch,:],3), \" | \", np.round(relcost[epoch,:], 3) )    \n",
    "\n",
    "print('method time:', time_test)\n",
    "\n",
    "print('current infeasibility:', (infeasible_cases_tot/Nsamples)*100, '%', 'goal:', 1, '%')\n",
    "    \n",
    "print('')\n",
    "print('N1 infeasible contingency cases:', crit_violation_tot_N1, (crit_violation_tot_N1/(Ncontingencies_N1*Nsamples))*100, '%')\n",
    "print('LOLP N1:', LOLP_N1_tot )\n",
    "print('LOLE h/y N1:', LOLP_N1_tot*8760)\n",
    "print('EENS N1:', error[epoch,0]*Sbase*frequency*repair_time*line_avg)\n",
    "\n",
    "print('')\n",
    "print('N2 infeasible contingency cases:', crit_violation_tot_N2, (crit_violation_tot_N2/(Ncontingencies_N2*Nsamples))*100, '%')\n",
    "print('LOLP N2:', LOLP_N2_tot)\n",
    "print('LOLE h/y N2:', LOLP_N2_tot*8760)\n",
    "print('EENS N2:', error[epoch,1]*Sbase*frequency*repair_time*2*line_avg*2) # double repair time for doubble fault, also double line length\n",
    "\n",
    "print('')\n",
    "print('N3 infeasible contingency cases:', crit_violation_tot_N3, (crit_violation_tot_N3/(Ncontingencies_N3*Nsamples))*100, '%')\n",
    "print('LOLP N3:', LOLP_N3_tot)\n",
    "print('LOLE h/y N3:', LOLP_N3_tot*8760)\n",
    "print('EENS N3:', error[epoch,2]*Sbase*frequency*repair_time*3*line_avg*3) # triple repair time for triple fault, also triple line length\n",
    "\n",
    "\n",
    "\n",
    "print('total cost:', cost.sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cfb67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#============== Create new training data (with high variability from RES)\n",
    "X = np.array(data['load']['p'])/Sbase*np.random.uniform(low=0.75, high=1.25, size = (Nsamples,Nloads))\n",
    "Xtrain = torch.tensor(X[0:int(Nsamples),:],dtype = torch.float32)        \n",
    "Xtrain_transpose = Xtrain.transpose(0,1)\n",
    "#Xtest = torch.tensor(X[int(train*Nsamples):,:],dtype = torch.float64)\n",
    "#standardise\n",
    "Xmin, Xmax,Xmean,Xstd = np.min(X, axis = 0),np.max(X, axis = 0),np.mean(X, axis = 0),np.std(X, axis = 0)\n",
    "Xscal = torch.tensor(( X - Xmean ) / Xstd, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d036af4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============== Benchmark #2 heuristic approach\n",
    "if case == 'N-1':\n",
    "    benchmark = 'SCOPF1screening'\n",
    "    \n",
    "#benchmark = 'SCOPF4screening'\n",
    "\n",
    "if benchmark == 'SCOPF1screening':\n",
    "    Nk_contingencies = np.array(list(itertools.combinations(range(Nlines),1)))\n",
    "elif benchmark == 'SCOPF2screening':\n",
    "    Nk_contingencies = np.array(list(itertools.combinations(range(Nlines),2)))\n",
    "elif benchmark == 'SCOPF3screening':\n",
    "    Nk_contingencies = np.array(list(itertools.combinations(range(Nlines),3)))\n",
    "elif benchmark == 'SCOPF4screening':\n",
    "    Nk_contingencies = np.array(list(itertools.combinations(range(Nlines),4)))\n",
    "\n",
    "Pgisscopf = torch.zeros(Ngens,int(Nsamples),dtype = torch.float32)\n",
    "Fl0sscopf = torch.zeros(Nlines,int(Nsamples),dtype = torch.float32)\n",
    "#Flcs = torch.zeros(Nlines,len(Nk_contingencies),int(Nsamples),dtype = torch.float32)\n",
    "lam1s = torch.zeros(Nbus,int(Nsamples),dtype = torch.float32)\n",
    "lam2s = torch.zeros(Nbus,int(Nsamples),dtype = torch.float32)\n",
    "#lam1sc = torch.zeros(Nbus,len(Nk_contingencies),int(Nsamples),dtype = torch.float32)\n",
    "#lam2sc = torch.zeros(Nbus,len(Nk_contingencies),int(Nsamples),dtype = torch.float32)\n",
    "Th0 = torch.zeros(Nbus,int(Nsamples),dtype = torch.float32)\n",
    "\n",
    "time_scopfs = time.time()\n",
    "\n",
    "#lcontingencies = [0, 18426, 26007, 26009, 6858, 18433, 2363, 26011, 23394, 23371, 25970, 26010, 26006, 20773, 14570, 26046, 6856, 17846, 2361, 20771, 26005, 11034, 11052, 10820, 10785, 10780, 523, 5073, 10610, 5085, 10833, 11620, 10806, 11367, 10268, 680, 578, 10611, 590, 10995, 13086, 11874, 13096, 13114, 13113, 13108, 13112, 11390, 12274, 11643, 13107, 13109, 11114, 12084, 12854, 10814, 26013, 26014, 10667, 25995, 25996, 10916]\n",
    "Nk_zeros = list(range(len(Nk_contingencies)))\n",
    "zero_indices = [l for l in Nk_zeros if l not in lcontingencies]\n",
    "print(lcontingencies)\n",
    "\n",
    "\n",
    "if benchmark == 'SCOPF1screening':\n",
    "    problem0scopf, Pd0scopf, cg0scopf, Pgiscopf, Fl0scopf, Th0scopf, Flc, lam1, lam2, lam1c, lam2c = cs.create_scopf1_screening(data, lcontingencies)\n",
    "elif benchmark == 'SCOPF2screening':\n",
    "    problem0scopf, Pd0scopf, cg0scopf, Pgiscopf, Fl0scopf, Th0scopf, Flc, lam1, lam2, lam1c, lam2c = cs.create_scopf2_screening(data, lcontingencies)\n",
    "elif benchmark == 'SCOPF3screening':\n",
    "    problem0scopf, Pd0scopf, cg0scopf, Pgiscopf, Fl0scopf, Th0scopf, Flc, lam1, lam2, lam1c, lam2c = cs.create_scopf3_screening(data, lcontingencies)\n",
    "elif benchmark == 'SCOPF4screening':\n",
    "    problem0scopf, Pd0scopf, cg0scopf, Pgiscopf, Fl0scopf, Th0scopf, Flc, lam1, lam2, lam1c, lam2c = cs.create_scopf4_screening(data, lcontingencies)\n",
    "\n",
    "cg0scopf.value = np.array(data['gen']['cost'])\n",
    "for entry in tqdm(range(int(Nsamples)),position=0, leave=True):   \n",
    "    Pd0scopf.value = X[entry,:] #np.array(data['load']['p'])/Sbase    \n",
    "    solution  = problem0scopf.solve(solver=cp.ECOS)    # cp.MOSEK\n",
    "    if problem0scopf.status in [\"infeasible\", \"unbounded\"]:\n",
    "         # Otherwise, problem.value is inf or -inf, respectively.\n",
    "         print(\"Problem infeasible or unbounded\")\n",
    "         #print(\"Optimal value: %s\" % problem0scopf.value)\n",
    "    Pgisscopf[:,entry] = torch.tensor(Pgiscopf.value)\n",
    "    Fl0sscopf[:,entry] = torch.tensor(Fl0scopf.value)        \n",
    "    Th0[:,entry] = torch.tensor(Th0scopf.value)\n",
    "\n",
    "    lam1s[:,entry] = torch.tensor(lam1.value)\n",
    "    lam2s[:,entry] = torch.tensor(lam2.value)\n",
    "\n",
    "cost_scopf = torch.matmul(gencost,Pgisscopf)\n",
    "\n",
    "#============ compute Nk flows\n",
    "Flis = Fl0sscopf\n",
    "\n",
    "for b in (range(tot_lodf_batches)):\n",
    "    lodf_batch = lodf_batch_size\n",
    "    if b == (tot_lodf_batches - 1): \n",
    "        lodf_batch = last_batch_size\n",
    "    left_ones = torch.ones((lodf_batch,1))\n",
    "    right_ones = torch.ones((int(l*batch_size),1))\n",
    "    Fli_0 = Flis.expand(lodf_batch, -1, -1)\n",
    "\n",
    "    index1 = LODF_dict[b][0].detach()\n",
    "    index2 = LODF_dict[b][1].detach()\n",
    "    indices = torch.stack((index1,index2)).detach()\n",
    "    values = LODF_dict[b][2].detach()\n",
    "    shape = (l,int(l*lodf_batch))\n",
    "    lodf = torch.sparse_coo_tensor(indices,values,size = shape, requires_grad = False).T.to(device).detach()\n",
    "\n",
    "    imbalancetrain_batch_loop = (F.relu(torch.abs(Fli_0 + torch.sparse.mm(lodf,Flis).reshape((lodf_batch,l,Nsamples)))- max_f)).detach()\n",
    "    count_crit_violation_loop = torch.sum(imbalancetrain_batch_loop > tollerance_crit*max_f, dim=1)\n",
    "    count_line_violation_loop = torch.sum(imbalancetrain_batch_loop > 0, dim=1)\n",
    "\n",
    "    count_crit_violation += torch.count_nonzero(count_crit_violation_loop)\n",
    "    count_line_violation += torch.count_nonzero(count_line_violation_loop)\n",
    "\n",
    "crit_violation_tot = count_crit_violation\n",
    "line_violation_tot = count_line_violation\n",
    "\n",
    "print('infeasible N-k contingency cases:', line_violation_tot, (line_violation_tot/(Ncontingencies*Nsamples))*100, '%')\n",
    "print('10%+ contingency cases:', crit_violation_tot, (crit_violation_tot/(Ncontingencies*Nsamples))*100, '%')\n",
    "    \n",
    "time_scopfs = time.time() - time_scopfs\n",
    "print('heuristic approach time:', time_scopfs)\n",
    "\n",
    "##============= check infeasibility\n",
    "infeasibility_base = (lam1s + lam2s).permute(1,0)\n",
    "infeasibility_base[abs(infeasibility_base) < 1e-4] = 0\n",
    "infeasible_buses_base = (abs(infeasibility_base) > 1e-4) #0.001*load_profile) # 10% of total load available for generation/load shedding\n",
    "scopf_cost_average = cost_scopf.mean()\n",
    "\n",
    "infeasible_cases_base = torch.zeros((Nsamples,1))\n",
    "row_has_true_base = torch.any(infeasible_buses_base, axis=1)\n",
    "infeasible_cases_base[row_has_true_base] = True\n",
    "infeasible_cases_tot_base = infeasible_cases_base.sum()\n",
    "print('infeasible base cases:', infeasible_cases_tot_base)\n",
    "print('percentage base infeasibility:', (infeasible_cases_tot_base/(Nsamples))*100, '%')\n",
    "print('total average cost:', scopf_cost_average)\n",
    "\n",
    "print('RAM Used (GB):', psutil.virtual_memory()[3]/1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d097e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_test = time.time()\n",
    "#============== Initialize training testing data\n",
    "testing_dataset = Data.TensorDataset(Xscal, cost_scopf)\n",
    "test_loader = Data.DataLoader(dataset=testing_dataset, batch_size=batch_size, shuffle=False)  # Note: shuffle=False for testing\n",
    "\n",
    "#================ Initialize tensors\n",
    "error = np.zeros(shape=(Niterations,1))\n",
    "avg_infeasibility = np.zeros(shape=(Niterations,1))\n",
    "relcost = np.zeros(shape=(Niterations,1))\n",
    "cost = np.zeros(shape=(Niterations,1))\n",
    "\n",
    "Pgis = torch.zeros(size = (Ngens,int(batch_size)),dtype = torch.float32)\n",
    "Flis = torch.zeros(size = (Nlines,int(batch_size)),dtype = torch.float32)\n",
    "clil = torch.zeros(size = (Ngens,int(batch_size)),dtype = torch.float32)\n",
    "infeasibility = torch.zeros(int(batch_size), requires_grad = False)\n",
    "\n",
    "relcotest_epoch = torch.zeros(size = (int(Nsamples/batch_size),int(batch_size)),dtype = torch.float32, requires_grad = False)\n",
    "cost_epoch = torch.zeros(size = (int(Nsamples/batch_size),int(batch_size)),dtype = torch.float32, requires_grad = False)\n",
    "\n",
    "imbalancetest_epoch = torch.zeros(size = (round((Nsamples)/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "\n",
    "infeasibilitytest_epoch = torch.zeros(size = (int(Nsamples/batch_size),Nbus),dtype = torch.float32, requires_grad = False)\n",
    "infeasibility_count = torch.zeros(size = (int(Nsamples/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "line_violation_count = torch.zeros(size = (int(Nsamples/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "crit_violation_count = torch.zeros(size = (int(Nsamples/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "avg_violation_list = torch.zeros(size = (int(Nsamples/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "count_violation_list = torch.zeros(size = (int(Nsamples/batch_size),1),dtype = torch.float32, requires_grad = False)\n",
    "\n",
    "print(\"It |   Time   |    Imbalance    | Mod out  |   Infeasibility   |  Rel cost\")\n",
    "for epoch in range(1):    \n",
    "    with torch.no_grad():\n",
    "        for i, (Xbatch, cost_scopf_batch) in enumerate(tqdm(test_loader)):\n",
    "                \n",
    "                model.eval() # deactivate dropout, when I changed this, testing error stayed same\n",
    "\n",
    "                Xbatch = Xbatch.clone().detach()\n",
    "                cost_scopf_batch = cost_scopf_batch.clone().detach()\n",
    "\n",
    "                Xtest_batch = ((Xbatch.cpu() * Xstd)+Xmean).to(torch.float32).to(device)\n",
    "                load_profile = torch.zeros((batch_size,Nbus)).to(device)\n",
    "                load_profile[:, load_loc] = Xtest_batch\n",
    "\n",
    "                #================== Perform forward pass\n",
    "                #clil = (model(Xbatch))\n",
    "                clil = (model(Xbatch)+1)/2\n",
    "                \n",
    "                Pgi, Fli, Thi, lam1i, lam2i = cvxpylayer0(Xtest_batch, gencost, clil, solver_args=ECOS_solver_args)\n",
    "                Pgis = Pgi.T\n",
    "                Flis = Fli.T.to(torch.float32)\n",
    "                \n",
    "                load_profile[:, gen_loc] += Pgi\n",
    "                nonzero_values, _ = torch.min(load_profile.masked_fill(load_profile == 0, float('inf')), dim=1, keepdim=True)\n",
    "                mask = load_profile == 0\n",
    "                load_profile = torch.where(mask, nonzero_values, load_profile)\n",
    "\n",
    "                #================ compute cost and infeasibility\n",
    "                infeasibility_batch = sum(lam1i.cpu().detach().numpy()) + sum(lam2i.cpu().detach().numpy())\n",
    "                infeasibility_batch = torch.tensor(infeasibility_batch)\n",
    "                infeasibility_tensor = lam1i + lam2i\n",
    "                infeasibility_tensor[abs(infeasibility_tensor) < 1e-5 ] = 0\n",
    "                infeasible_buses = (abs(infeasibility_tensor) > 1e-5)\n",
    "                \n",
    "                infeasible_cases = torch.zeros((batch_size,1))\n",
    "                row_has_true = torch.any(infeasible_buses, axis=1)\n",
    "                infeasible_cases[row_has_true] = True\n",
    "                infeasible_cases = infeasible_cases.sum()\n",
    "                \n",
    "                batch_cost = torch.matmul(gencost,Pgis).cpu()\n",
    "                batch_relco = ((batch_cost-cost_scopf_batch)/cost_scopf_batch)*100\n",
    "\n",
    "                #===================== compute contingency flows for each sample\n",
    "                imbalancetest_batch = 0\n",
    "                count_crit_violation = 0\n",
    "                count_line_violation = 0\n",
    "                count_violation = 0\n",
    "\n",
    "                for b in (range(tot_lodf_batches)):\n",
    "                    lodf_batch = lodf_batch_size\n",
    "                    if b == (tot_lodf_batches - 1): \n",
    "                        lodf_batch = last_batch_size\n",
    "                    left_ones = torch.ones((lodf_batch_size,1))\n",
    "                    right_ones = torch.ones((int(l*batch_size),1))\n",
    "                    Fli_0 = Flis.expand(lodf_batch, -1, -1)#.contiguous().view(-1, len(Flis[1])).detach()#.to(torch.float32).clone()\n",
    "                    #mask = mask_base(b, lodf_batch)\n",
    "                    #Fli_0[~mask] = 0\n",
    "\n",
    "                    index1 = LODF_dict[b][0].detach()\n",
    "                    index2 = LODF_dict[b][1].detach()\n",
    "                    indices = torch.stack((index1,index2)).detach()\n",
    "                    values = LODF_dict[b][2].detach()\n",
    "                    shape = (l,int(l*lodf_batch))\n",
    "                    lodf = torch.sparse_coo_tensor(indices,values,size = shape).T.to(device)\n",
    "\n",
    "                    imbalancetest_batch_loop = (F.relu(torch.abs(Fli_0 + torch.sparse.mm(lodf,Flis).reshape((lodf_batch,l,batch_size)))- max_f)).to(device)\n",
    "                    imbalancetest_batch_loop[abs(imbalancetest_batch_loop) < 1e-5] = 0\n",
    "                    count_crit_violation_loop = torch.sum(imbalancetest_batch_loop > tollerance_crit*max_f, dim=1)\n",
    "                    count_line_violation_loop = torch.sum(imbalancetest_batch_loop > 0, dim=1)                   \n",
    "                    \n",
    "                    imbalancetest_batch_loop = imbalancetest_batch_loop.reshape(lodf_batch,int(l*batch_size)).to(device)\n",
    "                    count_violation_loop = torch.count_nonzero(imbalancetest_batch_loop)\n",
    "                \n",
    "                    imbalancetest_batch += torch.sum(imbalancetest_batch_loop)/int(batch_size)\n",
    "                    count_violation += count_violation_loop\n",
    "                    count_crit_violation += torch.count_nonzero(count_crit_violation_loop.detach()).to(device)\n",
    "                    count_line_violation += torch.count_nonzero(count_line_violation_loop.detach()).to(device)\n",
    "        \n",
    "                relcotest_epoch[i,:] = batch_relco.detach()\n",
    "                cost_epoch[i,:] = batch_cost.detach()\n",
    "                imbalancetest_epoch[i,:] = imbalancetest_batch.detach()\n",
    "                infeasibilitytest_epoch[i,:] = infeasibility_batch.detach()\n",
    "                \n",
    "                infeasibility_count[i,:] = infeasible_cases.detach()\n",
    "                line_violation_count[i,:] = count_line_violation.detach()\n",
    "                crit_violation_count[i,:] = count_crit_violation.detach()\n",
    "                #avg_violation_list[i,:] = avg_violation.detach()\n",
    "                count_violation_list[i,:] = count_violation\n",
    "            \n",
    "\n",
    "    imbalancetest = torch.mean(imbalancetest_epoch).detach()\n",
    "\n",
    "    infeasibilitytest = torch.mean(infeasibilitytest_epoch).detach()\n",
    "\n",
    "    relcotest = torch.mean(relcotest_epoch).detach()\n",
    "    costtest = torch.mean(cost_epoch).detach()\n",
    "    \n",
    "    #====== convergence criteria\n",
    "    infeasible_cases_tot = torch.sum(infeasibility_count).detach()\n",
    "    line_violation_tot = torch.sum(line_violation_count).detach()\n",
    "    crit_violation_tot = torch.sum(crit_violation_count).detach()\n",
    "    #tot_avg_violation = torch.sum(avg_violation_list).detach()\n",
    "    count_violation_tot = torch.sum(count_violation_list).detach()\n",
    "    tot_flows = l*Ncontingencies*batch_size\n",
    "    \n",
    "    error[epoch,:] = imbalancetest.detach().numpy()\n",
    "    avg_infeasibility[epoch,:] =  infeasibilitytest.detach().numpy()\n",
    "    relcost[epoch,:] =  relcotest.detach().numpy()\n",
    "    cost[epoch,:] =  costtest.detach().numpy()\n",
    "    end_test = time.time()\n",
    "    time_test = end_test-start_test\n",
    "\n",
    "    print(epoch, \" | \", round(time_test,0), \" | \",np.round(error[epoch,:],3), \" | \", np.round(torch.mean(clil.cpu()).detach().numpy(),3), \" | \", np.round(avg_infeasibility[epoch,:],3), \" | \", np.round(relcost[epoch,:], 3) )\n",
    "        \n",
    "\n",
    "print('method time:', time_test)\n",
    "\n",
    "print('current infeasibility:', (infeasible_cases_tot/Nsamples)*100, '%', 'goal:', 1, '%')\n",
    "    \n",
    "#print('current avg violation:', (tot_avg_violation/initial_violation_count),'pu', 'goal', 0.01*max_f, 'pu')\n",
    "print('infeasible contingency cases:', line_violation_tot, (line_violation_tot/(Ncontingencies*Nsamples))*100, '%')\n",
    "print('10%+ infeasible cases:', crit_violation_tot, (crit_violation_tot/(Ncontingencies*Nsamples))*100, '%')\n",
    "print('number of violations:', count_violation_tot)\n",
    "print('total cost:', cost.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde65dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cea26a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
